{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25973915358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ 99% of human images are detected with human face.\n",
    "            12% of dog faces are detected with human face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99% \n",
      "12% \n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "human_in_human_picture_count=0\n",
    "human_in_dog_picture_count=0\n",
    "for imge in human_files_short:\n",
    "    if(face_detector(imge)):\n",
    "        human_in_human_picture_count+=1\n",
    "for imge in dog_files_short:\n",
    "    if(face_detector(imge)):\n",
    "        human_in_dog_picture_count+=1\n",
    "print(\"{}% \".format(human_in_human_picture_count))\n",
    "print(\"{}% \".format(human_in_dog_picture_count))\n",
    "## TODO: Test the performance of the face_detector algorithm \n",
    "## on the images in human_files_short and dog_files_short\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__ No, this is not resonable expectation to pose on the user.In my opinion there can be two approaches to this. \n",
    "The first can be focussing heavily on individual facial features like nose, eyes, lips and ears. So that if the classifier detects a part of the face, it can assert that it is a face.The second approach can be combining pre processing method for facial image data which will lead to better processing of raw data for training.\n",
    "\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ 1% images in `human_files_short` have a detected dog and 100% images in `dog_files_short` have a detected dog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1%\n",
      "100%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "dog_detection_in_dog=0\n",
    "dog_detection_in_human=0\n",
    "for imge in human_files_short:\n",
    "    if dog_detector(imge):\n",
    "        dog_detection_in_human+=1\n",
    "for imge in dog_files_short:\n",
    "    if dog_detector(imge):\n",
    "        dog_detection_in_dog+=1\n",
    "print(\"{}%\".format(dog_detection_in_human))\n",
    "print(\"{}%\".format(dog_detection_in_dog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 6680/6680 [05:42<00:00, 19.51it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 835/835 [01:36<00:00,  8.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 836/836 [00:38<00:00, 21.70it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__  Step 1) I started with adding four layers of convolutional neural network. \n",
    "             Step 2) I then added Max Pooling layer for reducing dimensionality. \n",
    "             Step 3) In the above description, it is mentioned that making a convolutional network for CPU having 5 epochs will                      provide a better result. \n",
    "             Step 4) I provided 2^x values to filters and that increased gradually with each convolutional network. \n",
    "             Step 5) I then added a Flatten layer, to convert the input into a vector, and then a dense layer.\n",
    "             Step 6) In the end I added an output layer with 133 nodes. \n",
    "             \n",
    "To achieve increased image accuracy, we need to increase the number of filters. This in turn causes an increaase in dimentionality which leads to overfitting. In order to overcome this obstacle, I used maxpooling2D layer which reduces dimentionality, thus preventing overfitting. In this way we can avail the benefits of multiple filters without compromising on overfitting, which makes this CNN architecture work well for image classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 28, 28, 128)       32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               12544500  \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 133)               66633     \n",
      "=================================================================\n",
      "Total params: 12,654,573\n",
      "Trainable params: 12,654,573\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "### TODO: Define your architecture.\n",
    "model.add(Conv2D(filters=16, kernel_size=2,padding='same',input_shape=(224,224,3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2,padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2,padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2,padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500,activation='relu'))\n",
    "model.add(Dense(133,activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4040/6680 [=================>............] - ETA: 2:18:13 - loss: 4.9037 - acc: 0.0000e+ - ETA: 1:20:53 - loss: 9.5388 - acc: 0.0250   - ETA: 57:25 - loss: 8.5577 - acc: 0.0167 - ETA: 46:23 - loss: 7.8939 - acc: 0.01 - ETA: 39:29 - loss: 9.2309 - acc: 0.02 - ETA: 34:53 - loss: 8.9146 - acc: 0.01 - ETA: 34:48 - loss: 8.7855 - acc: 0.01 - ETA: 32:14 - loss: 8.4367 - acc: 0.01 - ETA: 30:38 - loss: 8.0695 - acc: 0.01 - ETA: 28:37 - loss: 7.7857 - acc: 0.01 - ETA: 26:59 - loss: 7.5416 - acc: 0.00 - ETA: 25:27 - loss: 7.3303 - acc: 0.00 - ETA: 24:19 - loss: 7.1560 - acc: 0.00 - ETA: 23:15 - loss: 6.9874 - acc: 0.01 - ETA: 22:21 - loss: 6.8867 - acc: 0.01 - ETA: 21:36 - loss: 6.7841 - acc: 0.00 - ETA: 20:53 - loss: 6.6692 - acc: 0.00 - ETA: 20:15 - loss: 6.5755 - acc: 0.00 - ETA: 20:07 - loss: 6.4991 - acc: 0.00 - ETA: 20:31 - loss: 6.4295 - acc: 0.00 - ETA: 20:02 - loss: 6.3611 - acc: 0.00 - ETA: 19:30 - loss: 6.2960 - acc: 0.00 - ETA: 19:01 - loss: 6.2330 - acc: 0.00 - ETA: 19:01 - loss: 6.1777 - acc: 0.00 - ETA: 18:38 - loss: 6.1265 - acc: 0.00 - ETA: 18:14 - loss: 6.0770 - acc: 0.00 - ETA: 17:53 - loss: 6.0303 - acc: 0.00 - ETA: 17:41 - loss: 6.0062 - acc: 0.00 - ETA: 17:25 - loss: 5.9624 - acc: 0.00 - ETA: 17:11 - loss: 5.9256 - acc: 0.00 - ETA: 17:19 - loss: 5.9013 - acc: 0.00 - ETA: 17:04 - loss: 5.8692 - acc: 0.00 - ETA: 16:50 - loss: 5.8443 - acc: 0.00 - ETA: 16:35 - loss: 5.8164 - acc: 0.00 - ETA: 16:22 - loss: 5.7872 - acc: 0.00 - ETA: 16:33 - loss: 5.7736 - acc: 0.00 - ETA: 16:26 - loss: 5.7556 - acc: 0.00 - ETA: 16:18 - loss: 5.7349 - acc: 0.00 - ETA: 16:09 - loss: 5.7167 - acc: 0.00 - ETA: 16:05 - loss: 5.6972 - acc: 0.00 - ETA: 15:58 - loss: 5.6799 - acc: 0.00 - ETA: 15:48 - loss: 5.6614 - acc: 0.00 - ETA: 15:36 - loss: 5.6433 - acc: 0.00 - ETA: 15:26 - loss: 5.6315 - acc: 0.00 - ETA: 15:26 - loss: 5.6148 - acc: 0.00 - ETA: 15:16 - loss: 5.5975 - acc: 0.00 - ETA: 15:25 - loss: 5.5821 - acc: 0.00 - ETA: 15:43 - loss: 5.5705 - acc: 0.00 - ETA: 15:33 - loss: 5.5568 - acc: 0.01 - ETA: 15:26 - loss: 5.5428 - acc: 0.01 - ETA: 15:32 - loss: 5.5314 - acc: 0.01 - ETA: 15:23 - loss: 5.5224 - acc: 0.01 - ETA: 15:15 - loss: 5.5110 - acc: 0.01 - ETA: 15:05 - loss: 5.4993 - acc: 0.01 - ETA: 14:56 - loss: 5.4863 - acc: 0.01 - ETA: 14:47 - loss: 5.4777 - acc: 0.01 - ETA: 14:38 - loss: 5.4687 - acc: 0.01 - ETA: 14:29 - loss: 5.4581 - acc: 0.01 - ETA: 14:21 - loss: 5.4503 - acc: 0.01 - ETA: 14:14 - loss: 5.4394 - acc: 0.01 - ETA: 14:05 - loss: 5.4289 - acc: 0.01 - ETA: 13:58 - loss: 5.4212 - acc: 0.01 - ETA: 13:50 - loss: 5.4171 - acc: 0.01 - ETA: 13:42 - loss: 5.4096 - acc: 0.01 - ETA: 13:41 - loss: 5.4022 - acc: 0.01 - ETA: 13:36 - loss: 5.3931 - acc: 0.01 - ETA: 13:40 - loss: 5.3869 - acc: 0.01 - ETA: 13:36 - loss: 5.3798 - acc: 0.01 - ETA: 13:32 - loss: 5.3731 - acc: 0.01 - ETA: 13:30 - loss: 5.3638 - acc: 0.01 - ETA: 13:24 - loss: 5.3545 - acc: 0.01 - ETA: 13:18 - loss: 5.3492 - acc: 0.01 - ETA: 13:11 - loss: 5.3450 - acc: 0.01 - ETA: 13:11 - loss: 5.3368 - acc: 0.01 - ETA: 13:06 - loss: 5.3310 - acc: 0.01 - ETA: 13:04 - loss: 5.3254 - acc: 0.01 - ETA: 13:06 - loss: 5.3204 - acc: 0.01 - ETA: 13:04 - loss: 5.3166 - acc: 0.01 - ETA: 12:59 - loss: 5.3128 - acc: 0.01 - ETA: 12:55 - loss: 5.3047 - acc: 0.01 - ETA: 12:59 - loss: 5.2991 - acc: 0.01 - ETA: 12:55 - loss: 5.2918 - acc: 0.01 - ETA: 12:52 - loss: 5.2846 - acc: 0.01 - ETA: 12:50 - loss: 5.2787 - acc: 0.01 - ETA: 12:44 - loss: 5.2725 - acc: 0.01 - ETA: 12:38 - loss: 5.2659 - acc: 0.01 - ETA: 12:32 - loss: 5.2624 - acc: 0.01 - ETA: 12:27 - loss: 5.2574 - acc: 0.01 - ETA: 12:41 - loss: 5.2551 - acc: 0.01 - ETA: 12:47 - loss: 5.2496 - acc: 0.01 - ETA: 12:42 - loss: 5.2508 - acc: 0.01 - ETA: 12:36 - loss: 5.2446 - acc: 0.01 - ETA: 12:30 - loss: 5.2382 - acc: 0.01 - ETA: 12:26 - loss: 5.2443 - acc: 0.01 - ETA: 12:20 - loss: 5.2401 - acc: 0.01 - ETA: 12:15 - loss: 5.2350 - acc: 0.01 - ETA: 12:09 - loss: 5.2319 - acc: 0.01 - ETA: 12:04 - loss: 5.2252 - acc: 0.01 - ETA: 11:59 - loss: 5.2197 - acc: 0.01 - ETA: 11:53 - loss: 5.2172 - acc: 0.01 - ETA: 11:48 - loss: 5.2141 - acc: 0.01 - ETA: 11:43 - loss: 5.2100 - acc: 0.01 - ETA: 11:38 - loss: 5.2067 - acc: 0.01 - ETA: 11:32 - loss: 5.2021 - acc: 0.01 - ETA: 11:28 - loss: 5.1964 - acc: 0.01 - ETA: 11:23 - loss: 5.1925 - acc: 0.01 - ETA: 11:18 - loss: 5.1878 - acc: 0.01 - ETA: 11:13 - loss: 5.1856 - acc: 0.01 - ETA: 11:11 - loss: 5.1828 - acc: 0.01 - ETA: 11:08 - loss: 5.1786 - acc: 0.01 - ETA: 11:05 - loss: 5.1735 - acc: 0.01 - ETA: 11:02 - loss: 5.1713 - acc: 0.01 - ETA: 10:59 - loss: 5.1692 - acc: 0.01 - ETA: 10:55 - loss: 5.1653 - acc: 0.01 - ETA: 10:55 - loss: 5.1635 - acc: 0.01 - ETA: 10:50 - loss: 5.1592 - acc: 0.01 - ETA: 10:46 - loss: 5.1577 - acc: 0.01 - ETA: 10:41 - loss: 5.1541 - acc: 0.01 - ETA: 10:37 - loss: 5.1538 - acc: 0.01 - ETA: 10:32 - loss: 5.1486 - acc: 0.01 - ETA: 10:28 - loss: 5.1446 - acc: 0.01 - ETA: 10:24 - loss: 5.1400 - acc: 0.01 - ETA: 10:20 - loss: 5.1383 - acc: 0.01 - ETA: 10:17 - loss: 5.1349 - acc: 0.01 - ETA: 10:15 - loss: 5.1346 - acc: 0.01 - ETA: 10:13 - loss: 5.1320 - acc: 0.01 - ETA: 10:10 - loss: 5.1277 - acc: 0.01 - ETA: 10:09 - loss: 5.1246 - acc: 0.01 - ETA: 10:07 - loss: 5.1218 - acc: 0.01 - ETA: 10:03 - loss: 5.1196 - acc: 0.01 - ETA: 10:00 - loss: 5.1162 - acc: 0.01 - ETA: 9:58 - loss: 5.1140 - acc: 0.0174 - ETA: 9:56 - loss: 5.1121 - acc: 0.018 - ETA: 9:55 - loss: 5.1112 - acc: 0.017 - ETA: 9:52 - loss: 5.1071 - acc: 0.018 - ETA: 9:55 - loss: 5.1041 - acc: 0.018 - ETA: 9:51 - loss: 5.1003 - acc: 0.017 - ETA: 9:47 - loss: 5.1002 - acc: 0.017 - ETA: 9:43 - loss: 5.0973 - acc: 0.017 - ETA: 9:39 - loss: 5.0950 - acc: 0.017 - ETA: 9:34 - loss: 5.0907 - acc: 0.018 - ETA: 9:30 - loss: 5.0886 - acc: 0.018 - ETA: 9:26 - loss: 5.0871 - acc: 0.018 - ETA: 9:22 - loss: 5.0836 - acc: 0.018 - ETA: 9:19 - loss: 5.0798 - acc: 0.018 - ETA: 9:15 - loss: 5.0766 - acc: 0.018 - ETA: 9:11 - loss: 5.0754 - acc: 0.018 - ETA: 9:08 - loss: 5.0727 - acc: 0.018 - ETA: 9:05 - loss: 5.0684 - acc: 0.018 - ETA: 9:01 - loss: 5.0675 - acc: 0.019 - ETA: 8:58 - loss: 5.0657 - acc: 0.019 - ETA: 8:55 - loss: 5.0631 - acc: 0.019 - ETA: 8:55 - loss: 5.0611 - acc: 0.019 - ETA: 8:51 - loss: 5.0589 - acc: 0.018 - ETA: 8:47 - loss: 5.0575 - acc: 0.018 - ETA: 8:43 - loss: 5.0554 - acc: 0.018 - ETA: 8:39 - loss: 5.0519 - acc: 0.019 - ETA: 8:36 - loss: 5.0515 - acc: 0.019 - ETA: 8:32 - loss: 5.0500 - acc: 0.019 - ETA: 8:30 - loss: 5.0473 - acc: 0.020 - ETA: 8:26 - loss: 5.0437 - acc: 0.020 - ETA: 8:22 - loss: 5.0427 - acc: 0.020 - ETA: 8:18 - loss: 5.0405 - acc: 0.020 - ETA: 8:15 - loss: 5.0386 - acc: 0.020 - ETA: 8:11 - loss: 5.0368 - acc: 0.020 - ETA: 8:09 - loss: 5.0331 - acc: 0.020 - ETA: 8:11 - loss: 5.0314 - acc: 0.020 - ETA: 8:07 - loss: 5.0306 - acc: 0.020 - ETA: 8:04 - loss: 5.0292 - acc: 0.020 - ETA: 8:00 - loss: 5.0262 - acc: 0.020 - ETA: 7:56 - loss: 5.0255 - acc: 0.020 - ETA: 7:53 - loss: 5.0235 - acc: 0.020 - ETA: 7:49 - loss: 5.0212 - acc: 0.020 - ETA: 7:46 - loss: 5.0202 - acc: 0.020 - ETA: 7:42 - loss: 5.0179 - acc: 0.020 - ETA: 7:42 - loss: 5.0164 - acc: 0.019 - ETA: 7:42 - loss: 5.0147 - acc: 0.020 - ETA: 7:40 - loss: 5.0126 - acc: 0.019 - ETA: 7:37 - loss: 5.0117 - acc: 0.019 - ETA: 7:34 - loss: 5.0098 - acc: 0.019 - ETA: 7:30 - loss: 5.0073 - acc: 0.019 - ETA: 7:27 - loss: 5.0069 - acc: 0.019 - ETA: 7:23 - loss: 5.0056 - acc: 0.019 - ETA: 7:20 - loss: 5.0042 - acc: 0.019 - ETA: 7:16 - loss: 5.0034 - acc: 0.019 - ETA: 7:13 - loss: 5.0031 - acc: 0.019 - ETA: 7:10 - loss: 5.0007 - acc: 0.019 - ETA: 7:06 - loss: 5.0002 - acc: 0.019 - ETA: 7:03 - loss: 4.9979 - acc: 0.019 - ETA: 6:59 - loss: 4.9945 - acc: 0.019 - ETA: 6:57 - loss: 4.9931 - acc: 0.019 - ETA: 6:53 - loss: 4.9925 - acc: 0.019 - ETA: 6:51 - loss: 4.9901 - acc: 0.019 - ETA: 6:49 - loss: 4.9885 - acc: 0.019 - ETA: 6:45 - loss: 4.9858 - acc: 0.020 - ETA: 6:43 - loss: 4.9842 - acc: 0.020 - ETA: 6:44 - loss: 4.9834 - acc: 0.020 - ETA: 6:40 - loss: 4.9812 - acc: 0.020 - ETA: 6:37 - loss: 4.9801 - acc: 0.021 - ETA: 6:34 - loss: 4.9787 - acc: 0.021 - ETA: 6:31 - loss: 4.9779 - acc: 0.020 - ETA: 6:28 - loss: 4.9755 - acc: 0.0206680/6680 [==============================] - ETA: 6:24 - loss: 4.9743 - acc: 0.020 - ETA: 6:21 - loss: 4.9726 - acc: 0.021 - ETA: 6:18 - loss: 4.9732 - acc: 0.021 - ETA: 6:15 - loss: 4.9703 - acc: 0.021 - ETA: 6:12 - loss: 4.9690 - acc: 0.022 - ETA: 6:10 - loss: 4.9673 - acc: 0.021 - ETA: 6:07 - loss: 4.9658 - acc: 0.022 - ETA: 6:04 - loss: 4.9642 - acc: 0.022 - ETA: 6:01 - loss: 4.9629 - acc: 0.022 - ETA: 5:59 - loss: 4.9616 - acc: 0.022 - ETA: 5:56 - loss: 4.9587 - acc: 0.022 - ETA: 5:53 - loss: 4.9595 - acc: 0.022 - ETA: 5:49 - loss: 4.9585 - acc: 0.022 - ETA: 5:46 - loss: 4.9568 - acc: 0.022 - ETA: 5:43 - loss: 4.9564 - acc: 0.022 - ETA: 5:40 - loss: 4.9539 - acc: 0.022 - ETA: 5:37 - loss: 4.9526 - acc: 0.022 - ETA: 5:36 - loss: 4.9510 - acc: 0.022 - ETA: 5:32 - loss: 4.9513 - acc: 0.022 - ETA: 5:29 - loss: 4.9494 - acc: 0.022 - ETA: 5:26 - loss: 4.9470 - acc: 0.023 - ETA: 5:23 - loss: 4.9444 - acc: 0.023 - ETA: 5:19 - loss: 4.9431 - acc: 0.024 - ETA: 5:16 - loss: 4.9427 - acc: 0.023 - ETA: 5:13 - loss: 4.9416 - acc: 0.024 - ETA: 5:09 - loss: 4.9399 - acc: 0.023 - ETA: 5:06 - loss: 4.9381 - acc: 0.024 - ETA: 5:03 - loss: 4.9376 - acc: 0.023 - ETA: 5:00 - loss: 4.9361 - acc: 0.024 - ETA: 4:57 - loss: 4.9352 - acc: 0.023 - ETA: 4:53 - loss: 4.9334 - acc: 0.023 - ETA: 4:51 - loss: 4.9331 - acc: 0.023 - ETA: 4:48 - loss: 4.9328 - acc: 0.023 - ETA: 4:45 - loss: 4.9308 - acc: 0.024 - ETA: 4:42 - loss: 4.9300 - acc: 0.024 - ETA: 4:39 - loss: 4.9287 - acc: 0.024 - ETA: 4:36 - loss: 4.9280 - acc: 0.024 - ETA: 4:33 - loss: 4.9256 - acc: 0.024 - ETA: 4:30 - loss: 4.9251 - acc: 0.024 - ETA: 4:27 - loss: 4.9250 - acc: 0.024 - ETA: 4:24 - loss: 4.9239 - acc: 0.024 - ETA: 4:21 - loss: 4.9224 - acc: 0.024 - ETA: 4:18 - loss: 4.9209 - acc: 0.024 - ETA: 4:15 - loss: 4.9198 - acc: 0.024 - ETA: 4:12 - loss: 4.9183 - acc: 0.024 - ETA: 4:09 - loss: 4.9167 - acc: 0.025 - ETA: 4:07 - loss: 4.9151 - acc: 0.025 - ETA: 4:04 - loss: 4.9125 - acc: 0.025 - ETA: 4:02 - loss: 4.9112 - acc: 0.024 - ETA: 3:59 - loss: 4.9088 - acc: 0.025 - ETA: 3:56 - loss: 4.9071 - acc: 0.025 - ETA: 3:52 - loss: 4.9059 - acc: 0.025 - ETA: 3:49 - loss: 4.9033 - acc: 0.025 - ETA: 3:46 - loss: 4.9026 - acc: 0.025 - ETA: 3:43 - loss: 4.9024 - acc: 0.025 - ETA: 3:40 - loss: 4.8999 - acc: 0.025 - ETA: 3:37 - loss: 4.8999 - acc: 0.025 - ETA: 3:35 - loss: 4.8984 - acc: 0.026 - ETA: 3:32 - loss: 4.8963 - acc: 0.026 - ETA: 3:29 - loss: 4.8944 - acc: 0.026 - ETA: 3:26 - loss: 4.8945 - acc: 0.026 - ETA: 3:23 - loss: 4.8935 - acc: 0.026 - ETA: 3:20 - loss: 4.8925 - acc: 0.026 - ETA: 3:17 - loss: 4.8905 - acc: 0.026 - ETA: 3:14 - loss: 4.8907 - acc: 0.026 - ETA: 3:11 - loss: 4.8885 - acc: 0.026 - ETA: 3:08 - loss: 4.8865 - acc: 0.026 - ETA: 3:05 - loss: 4.8848 - acc: 0.026 - ETA: 3:02 - loss: 4.8824 - acc: 0.026 - ETA: 2:59 - loss: 4.8812 - acc: 0.026 - ETA: 2:56 - loss: 4.8794 - acc: 0.027 - ETA: 2:53 - loss: 4.8776 - acc: 0.027 - ETA: 2:50 - loss: 4.8757 - acc: 0.027 - ETA: 2:47 - loss: 4.8763 - acc: 0.027 - ETA: 2:44 - loss: 4.8758 - acc: 0.027 - ETA: 2:41 - loss: 4.8748 - acc: 0.027 - ETA: 2:38 - loss: 4.8741 - acc: 0.027 - ETA: 2:36 - loss: 4.8717 - acc: 0.027 - ETA: 2:33 - loss: 4.8706 - acc: 0.027 - ETA: 2:30 - loss: 4.8706 - acc: 0.028 - ETA: 2:27 - loss: 4.8694 - acc: 0.027 - ETA: 2:24 - loss: 4.8679 - acc: 0.028 - ETA: 2:22 - loss: 4.8672 - acc: 0.027 - ETA: 2:19 - loss: 4.8660 - acc: 0.027 - ETA: 2:16 - loss: 4.8648 - acc: 0.027 - ETA: 2:13 - loss: 4.8642 - acc: 0.028 - ETA: 2:10 - loss: 4.8630 - acc: 0.028 - ETA: 2:07 - loss: 4.8630 - acc: 0.028 - ETA: 2:04 - loss: 4.8614 - acc: 0.028 - ETA: 2:01 - loss: 4.8606 - acc: 0.028 - ETA: 1:58 - loss: 4.8586 - acc: 0.028 - ETA: 1:55 - loss: 4.8577 - acc: 0.028 - ETA: 1:52 - loss: 4.8550 - acc: 0.028 - ETA: 1:49 - loss: 4.8531 - acc: 0.028 - ETA: 1:46 - loss: 4.8525 - acc: 0.028 - ETA: 1:43 - loss: 4.8514 - acc: 0.028 - ETA: 1:40 - loss: 4.8511 - acc: 0.028 - ETA: 1:37 - loss: 4.8495 - acc: 0.028 - ETA: 1:35 - loss: 4.8483 - acc: 0.028 - ETA: 1:33 - loss: 4.8470 - acc: 0.028 - ETA: 1:30 - loss: 4.8474 - acc: 0.028 - ETA: 1:29 - loss: 4.8463 - acc: 0.028 - ETA: 1:26 - loss: 4.8455 - acc: 0.028 - ETA: 1:24 - loss: 4.8427 - acc: 0.028 - ETA: 1:21 - loss: 4.8417 - acc: 0.029 - ETA: 1:18 - loss: 4.8418 - acc: 0.028 - ETA: 1:15 - loss: 4.8402 - acc: 0.029 - ETA: 1:12 - loss: 4.8389 - acc: 0.029 - ETA: 1:09 - loss: 4.8379 - acc: 0.029 - ETA: 1:06 - loss: 4.8365 - acc: 0.029 - ETA: 1:03 - loss: 4.8354 - acc: 0.029 - ETA: 1:00 - loss: 4.8336 - acc: 0.029 - ETA: 57s - loss: 4.8330 - acc: 0.029 - ETA: 54s - loss: 4.8326 - acc: 0.02 - ETA: 51s - loss: 4.8326 - acc: 0.02 - ETA: 48s - loss: 4.8314 - acc: 0.02 - ETA: 46s - loss: 4.8302 - acc: 0.02 - ETA: 42s - loss: 4.8292 - acc: 0.02 - ETA: 39s - loss: 4.8289 - acc: 0.02 - ETA: 36s - loss: 4.8271 - acc: 0.03 - ETA: 33s - loss: 4.8255 - acc: 0.03 - ETA: 30s - loss: 4.8241 - acc: 0.03 - ETA: 27s - loss: 4.8230 - acc: 0.03 - ETA: 24s - loss: 4.8217 - acc: 0.02 - ETA: 21s - loss: 4.8208 - acc: 0.02 - ETA: 18s - loss: 4.8201 - acc: 0.03 - ETA: 15s - loss: 4.8193 - acc: 0.03 - ETA: 12s - loss: 4.8180 - acc: 0.03 - ETA: 9s - loss: 4.8166 - acc: 0.0304 - ETA: 6s - loss: 4.8149 - acc: 0.030 - ETA: 3s - loss: 4.8129 - acc: 0.030 - 1157s 173ms/step - loss: 4.8116 - acc: 0.0307 - val_loss: 4.4240 - val_acc: 0.0467\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.42404, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 24:45 - loss: 4.2823 - acc: 0.20 - ETA: 24:31 - loss: 4.3080 - acc: 0.12 - ETA: 23:54 - loss: 4.1336 - acc: 0.10 - ETA: 23:56 - loss: 4.0638 - acc: 0.08 - ETA: 21:27 - loss: 4.0127 - acc: 0.11 - ETA: 19:48 - loss: 4.1198 - acc: 0.10 - ETA: 18:52 - loss: 4.1099 - acc: 0.09 - ETA: 17:41 - loss: 4.0747 - acc: 0.10 - ETA: 16:50 - loss: 4.0482 - acc: 0.10 - ETA: 16:23 - loss: 4.0417 - acc: 0.10 - ETA: 16:23 - loss: 4.0475 - acc: 0.10 - ETA: 15:52 - loss: 4.0739 - acc: 0.09 - ETA: 15:47 - loss: 4.0752 - acc: 0.10 - ETA: 15:29 - loss: 4.0794 - acc: 0.10 - ETA: 15:20 - loss: 4.0607 - acc: 0.10 - ETA: 14:57 - loss: 4.0685 - acc: 0.10 - ETA: 14:42 - loss: 4.0818 - acc: 0.10 - ETA: 14:26 - loss: 4.0933 - acc: 0.10 - ETA: 14:12 - loss: 4.1127 - acc: 0.09 - ETA: 14:01 - loss: 4.1123 - acc: 0.09 - ETA: 13:49 - loss: 4.1106 - acc: 0.09 - ETA: 13:35 - loss: 4.1254 - acc: 0.08 - ETA: 13:22 - loss: 4.1080 - acc: 0.08 - ETA: 13:12 - loss: 4.1029 - acc: 0.08 - ETA: 13:16 - loss: 4.0954 - acc: 0.09 - ETA: 13:08 - loss: 4.1066 - acc: 0.09 - ETA: 13:02 - loss: 4.1026 - acc: 0.09 - ETA: 12:51 - loss: 4.1048 - acc: 0.08 - ETA: 12:50 - loss: 4.0881 - acc: 0.09 - ETA: 12:45 - loss: 4.0833 - acc: 0.09 - ETA: 12:49 - loss: 4.0980 - acc: 0.09 - ETA: 12:43 - loss: 4.0901 - acc: 0.09 - ETA: 12:46 - loss: 4.0989 - acc: 0.09 - ETA: 12:39 - loss: 4.0989 - acc: 0.09 - ETA: 12:49 - loss: 4.0964 - acc: 0.09 - ETA: 12:42 - loss: 4.0888 - acc: 0.09 - ETA: 12:46 - loss: 4.0873 - acc: 0.09 - ETA: 12:40 - loss: 4.0834 - acc: 0.09 - ETA: 12:33 - loss: 4.0824 - acc: 0.09 - ETA: 12:25 - loss: 4.0812 - acc: 0.09 - ETA: 12:19 - loss: 4.0811 - acc: 0.09 - ETA: 12:15 - loss: 4.0768 - acc: 0.09 - ETA: 12:08 - loss: 4.0735 - acc: 0.09 - ETA: 12:02 - loss: 4.0676 - acc: 0.09 - ETA: 11:56 - loss: 4.0599 - acc: 0.09 - ETA: 11:51 - loss: 4.0589 - acc: 0.09 - ETA: 11:47 - loss: 4.0584 - acc: 0.09 - ETA: 11:41 - loss: 4.0610 - acc: 0.09 - ETA: 11:36 - loss: 4.0654 - acc: 0.09 - ETA: 11:31 - loss: 4.0656 - acc: 0.09 - ETA: 11:26 - loss: 4.0642 - acc: 0.09 - ETA: 11:22 - loss: 4.0683 - acc: 0.09 - ETA: 11:19 - loss: 4.0630 - acc: 0.09 - ETA: 11:16 - loss: 4.0675 - acc: 0.09 - ETA: 11:12 - loss: 4.0661 - acc: 0.09 - ETA: 11:07 - loss: 4.0590 - acc: 0.09 - ETA: 11:04 - loss: 4.0540 - acc: 0.09 - ETA: 11:07 - loss: 4.0598 - acc: 0.09 - ETA: 11:06 - loss: 4.0614 - acc: 0.09 - ETA: 11:03 - loss: 4.0724 - acc: 0.09 - ETA: 11:00 - loss: 4.0651 - acc: 0.09 - ETA: 10:58 - loss: 4.0651 - acc: 0.09 - ETA: 10:54 - loss: 4.0660 - acc: 0.09 - ETA: 10:50 - loss: 4.0670 - acc: 0.09 - ETA: 10:48 - loss: 4.0693 - acc: 0.09 - ETA: 10:45 - loss: 4.0765 - acc: 0.09 - ETA: 10:43 - loss: 4.0793 - acc: 0.09 - ETA: 10:43 - loss: 4.0793 - acc: 0.09 - ETA: 10:39 - loss: 4.0768 - acc: 0.09 - ETA: 10:38 - loss: 4.0750 - acc: 0.09 - ETA: 10:38 - loss: 4.0804 - acc: 0.09 - ETA: 10:37 - loss: 4.0814 - acc: 0.09 - ETA: 10:36 - loss: 4.0849 - acc: 0.09 - ETA: 10:32 - loss: 4.0904 - acc: 0.09 - ETA: 10:28 - loss: 4.0915 - acc: 0.09 - ETA: 10:24 - loss: 4.0879 - acc: 0.09 - ETA: 10:20 - loss: 4.0893 - acc: 0.09 - ETA: 10:16 - loss: 4.0862 - acc: 0.09 - ETA: 10:12 - loss: 4.0827 - acc: 0.09 - ETA: 10:09 - loss: 4.0776 - acc: 0.09 - ETA: 10:06 - loss: 4.0785 - acc: 0.09 - ETA: 10:09 - loss: 4.0814 - acc: 0.09 - ETA: 10:06 - loss: 4.0798 - acc: 0.09 - ETA: 10:03 - loss: 4.0817 - acc: 0.09 - ETA: 10:02 - loss: 4.0808 - acc: 0.09 - ETA: 10:01 - loss: 4.0796 - acc: 0.09 - ETA: 9:59 - loss: 4.0817 - acc: 0.0943 - ETA: 9:57 - loss: 4.0818 - acc: 0.093 - ETA: 9:54 - loss: 4.0768 - acc: 0.094 - ETA: 9:54 - loss: 4.0753 - acc: 0.095 - ETA: 9:55 - loss: 4.0756 - acc: 0.094 - ETA: 9:55 - loss: 4.0720 - acc: 0.095 - ETA: 9:54 - loss: 4.0726 - acc: 0.095 - ETA: 9:53 - loss: 4.0730 - acc: 0.094 - ETA: 9:52 - loss: 4.0724 - acc: 0.094 - ETA: 9:51 - loss: 4.0724 - acc: 0.094 - ETA: 9:50 - loss: 4.0720 - acc: 0.094 - ETA: 9:47 - loss: 4.0719 - acc: 0.094 - ETA: 9:46 - loss: 4.0744 - acc: 0.096 - ETA: 9:44 - loss: 4.0780 - acc: 0.096 - ETA: 9:43 - loss: 4.0758 - acc: 0.097 - ETA: 9:42 - loss: 4.0745 - acc: 0.097 - ETA: 9:40 - loss: 4.0801 - acc: 0.098 - ETA: 9:42 - loss: 4.0778 - acc: 0.099 - ETA: 9:41 - loss: 4.0759 - acc: 0.099 - ETA: 9:40 - loss: 4.0779 - acc: 0.098 - ETA: 9:38 - loss: 4.0775 - acc: 0.098 - ETA: 9:36 - loss: 4.0756 - acc: 0.097 - ETA: 9:34 - loss: 4.0751 - acc: 0.096 - ETA: 9:33 - loss: 4.0721 - acc: 0.095 - ETA: 9:33 - loss: 4.0759 - acc: 0.095 - ETA: 9:47 - loss: 4.0751 - acc: 0.095 - ETA: 9:52 - loss: 4.0749 - acc: 0.095 - ETA: 9:49 - loss: 4.0722 - acc: 0.096 - ETA: 9:46 - loss: 4.0755 - acc: 0.095 - ETA: 9:44 - loss: 4.0753 - acc: 0.095 - ETA: 9:41 - loss: 4.0748 - acc: 0.095 - ETA: 9:38 - loss: 4.0752 - acc: 0.095 - ETA: 9:35 - loss: 4.0746 - acc: 0.095 - ETA: 9:31 - loss: 4.0739 - acc: 0.095 - ETA: 9:27 - loss: 4.0774 - acc: 0.095 - ETA: 9:24 - loss: 4.0765 - acc: 0.095 - ETA: 9:21 - loss: 4.0729 - acc: 0.096 - ETA: 9:17 - loss: 4.0702 - acc: 0.096 - ETA: 9:14 - loss: 4.0713 - acc: 0.095 - ETA: 9:11 - loss: 4.0695 - acc: 0.095 - ETA: 9:07 - loss: 4.0734 - acc: 0.095 - ETA: 9:04 - loss: 4.0735 - acc: 0.095 - ETA: 9:01 - loss: 4.0728 - acc: 0.095 - ETA: 8:57 - loss: 4.0717 - acc: 0.095 - ETA: 8:54 - loss: 4.0710 - acc: 0.095 - ETA: 8:52 - loss: 4.0693 - acc: 0.095 - ETA: 8:48 - loss: 4.0647 - acc: 0.096 - ETA: 8:45 - loss: 4.0666 - acc: 0.096 - ETA: 8:42 - loss: 4.0670 - acc: 0.096 - ETA: 8:39 - loss: 4.0669 - acc: 0.096 - ETA: 8:36 - loss: 4.0616 - acc: 0.097 - ETA: 8:33 - loss: 4.0634 - acc: 0.096 - ETA: 8:37 - loss: 4.0604 - acc: 0.097 - ETA: 8:34 - loss: 4.0592 - acc: 0.097 - ETA: 8:30 - loss: 4.0584 - acc: 0.098 - ETA: 8:27 - loss: 4.0618 - acc: 0.097 - ETA: 8:24 - loss: 4.0628 - acc: 0.097 - ETA: 8:20 - loss: 4.0579 - acc: 0.099 - ETA: 8:17 - loss: 4.0573 - acc: 0.099 - ETA: 8:14 - loss: 4.0543 - acc: 0.100 - ETA: 8:11 - loss: 4.0514 - acc: 0.100 - ETA: 8:08 - loss: 4.0569 - acc: 0.100 - ETA: 8:05 - loss: 4.0552 - acc: 0.100 - ETA: 8:02 - loss: 4.0555 - acc: 0.099 - ETA: 7:58 - loss: 4.0558 - acc: 0.099 - ETA: 7:55 - loss: 4.0551 - acc: 0.098 - ETA: 7:54 - loss: 4.0524 - acc: 0.098 - ETA: 7:54 - loss: 4.0517 - acc: 0.098 - ETA: 7:55 - loss: 4.0503 - acc: 0.099 - ETA: 7:52 - loss: 4.0503 - acc: 0.099 - ETA: 7:51 - loss: 4.0523 - acc: 0.099 - ETA: 7:49 - loss: 4.0529 - acc: 0.098 - ETA: 7:47 - loss: 4.0508 - acc: 0.098 - ETA: 7:45 - loss: 4.0495 - acc: 0.099 - ETA: 7:43 - loss: 4.0493 - acc: 0.098 - ETA: 7:41 - loss: 4.0504 - acc: 0.098 - ETA: 7:39 - loss: 4.0522 - acc: 0.098 - ETA: 7:35 - loss: 4.0531 - acc: 0.099 - ETA: 7:32 - loss: 4.0527 - acc: 0.099 - ETA: 7:29 - loss: 4.0510 - acc: 0.100 - ETA: 7:27 - loss: 4.0525 - acc: 0.100 - ETA: 7:25 - loss: 4.0517 - acc: 0.100 - ETA: 7:21 - loss: 4.0499 - acc: 0.100 - ETA: 7:18 - loss: 4.0488 - acc: 0.100 - ETA: 7:15 - loss: 4.0444 - acc: 0.101 - ETA: 7:12 - loss: 4.0445 - acc: 0.101 - ETA: 7:09 - loss: 4.0406 - acc: 0.102 - ETA: 7:06 - loss: 4.0402 - acc: 0.102 - ETA: 7:03 - loss: 4.0398 - acc: 0.102 - ETA: 7:00 - loss: 4.0378 - acc: 0.102 - ETA: 6:57 - loss: 4.0355 - acc: 0.101 - ETA: 6:54 - loss: 4.0349 - acc: 0.101 - ETA: 6:52 - loss: 4.0313 - acc: 0.102 - ETA: 6:51 - loss: 4.0294 - acc: 0.102 - ETA: 6:49 - loss: 4.0271 - acc: 0.102 - ETA: 6:46 - loss: 4.0286 - acc: 0.102 - ETA: 6:50 - loss: 4.0308 - acc: 0.102 - ETA: 6:48 - loss: 4.0288 - acc: 0.102 - ETA: 6:47 - loss: 4.0287 - acc: 0.102 - ETA: 6:45 - loss: 4.0284 - acc: 0.102 - ETA: 6:43 - loss: 4.0264 - acc: 0.102 - ETA: 6:41 - loss: 4.0314 - acc: 0.102 - ETA: 6:44 - loss: 4.0293 - acc: 0.102 - ETA: 6:51 - loss: 4.0316 - acc: 0.102 - ETA: 6:55 - loss: 4.0295 - acc: 0.102 - ETA: 7:00 - loss: 4.0294 - acc: 0.103 - ETA: 7:04 - loss: 4.0291 - acc: 0.102 - ETA: 7:10 - loss: 4.0290 - acc: 0.102 - ETA: 7:13 - loss: 4.0302 - acc: 0.102 - ETA: 7:19 - loss: 4.0322 - acc: 0.101 - ETA: 7:22 - loss: 4.0311 - acc: 0.102 - ETA: 7:24 - loss: 4.0319 - acc: 0.101 - ETA: 1:02:50 - loss: 4.0314 - acc: 0.10 - ETA: 1:02:19 - loss: 4.0317 - acc: 0.10 - ETA: 1:01:46 - loss: 4.0294 - acc: 0.10 - ETA: 1:01:10 - loss: 4.0262 - acc: 0.10 - ETA: 1:00:37 - loss: 4.0274 - acc: 0.10 - ETA: 1:00:01 - loss: 4.0278 - acc: 0.1029"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 59:24 - loss: 4.0264 - acc: 0.1034 - ETA: 58:46 - loss: 4.0239 - acc: 0.10 - ETA: 58:07 - loss: 4.0230 - acc: 0.10 - ETA: 57:29 - loss: 4.0232 - acc: 0.10 - ETA: 56:51 - loss: 4.0246 - acc: 0.10 - ETA: 56:14 - loss: 4.0217 - acc: 0.10 - ETA: 55:36 - loss: 4.0206 - acc: 0.10 - ETA: 54:56 - loss: 4.0181 - acc: 0.10 - ETA: 54:17 - loss: 4.0188 - acc: 0.10 - ETA: 53:37 - loss: 4.0163 - acc: 0.10 - ETA: 52:58 - loss: 4.0160 - acc: 0.10 - ETA: 52:20 - loss: 4.0154 - acc: 0.10 - ETA: 51:44 - loss: 4.0166 - acc: 0.10 - ETA: 51:07 - loss: 4.0150 - acc: 0.10 - ETA: 50:29 - loss: 4.0141 - acc: 0.10 - ETA: 49:51 - loss: 4.0152 - acc: 0.10 - ETA: 49:14 - loss: 4.0162 - acc: 0.10 - ETA: 48:36 - loss: 4.0160 - acc: 0.10 - ETA: 47:58 - loss: 4.0174 - acc: 0.10 - ETA: 47:21 - loss: 4.0169 - acc: 0.10 - ETA: 46:44 - loss: 4.0177 - acc: 0.10 - ETA: 46:07 - loss: 4.0209 - acc: 0.10 - ETA: 45:31 - loss: 4.0196 - acc: 0.10 - ETA: 44:55 - loss: 4.0192 - acc: 0.10 - ETA: 44:20 - loss: 4.0175 - acc: 0.10 - ETA: 43:45 - loss: 4.0180 - acc: 0.10 - ETA: 43:10 - loss: 4.0191 - acc: 0.10 - ETA: 42:36 - loss: 4.0193 - acc: 0.10 - ETA: 42:01 - loss: 4.0178 - acc: 0.10 - ETA: 41:27 - loss: 4.0194 - acc: 0.10 - ETA: 40:53 - loss: 4.0179 - acc: 0.10 - ETA: 40:20 - loss: 4.0166 - acc: 0.10 - ETA: 39:46 - loss: 4.0162 - acc: 0.10 - ETA: 39:13 - loss: 4.0172 - acc: 0.10 - ETA: 38:41 - loss: 4.0172 - acc: 0.10 - ETA: 38:10 - loss: 4.0186 - acc: 0.10 - ETA: 37:38 - loss: 4.0172 - acc: 0.10 - ETA: 37:06 - loss: 4.0203 - acc: 0.10 - ETA: 36:34 - loss: 4.0214 - acc: 0.10 - ETA: 36:02 - loss: 4.0197 - acc: 0.10 - ETA: 35:31 - loss: 4.0194 - acc: 0.10 - ETA: 35:00 - loss: 4.0196 - acc: 0.10 - ETA: 34:30 - loss: 4.0189 - acc: 0.10 - ETA: 33:59 - loss: 4.0202 - acc: 0.10 - ETA: 33:29 - loss: 4.0191 - acc: 0.10 - ETA: 32:59 - loss: 4.0202 - acc: 0.10 - ETA: 32:29 - loss: 4.0201 - acc: 0.10 - ETA: 32:01 - loss: 4.0207 - acc: 0.10 - ETA: 31:31 - loss: 4.0200 - acc: 0.10 - ETA: 31:04 - loss: 4.0191 - acc: 0.10 - ETA: 30:37 - loss: 4.0191 - acc: 0.10 - ETA: 30:07 - loss: 4.0191 - acc: 0.10 - ETA: 29:37 - loss: 4.0195 - acc: 0.10 - ETA: 29:08 - loss: 4.0207 - acc: 0.10 - ETA: 28:39 - loss: 4.0199 - acc: 0.10 - ETA: 28:11 - loss: 4.0194 - acc: 0.10 - ETA: 27:42 - loss: 4.0176 - acc: 0.10 - ETA: 27:14 - loss: 4.0165 - acc: 0.10 - ETA: 26:45 - loss: 4.0147 - acc: 0.10 - ETA: 26:17 - loss: 4.0166 - acc: 0.10 - ETA: 25:50 - loss: 4.0158 - acc: 0.10 - ETA: 25:22 - loss: 4.0152 - acc: 0.10 - ETA: 24:55 - loss: 4.0168 - acc: 0.10 - ETA: 24:28 - loss: 4.0171 - acc: 0.10 - ETA: 24:01 - loss: 4.0168 - acc: 0.10 - ETA: 23:35 - loss: 4.0172 - acc: 0.10 - ETA: 23:08 - loss: 4.0156 - acc: 0.10 - ETA: 22:41 - loss: 4.0144 - acc: 0.10 - ETA: 22:15 - loss: 4.0149 - acc: 0.10 - ETA: 21:49 - loss: 4.0139 - acc: 0.10 - ETA: 21:23 - loss: 4.0117 - acc: 0.10 - ETA: 20:57 - loss: 4.0121 - acc: 0.10 - ETA: 20:32 - loss: 4.0119 - acc: 0.10 - ETA: 20:06 - loss: 4.0114 - acc: 0.10 - ETA: 19:41 - loss: 4.0113 - acc: 0.10 - ETA: 19:16 - loss: 4.0094 - acc: 0.10 - ETA: 18:51 - loss: 4.0081 - acc: 0.10 - ETA: 18:26 - loss: 4.0055 - acc: 0.11 - ETA: 18:01 - loss: 4.0049 - acc: 0.11 - ETA: 17:37 - loss: 4.0048 - acc: 0.11 - ETA: 17:13 - loss: 4.0057 - acc: 0.11 - ETA: 16:49 - loss: 4.0036 - acc: 0.11 - ETA: 16:25 - loss: 4.0027 - acc: 0.11 - ETA: 16:01 - loss: 4.0024 - acc: 0.11 - ETA: 15:37 - loss: 4.0033 - acc: 0.10 - ETA: 15:13 - loss: 4.0035 - acc: 0.10 - ETA: 14:50 - loss: 4.0042 - acc: 0.10 - ETA: 14:26 - loss: 4.0028 - acc: 0.10 - ETA: 14:03 - loss: 4.0022 - acc: 0.10 - ETA: 13:40 - loss: 4.0023 - acc: 0.11 - ETA: 13:17 - loss: 4.0019 - acc: 0.10 - ETA: 12:55 - loss: 4.0018 - acc: 0.10 - ETA: 12:32 - loss: 4.0005 - acc: 0.11 - ETA: 12:09 - loss: 3.9995 - acc: 0.11 - ETA: 11:47 - loss: 3.9991 - acc: 0.11 - ETA: 11:25 - loss: 3.9998 - acc: 0.11 - ETA: 11:03 - loss: 4.0016 - acc: 0.11 - ETA: 10:41 - loss: 4.0019 - acc: 0.10 - ETA: 10:19 - loss: 4.0015 - acc: 0.10 - ETA: 9:57 - loss: 3.9993 - acc: 0.1100 - ETA: 9:36 - loss: 3.9980 - acc: 0.110 - ETA: 9:14 - loss: 3.9980 - acc: 0.110 - ETA: 8:53 - loss: 3.9983 - acc: 0.110 - ETA: 8:32 - loss: 3.9983 - acc: 0.110 - ETA: 8:10 - loss: 3.9990 - acc: 0.110 - ETA: 7:50 - loss: 3.9984 - acc: 0.110 - ETA: 7:29 - loss: 3.9982 - acc: 0.110 - ETA: 7:08 - loss: 3.9974 - acc: 0.110 - ETA: 6:47 - loss: 3.9977 - acc: 0.110 - ETA: 6:27 - loss: 3.9957 - acc: 0.110 - ETA: 6:07 - loss: 3.9955 - acc: 0.110 - ETA: 5:47 - loss: 3.9966 - acc: 0.110 - ETA: 5:26 - loss: 3.9943 - acc: 0.111 - ETA: 5:06 - loss: 3.9923 - acc: 0.111 - ETA: 4:46 - loss: 3.9931 - acc: 0.111 - ETA: 4:26 - loss: 3.9927 - acc: 0.111 - ETA: 4:07 - loss: 3.9925 - acc: 0.111 - ETA: 3:47 - loss: 3.9920 - acc: 0.111 - ETA: 3:28 - loss: 3.9919 - acc: 0.111 - ETA: 3:08 - loss: 3.9914 - acc: 0.111 - ETA: 2:49 - loss: 3.9909 - acc: 0.111 - ETA: 2:30 - loss: 3.9908 - acc: 0.111 - ETA: 2:11 - loss: 3.9918 - acc: 0.111 - ETA: 1:52 - loss: 3.9932 - acc: 0.111 - ETA: 1:33 - loss: 3.9924 - acc: 0.110 - ETA: 1:14 - loss: 3.9925 - acc: 0.110 - ETA: 55s - loss: 3.9922 - acc: 0.110 - ETA: 37s - loss: 3.9916 - acc: 0.11 - ETA: 18s - loss: 3.9906 - acc: 0.11 - 6241s 934ms/step - loss: 3.9921 - acc: 0.1102 - val_loss: 4.1497 - val_acc: 0.0778\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.42404 to 4.14969, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 15:11 - loss: 2.9365 - acc: 0.50 - ETA: 18:34 - loss: 3.2169 - acc: 0.42 - ETA: 18:54 - loss: 2.9837 - acc: 0.41 - ETA: 19:22 - loss: 2.9029 - acc: 0.42 - ETA: 18:59 - loss: 2.9831 - acc: 0.37 - ETA: 18:37 - loss: 2.9716 - acc: 0.35 - ETA: 17:41 - loss: 2.9596 - acc: 0.35 - ETA: 17:01 - loss: 2.9611 - acc: 0.34 - ETA: 20:09 - loss: 2.9037 - acc: 0.35 - ETA: 21:39 - loss: 2.9087 - acc: 0.35 - ETA: 20:39 - loss: 2.9194 - acc: 0.34 - ETA: 19:40 - loss: 2.8874 - acc: 0.35 - ETA: 19:00 - loss: 2.8818 - acc: 0.36 - ETA: 18:17 - loss: 2.8999 - acc: 0.35 - ETA: 17:47 - loss: 2.8773 - acc: 0.36 - ETA: 17:19 - loss: 2.8542 - acc: 0.36 - ETA: 16:53 - loss: 2.8206 - acc: 0.36 - ETA: 16:29 - loss: 2.8269 - acc: 0.35 - ETA: 16:07 - loss: 2.8064 - acc: 0.36 - ETA: 15:50 - loss: 2.7552 - acc: 0.38 - ETA: 15:33 - loss: 2.7981 - acc: 0.36 - ETA: 15:13 - loss: 2.7735 - acc: 0.37 - ETA: 14:59 - loss: 2.7496 - acc: 0.38 - ETA: 14:48 - loss: 2.7387 - acc: 0.38 - ETA: 14:38 - loss: 2.7470 - acc: 0.38 - ETA: 14:27 - loss: 2.7560 - acc: 0.37 - ETA: 14:19 - loss: 2.7876 - acc: 0.36 - ETA: 14:06 - loss: 2.7763 - acc: 0.37 - ETA: 13:57 - loss: 2.7898 - acc: 0.36 - ETA: 13:48 - loss: 2.7998 - acc: 0.36 - ETA: 13:39 - loss: 2.8073 - acc: 0.35 - ETA: 13:30 - loss: 2.7908 - acc: 0.36 - ETA: 13:22 - loss: 2.7860 - acc: 0.35 - ETA: 13:15 - loss: 2.7762 - acc: 0.35 - ETA: 13:08 - loss: 2.7748 - acc: 0.35 - ETA: 13:01 - loss: 2.7587 - acc: 0.35 - ETA: 13:01 - loss: 2.7466 - acc: 0.35 - ETA: 13:36 - loss: 2.7324 - acc: 0.35 - ETA: 14:01 - loss: 2.7187 - acc: 0.36 - ETA: 14:02 - loss: 2.7178 - acc: 0.36 - ETA: 14:51 - loss: 2.7099 - acc: 0.36 - ETA: 14:42 - loss: 2.6969 - acc: 0.36 - ETA: 14:34 - loss: 2.7000 - acc: 0.36 - ETA: 14:36 - loss: 2.7107 - acc: 0.36 - ETA: 14:27 - loss: 2.7115 - acc: 0.36 - ETA: 14:19 - loss: 2.7126 - acc: 0.36 - ETA: 14:12 - loss: 2.7171 - acc: 0.36 - ETA: 14:03 - loss: 2.7258 - acc: 0.35 - ETA: 14:00 - loss: 2.7254 - acc: 0.35 - ETA: 13:55 - loss: 2.7248 - acc: 0.35 - ETA: 13:49 - loss: 2.7228 - acc: 0.35 - ETA: 13:46 - loss: 2.7235 - acc: 0.35 - ETA: 13:45 - loss: 2.7191 - acc: 0.35 - ETA: 13:46 - loss: 2.7221 - acc: 0.35 - ETA: 13:43 - loss: 2.7215 - acc: 0.35 - ETA: 13:48 - loss: 2.7322 - acc: 0.35 - ETA: 14:02 - loss: 2.7305 - acc: 0.35 - ETA: 13:55 - loss: 2.7295 - acc: 0.35 - ETA: 13:48 - loss: 2.7301 - acc: 0.35 - ETA: 13:41 - loss: 2.7286 - acc: 0.35 - ETA: 13:39 - loss: 2.7257 - acc: 0.35 - ETA: 13:35 - loss: 2.7232 - acc: 0.35 - ETA: 13:28 - loss: 2.7256 - acc: 0.35 - ETA: 13:22 - loss: 2.7334 - acc: 0.35 - ETA: 13:20 - loss: 2.7284 - acc: 0.35 - ETA: 13:18 - loss: 2.7320 - acc: 0.35 - ETA: 13:29 - loss: 2.7443 - acc: 0.34 - ETA: 13:22 - loss: 2.7404 - acc: 0.34 - ETA: 13:15 - loss: 2.7316 - acc: 0.35 - ETA: 13:10 - loss: 2.7242 - acc: 0.35 - ETA: 13:07 - loss: 2.7219 - acc: 0.35 - ETA: 13:01 - loss: 2.7236 - acc: 0.35 - ETA: 12:55 - loss: 2.7151 - acc: 0.35 - ETA: 12:50 - loss: 2.7067 - acc: 0.35 - ETA: 12:44 - loss: 2.7046 - acc: 0.35 - ETA: 12:39 - loss: 2.7036 - acc: 0.35 - ETA: 12:33 - loss: 2.7012 - acc: 0.35 - ETA: 12:29 - loss: 2.7100 - acc: 0.35 - ETA: 12:27 - loss: 2.7037 - acc: 0.35 - ETA: 12:21 - loss: 2.7043 - acc: 0.35 - ETA: 12:16 - loss: 2.6952 - acc: 0.35 - ETA: 12:12 - loss: 2.6971 - acc: 0.35 - ETA: 12:07 - loss: 2.6939 - acc: 0.35 - ETA: 12:06 - loss: 2.7048 - acc: 0.35 - ETA: 12:01 - loss: 2.7069 - acc: 0.35 - ETA: 11:55 - loss: 2.7073 - acc: 0.35 - ETA: 11:52 - loss: 2.7064 - acc: 0.35 - ETA: 11:47 - loss: 2.7119 - acc: 0.35 - ETA: 11:43 - loss: 2.7047 - acc: 0.35 - ETA: 11:38 - loss: 2.7019 - acc: 0.35 - ETA: 11:41 - loss: 2.6999 - acc: 0.35 - ETA: 11:38 - loss: 2.6909 - acc: 0.35 - ETA: 11:33 - loss: 2.6889 - acc: 0.35 - ETA: 11:36 - loss: 2.6910 - acc: 0.35 - ETA: 11:40 - loss: 2.6906 - acc: 0.35 - ETA: 11:43 - loss: 2.6955 - acc: 0.35 - ETA: 11:41 - loss: 2.6970 - acc: 0.35 - ETA: 11:38 - loss: 2.6999 - acc: 0.35 - ETA: 11:36 - loss: 2.6955 - acc: 0.35 - ETA: 11:35 - loss: 2.6961 - acc: 0.35 - ETA: 11:35 - loss: 2.6918 - acc: 0.35 - ETA: 11:36 - loss: 2.6880 - acc: 0.35 - ETA: 11:31 - loss: 2.6920 - acc: 0.35 - ETA: 11:27 - loss: 2.6910 - acc: 0.35 - ETA: 11:22 - loss: 2.6900 - acc: 0.35 - ETA: 11:20 - loss: 2.6888 - acc: 0.35 - ETA: 11:18 - loss: 2.6963 - acc: 0.35 - ETA: 11:18 - loss: 2.6932 - acc: 0.35 - ETA: 11:15 - loss: 2.6927 - acc: 0.35 - ETA: 11:15 - loss: 2.6970 - acc: 0.35 - ETA: 11:13 - loss: 2.7007 - acc: 0.35 - ETA: 11:12 - loss: 2.6965 - acc: 0.35 - ETA: 11:10 - loss: 2.6958 - acc: 0.35 - ETA: 11:06 - loss: 2.6919 - acc: 0.35 - ETA: 11:05 - loss: 2.6830 - acc: 0.36 - ETA: 11:02 - loss: 2.6801 - acc: 0.36 - ETA: 11:02 - loss: 2.6783 - acc: 0.36 - ETA: 11:03 - loss: 2.6861 - acc: 0.36 - ETA: 11:02 - loss: 2.6869 - acc: 0.36 - ETA: 10:59 - loss: 2.6862 - acc: 0.36 - ETA: 10:57 - loss: 2.6875 - acc: 0.36 - ETA: 10:54 - loss: 2.6850 - acc: 0.36 - ETA: 10:51 - loss: 2.6883 - acc: 0.35 - ETA: 10:50 - loss: 2.6881 - acc: 0.35 - ETA: 10:47 - loss: 2.6888 - acc: 0.35 - ETA: 10:45 - loss: 2.6903 - acc: 0.35 - ETA: 10:43 - loss: 2.6922 - acc: 0.35 - ETA: 10:42 - loss: 2.6904 - acc: 0.35 - ETA: 10:38 - loss: 2.6869 - acc: 0.35 - ETA: 10:34 - loss: 2.6861 - acc: 0.35 - ETA: 10:30 - loss: 2.6863 - acc: 0.35 - ETA: 10:26 - loss: 2.6843 - acc: 0.35 - ETA: 10:23 - loss: 2.6815 - acc: 0.35 - ETA: 10:21 - loss: 2.6793 - acc: 0.35 - ETA: 10:19 - loss: 2.6795 - acc: 0.35 - ETA: 10:21 - loss: 2.6766 - acc: 0.35 - ETA: 10:27 - loss: 2.6821 - acc: 0.35 - ETA: 10:27 - loss: 2.6822 - acc: 0.35 - ETA: 10:28 - loss: 2.6830 - acc: 0.35 - ETA: 10:27 - loss: 2.6828 - acc: 0.35 - ETA: 10:23 - loss: 2.6844 - acc: 0.35 - ETA: 10:20 - loss: 2.6815 - acc: 0.36 - ETA: 10:15 - loss: 2.6847 - acc: 0.35 - ETA: 10:11 - loss: 2.6825 - acc: 0.36 - ETA: 10:07 - loss: 2.6784 - acc: 0.36 - ETA: 10:02 - loss: 2.6832 - acc: 0.36 - ETA: 10:00 - loss: 2.6797 - acc: 0.36 - ETA: 9:57 - loss: 2.6754 - acc: 0.3618 - ETA: 10:00 - loss: 2.6737 - acc: 0.36 - ETA: 10:02 - loss: 2.6740 - acc: 0.36 - ETA: 9:58 - loss: 2.6687 - acc: 0.3636 - ETA: 9:58 - loss: 2.6723 - acc: 0.362 - ETA: 9:57 - loss: 2.6727 - acc: 0.361 - ETA: 9:55 - loss: 2.6684 - acc: 0.362 - ETA: 9:52 - loss: 2.6665 - acc: 0.362 - ETA: 9:48 - loss: 2.6651 - acc: 0.362 - ETA: 9:43 - loss: 2.6629 - acc: 0.361 - ETA: 9:39 - loss: 2.6631 - acc: 0.361 - ETA: 9:35 - loss: 2.6620 - acc: 0.361 - ETA: 9:30 - loss: 2.6635 - acc: 0.360 - ETA: 9:26 - loss: 2.6656 - acc: 0.360 - ETA: 9:22 - loss: 2.6616 - acc: 0.361 - ETA: 9:18 - loss: 2.6615 - acc: 0.361 - ETA: 9:14 - loss: 2.6538 - acc: 0.362 - ETA: 9:10 - loss: 2.6576 - acc: 0.362 - ETA: 9:05 - loss: 2.6567 - acc: 0.363 - ETA: 9:01 - loss: 2.6576 - acc: 0.363 - ETA: 8:59 - loss: 2.6546 - acc: 0.364 - ETA: 8:57 - loss: 2.6569 - acc: 0.363 - ETA: 8:54 - loss: 2.6558 - acc: 0.362 - ETA: 8:50 - loss: 2.6523 - acc: 0.363 - ETA: 8:47 - loss: 2.6537 - acc: 0.363 - ETA: 8:44 - loss: 2.6514 - acc: 0.362 - ETA: 8:40 - loss: 2.6528 - acc: 0.362 - ETA: 8:36 - loss: 2.6502 - acc: 0.362 - ETA: 8:32 - loss: 2.6476 - acc: 0.363 - ETA: 8:28 - loss: 2.6543 - acc: 0.362 - ETA: 8:25 - loss: 2.6537 - acc: 0.362 - ETA: 8:21 - loss: 2.6479 - acc: 0.364 - ETA: 8:17 - loss: 2.6434 - acc: 0.364 - ETA: 8:13 - loss: 2.6491 - acc: 0.363 - ETA: 8:10 - loss: 2.6470 - acc: 0.363 - ETA: 8:06 - loss: 2.6471 - acc: 0.363 - ETA: 8:03 - loss: 2.6464 - acc: 0.364 - ETA: 8:00 - loss: 2.6426 - acc: 0.364 - ETA: 7:57 - loss: 2.6412 - acc: 0.364 - ETA: 7:54 - loss: 2.6451 - acc: 0.364 - ETA: 7:50 - loss: 2.6417 - acc: 0.364 - ETA: 7:47 - loss: 2.6426 - acc: 0.364 - ETA: 7:44 - loss: 2.6393 - acc: 0.365 - ETA: 7:40 - loss: 2.6395 - acc: 0.365 - ETA: 7:38 - loss: 2.6360 - acc: 0.366 - ETA: 7:35 - loss: 2.6377 - acc: 0.366 - ETA: 7:32 - loss: 2.6365 - acc: 0.366 - ETA: 7:29 - loss: 2.6384 - acc: 0.365 - ETA: 7:26 - loss: 2.6362 - acc: 0.365 - ETA: 7:24 - loss: 2.6344 - acc: 0.365 - ETA: 7:22 - loss: 2.6331 - acc: 0.366 - ETA: 7:19 - loss: 2.6295 - acc: 0.367 - ETA: 7:16 - loss: 2.6317 - acc: 0.367 - ETA: 7:13 - loss: 2.6289 - acc: 0.367 - ETA: 7:11 - loss: 2.6253 - acc: 0.367 - ETA: 7:08 - loss: 2.6265 - acc: 0.367 - ETA: 7:05 - loss: 2.6261 - acc: 0.36746680/6680 [==============================] - ETA: 7:02 - loss: 2.6227 - acc: 0.368 - ETA: 7:00 - loss: 2.6198 - acc: 0.368 - ETA: 6:57 - loss: 2.6174 - acc: 0.369 - ETA: 6:54 - loss: 2.6185 - acc: 0.368 - ETA: 6:51 - loss: 2.6188 - acc: 0.368 - ETA: 6:52 - loss: 2.6192 - acc: 0.368 - ETA: 6:51 - loss: 2.6176 - acc: 0.368 - ETA: 6:47 - loss: 2.6165 - acc: 0.368 - ETA: 6:44 - loss: 2.6188 - acc: 0.368 - ETA: 6:40 - loss: 2.6187 - acc: 0.368 - ETA: 6:36 - loss: 2.6202 - acc: 0.368 - ETA: 6:32 - loss: 2.6219 - acc: 0.368 - ETA: 6:29 - loss: 2.6198 - acc: 0.369 - ETA: 6:25 - loss: 2.6189 - acc: 0.369 - ETA: 6:21 - loss: 2.6190 - acc: 0.369 - ETA: 6:17 - loss: 2.6172 - acc: 0.369 - ETA: 6:15 - loss: 2.6190 - acc: 0.368 - ETA: 6:13 - loss: 2.6171 - acc: 0.369 - ETA: 6:09 - loss: 2.6178 - acc: 0.369 - ETA: 6:06 - loss: 2.6166 - acc: 0.369 - ETA: 6:02 - loss: 2.6167 - acc: 0.369 - ETA: 5:59 - loss: 2.6151 - acc: 0.369 - ETA: 5:55 - loss: 2.6150 - acc: 0.368 - ETA: 5:51 - loss: 2.6161 - acc: 0.368 - ETA: 5:48 - loss: 2.6173 - acc: 0.368 - ETA: 5:44 - loss: 2.6202 - acc: 0.367 - ETA: 5:40 - loss: 2.6207 - acc: 0.367 - ETA: 5:37 - loss: 2.6186 - acc: 0.368 - ETA: 5:33 - loss: 2.6181 - acc: 0.368 - ETA: 5:30 - loss: 2.6179 - acc: 0.368 - ETA: 5:26 - loss: 2.6195 - acc: 0.368 - ETA: 5:22 - loss: 2.6217 - acc: 0.367 - ETA: 5:19 - loss: 2.6207 - acc: 0.367 - ETA: 5:15 - loss: 2.6198 - acc: 0.368 - ETA: 5:13 - loss: 2.6190 - acc: 0.368 - ETA: 5:10 - loss: 2.6184 - acc: 0.368 - ETA: 5:10 - loss: 2.6178 - acc: 0.368 - ETA: 5:08 - loss: 2.6174 - acc: 0.368 - ETA: 5:04 - loss: 2.6138 - acc: 0.368 - ETA: 5:01 - loss: 2.6127 - acc: 0.369 - ETA: 4:57 - loss: 2.6140 - acc: 0.369 - ETA: 4:54 - loss: 2.6103 - acc: 0.369 - ETA: 4:50 - loss: 2.6102 - acc: 0.369 - ETA: 4:46 - loss: 2.6093 - acc: 0.370 - ETA: 4:43 - loss: 2.6087 - acc: 0.369 - ETA: 4:39 - loss: 2.6075 - acc: 0.370 - ETA: 4:35 - loss: 2.6104 - acc: 0.369 - ETA: 4:32 - loss: 2.6071 - acc: 0.370 - ETA: 4:28 - loss: 2.6063 - acc: 0.370 - ETA: 4:24 - loss: 2.6055 - acc: 0.370 - ETA: 4:21 - loss: 2.6045 - acc: 0.370 - ETA: 4:17 - loss: 2.6028 - acc: 0.370 - ETA: 4:14 - loss: 2.6018 - acc: 0.370 - ETA: 4:10 - loss: 2.6023 - acc: 0.370 - ETA: 4:07 - loss: 2.6001 - acc: 0.371 - ETA: 4:03 - loss: 2.6002 - acc: 0.371 - ETA: 4:00 - loss: 2.6006 - acc: 0.370 - ETA: 3:56 - loss: 2.6015 - acc: 0.370 - ETA: 3:53 - loss: 2.6033 - acc: 0.370 - ETA: 3:50 - loss: 2.6014 - acc: 0.370 - ETA: 3:49 - loss: 2.6007 - acc: 0.370 - ETA: 3:48 - loss: 2.5982 - acc: 0.371 - ETA: 3:45 - loss: 2.5981 - acc: 0.371 - ETA: 3:42 - loss: 2.5960 - acc: 0.372 - ETA: 3:39 - loss: 2.5977 - acc: 0.371 - ETA: 3:36 - loss: 2.5953 - acc: 0.372 - ETA: 3:33 - loss: 2.5981 - acc: 0.372 - ETA: 3:30 - loss: 2.5980 - acc: 0.371 - ETA: 3:28 - loss: 2.5980 - acc: 0.372 - ETA: 3:24 - loss: 2.5975 - acc: 0.372 - ETA: 3:21 - loss: 2.5992 - acc: 0.372 - ETA: 3:18 - loss: 2.5993 - acc: 0.371 - ETA: 3:14 - loss: 2.5997 - acc: 0.372 - ETA: 3:11 - loss: 2.5983 - acc: 0.372 - ETA: 3:07 - loss: 2.5988 - acc: 0.372 - ETA: 3:04 - loss: 2.5982 - acc: 0.372 - ETA: 3:00 - loss: 2.5983 - acc: 0.372 - ETA: 2:57 - loss: 2.5975 - acc: 0.372 - ETA: 2:54 - loss: 2.5973 - acc: 0.371 - ETA: 2:50 - loss: 2.5997 - acc: 0.371 - ETA: 2:47 - loss: 2.6016 - acc: 0.370 - ETA: 2:43 - loss: 2.6006 - acc: 0.370 - ETA: 2:40 - loss: 2.5993 - acc: 0.371 - ETA: 2:37 - loss: 2.5984 - acc: 0.371 - ETA: 2:33 - loss: 2.5980 - acc: 0.371 - ETA: 2:30 - loss: 2.5982 - acc: 0.371 - ETA: 2:27 - loss: 2.5979 - acc: 0.371 - ETA: 2:24 - loss: 2.5987 - acc: 0.371 - ETA: 2:21 - loss: 2.6011 - acc: 0.371 - ETA: 2:18 - loss: 2.6010 - acc: 0.371 - ETA: 2:15 - loss: 2.6006 - acc: 0.371 - ETA: 2:11 - loss: 2.5990 - acc: 0.372 - ETA: 2:08 - loss: 2.5996 - acc: 0.371 - ETA: 2:04 - loss: 2.5985 - acc: 0.372 - ETA: 2:01 - loss: 2.5986 - acc: 0.372 - ETA: 1:57 - loss: 2.5991 - acc: 0.372 - ETA: 1:54 - loss: 2.5992 - acc: 0.371 - ETA: 1:51 - loss: 2.5991 - acc: 0.371 - ETA: 1:47 - loss: 2.6008 - acc: 0.371 - ETA: 1:44 - loss: 2.6030 - acc: 0.370 - ETA: 1:41 - loss: 2.6024 - acc: 0.370 - ETA: 1:38 - loss: 2.6021 - acc: 0.370 - ETA: 1:35 - loss: 2.6018 - acc: 0.370 - ETA: 1:31 - loss: 2.6007 - acc: 0.370 - ETA: 1:28 - loss: 2.6009 - acc: 0.370 - ETA: 1:24 - loss: 2.6012 - acc: 0.370 - ETA: 1:21 - loss: 2.6016 - acc: 0.370 - ETA: 1:17 - loss: 2.6016 - acc: 0.370 - ETA: 1:14 - loss: 2.6024 - acc: 0.370 - ETA: 1:10 - loss: 2.6040 - acc: 0.369 - ETA: 1:07 - loss: 2.6042 - acc: 0.369 - ETA: 1:04 - loss: 2.6029 - acc: 0.369 - ETA: 1:00 - loss: 2.6039 - acc: 0.369 - ETA: 56s - loss: 2.6056 - acc: 0.369 - ETA: 53s - loss: 2.6050 - acc: 0.36 - ETA: 49s - loss: 2.6063 - acc: 0.36 - ETA: 46s - loss: 2.6080 - acc: 0.36 - ETA: 42s - loss: 2.6085 - acc: 0.36 - ETA: 38s - loss: 2.6090 - acc: 0.36 - ETA: 35s - loss: 2.6086 - acc: 0.36 - ETA: 31s - loss: 2.6081 - acc: 0.36 - ETA: 28s - loss: 2.6075 - acc: 0.36 - ETA: 24s - loss: 2.6082 - acc: 0.36 - ETA: 21s - loss: 2.6075 - acc: 0.36 - ETA: 17s - loss: 2.6070 - acc: 0.36 - ETA: 14s - loss: 2.6058 - acc: 0.36 - ETA: 10s - loss: 2.6044 - acc: 0.36 - ETA: 7s - loss: 2.6040 - acc: 0.3688 - ETA: 3s - loss: 2.6044 - acc: 0.368 - 1235s 185ms/step - loss: 2.6034 - acc: 0.3687 - val_loss: 4.3801 - val_acc: 0.1198\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 20:08 - loss: 1.0914 - acc: 0.80 - ETA: 17:35 - loss: 0.8028 - acc: 0.90 - ETA: 17:39 - loss: 0.6675 - acc: 0.91 - ETA: 17:29 - loss: 0.6839 - acc: 0.91 - ETA: 16:36 - loss: 0.6301 - acc: 0.91 - ETA: 16:33 - loss: 0.6739 - acc: 0.89 - ETA: 16:44 - loss: 0.6564 - acc: 0.88 - ETA: 16:42 - loss: 0.6692 - acc: 0.88 - ETA: 16:24 - loss: 0.6263 - acc: 0.89 - ETA: 16:05 - loss: 0.6453 - acc: 0.88 - ETA: 18:06 - loss: 0.6086 - acc: 0.89 - ETA: 17:35 - loss: 0.6268 - acc: 0.89 - ETA: 19:55 - loss: 0.6166 - acc: 0.89 - ETA: 19:48 - loss: 0.5932 - acc: 0.89 - ETA: 19:15 - loss: 0.6122 - acc: 0.89 - ETA: 18:43 - loss: 0.6287 - acc: 0.88 - ETA: 18:17 - loss: 0.6299 - acc: 0.88 - ETA: 18:04 - loss: 0.6497 - acc: 0.87 - ETA: 18:10 - loss: 0.6278 - acc: 0.87 - ETA: 18:07 - loss: 0.6108 - acc: 0.88 - ETA: 17:45 - loss: 0.6081 - acc: 0.88 - ETA: 17:25 - loss: 0.6073 - acc: 0.87 - ETA: 17:13 - loss: 0.5933 - acc: 0.88 - ETA: 17:06 - loss: 0.6048 - acc: 0.88 - ETA: 16:53 - loss: 0.6066 - acc: 0.87 - ETA: 16:35 - loss: 0.6006 - acc: 0.87 - ETA: 16:18 - loss: 0.5930 - acc: 0.87 - ETA: 16:01 - loss: 0.5989 - acc: 0.87 - ETA: 15:46 - loss: 0.6247 - acc: 0.86 - ETA: 15:38 - loss: 0.6337 - acc: 0.86 - ETA: 15:35 - loss: 0.6195 - acc: 0.86 - ETA: 15:46 - loss: 0.6111 - acc: 0.86 - ETA: 15:33 - loss: 0.6068 - acc: 0.86 - ETA: 15:23 - loss: 0.6050 - acc: 0.86 - ETA: 15:11 - loss: 0.5962 - acc: 0.87 - ETA: 15:01 - loss: 0.5997 - acc: 0.86 - ETA: 15:37 - loss: 0.6019 - acc: 0.87 - ETA: 15:32 - loss: 0.5983 - acc: 0.87 - ETA: 15:34 - loss: 0.6036 - acc: 0.86 - ETA: 15:22 - loss: 0.5933 - acc: 0.87 - ETA: 15:09 - loss: 0.5865 - acc: 0.87 - ETA: 15:00 - loss: 0.5898 - acc: 0.86 - ETA: 14:59 - loss: 0.5890 - acc: 0.86 - ETA: 14:51 - loss: 0.5864 - acc: 0.87 - ETA: 14:42 - loss: 0.5791 - acc: 0.87 - ETA: 14:32 - loss: 0.5811 - acc: 0.87 - ETA: 14:30 - loss: 0.5881 - acc: 0.87 - ETA: 14:25 - loss: 0.5858 - acc: 0.87 - ETA: 14:20 - loss: 0.5896 - acc: 0.87 - ETA: 14:15 - loss: 0.5884 - acc: 0.87 - ETA: 14:06 - loss: 0.5866 - acc: 0.87 - ETA: 14:18 - loss: 0.5868 - acc: 0.87 - ETA: 14:09 - loss: 0.5836 - acc: 0.87 - ETA: 14:01 - loss: 0.5889 - acc: 0.86 - ETA: 13:56 - loss: 0.6025 - acc: 0.86 - ETA: 13:50 - loss: 0.6011 - acc: 0.86 - ETA: 13:47 - loss: 0.5961 - acc: 0.87 - ETA: 13:44 - loss: 0.5943 - acc: 0.86 - ETA: 13:38 - loss: 0.5913 - acc: 0.87 - ETA: 13:30 - loss: 0.5854 - acc: 0.87 - ETA: 13:25 - loss: 0.5823 - acc: 0.87 - ETA: 13:18 - loss: 0.5800 - acc: 0.87 - ETA: 13:15 - loss: 0.5806 - acc: 0.87 - ETA: 13:10 - loss: 0.5821 - acc: 0.87 - ETA: 13:03 - loss: 0.5818 - acc: 0.87 - ETA: 12:59 - loss: 0.5802 - acc: 0.87 - ETA: 12:52 - loss: 0.5787 - acc: 0.87 - ETA: 12:47 - loss: 0.5794 - acc: 0.87 - ETA: 12:42 - loss: 0.5754 - acc: 0.87 - ETA: 12:38 - loss: 0.5799 - acc: 0.86 - ETA: 12:33 - loss: 0.5781 - acc: 0.87 - ETA: 12:28 - loss: 0.5760 - acc: 0.87 - ETA: 12:24 - loss: 0.5786 - acc: 0.86 - ETA: 12:20 - loss: 0.5745 - acc: 0.86 - ETA: 12:15 - loss: 0.5708 - acc: 0.87 - ETA: 12:10 - loss: 0.5703 - acc: 0.87 - ETA: 12:06 - loss: 0.5662 - acc: 0.87 - ETA: 12:01 - loss: 0.5622 - acc: 0.87 - ETA: 11:59 - loss: 0.5625 - acc: 0.87 - ETA: 12:00 - loss: 0.5586 - acc: 0.87 - ETA: 11:56 - loss: 0.5564 - acc: 0.87 - ETA: 11:53 - loss: 0.5557 - acc: 0.87 - ETA: 11:48 - loss: 0.5515 - acc: 0.87 - ETA: 11:44 - loss: 0.5486 - acc: 0.87 - ETA: 11:40 - loss: 0.5487 - acc: 0.87 - ETA: 11:36 - loss: 0.5486 - acc: 0.87 - ETA: 11:31 - loss: 0.5483 - acc: 0.87 - ETA: 11:28 - loss: 0.5477 - acc: 0.87 - ETA: 11:25 - loss: 0.5451 - acc: 0.87 - ETA: 11:21 - loss: 0.5478 - acc: 0.87 - ETA: 11:17 - loss: 0.5479 - acc: 0.87 - ETA: 11:13 - loss: 0.5490 - acc: 0.87 - ETA: 11:09 - loss: 0.5475 - acc: 0.87 - ETA: 11:06 - loss: 0.5444 - acc: 0.87 - ETA: 11:05 - loss: 0.5416 - acc: 0.87 - ETA: 11:04 - loss: 0.5384 - acc: 0.87 - ETA: 11:02 - loss: 0.5390 - acc: 0.87 - ETA: 11:02 - loss: 0.5377 - acc: 0.87 - ETA: 11:01 - loss: 0.5339 - acc: 0.87 - ETA: 10:58 - loss: 0.5338 - acc: 0.87 - ETA: 10:54 - loss: 0.5348 - acc: 0.87 - ETA: 10:49 - loss: 0.5328 - acc: 0.87 - ETA: 10:46 - loss: 0.5343 - acc: 0.87 - ETA: 10:44 - loss: 0.5312 - acc: 0.87 - ETA: 10:40 - loss: 0.5299 - acc: 0.87 - ETA: 10:36 - loss: 0.5332 - acc: 0.87 - ETA: 10:32 - loss: 0.5319 - acc: 0.87 - ETA: 10:28 - loss: 0.5351 - acc: 0.87 - ETA: 10:24 - loss: 0.5353 - acc: 0.87 - ETA: 10:23 - loss: 0.5368 - acc: 0.87 - ETA: 10:32 - loss: 0.5406 - acc: 0.87 - ETA: 10:28 - loss: 0.5404 - acc: 0.87 - ETA: 10:24 - loss: 0.5404 - acc: 0.87 - ETA: 10:21 - loss: 0.5414 - acc: 0.87 - ETA: 10:17 - loss: 0.5410 - acc: 0.87 - ETA: 10:13 - loss: 0.5424 - acc: 0.87 - ETA: 10:09 - loss: 0.5489 - acc: 0.87 - ETA: 10:07 - loss: 0.5500 - acc: 0.86 - ETA: 10:04 - loss: 0.5485 - acc: 0.87 - ETA: 10:00 - loss: 0.5489 - acc: 0.87 - ETA: 9:57 - loss: 0.5476 - acc: 0.8715 - ETA: 9:53 - loss: 0.5456 - acc: 0.871 - ETA: 9:50 - loss: 0.5472 - acc: 0.870 - ETA: 9:47 - loss: 0.5446 - acc: 0.871 - ETA: 9:43 - loss: 0.5421 - acc: 0.871 - ETA: 9:41 - loss: 0.5436 - acc: 0.871 - ETA: 9:38 - loss: 0.5444 - acc: 0.870 - ETA: 9:35 - loss: 0.5522 - acc: 0.869 - ETA: 9:32 - loss: 0.5545 - acc: 0.869 - ETA: 9:29 - loss: 0.5589 - acc: 0.868 - ETA: 9:28 - loss: 0.5590 - acc: 0.867 - ETA: 9:25 - loss: 0.5581 - acc: 0.867 - ETA: 9:21 - loss: 0.5630 - acc: 0.866 - ETA: 9:17 - loss: 0.5611 - acc: 0.867 - ETA: 9:14 - loss: 0.5629 - acc: 0.866 - ETA: 9:11 - loss: 0.5638 - acc: 0.866 - ETA: 9:07 - loss: 0.5624 - acc: 0.866 - ETA: 9:04 - loss: 0.5638 - acc: 0.865 - ETA: 9:00 - loss: 0.5671 - acc: 0.864 - ETA: 8:56 - loss: 0.5696 - acc: 0.863 - ETA: 8:58 - loss: 0.5699 - acc: 0.862 - ETA: 8:55 - loss: 0.5689 - acc: 0.862 - ETA: 8:52 - loss: 0.5669 - acc: 0.862 - ETA: 8:48 - loss: 0.5642 - acc: 0.863 - ETA: 8:46 - loss: 0.5627 - acc: 0.864 - ETA: 8:43 - loss: 0.5615 - acc: 0.865 - ETA: 8:40 - loss: 0.5633 - acc: 0.865 - ETA: 8:37 - loss: 0.5611 - acc: 0.865 - ETA: 8:35 - loss: 0.5596 - acc: 0.866 - ETA: 8:33 - loss: 0.5570 - acc: 0.867 - ETA: 8:32 - loss: 0.5569 - acc: 0.866 - ETA: 8:29 - loss: 0.5568 - acc: 0.866 - ETA: 8:26 - loss: 0.5588 - acc: 0.865 - ETA: 8:22 - loss: 0.5582 - acc: 0.865 - ETA: 8:19 - loss: 0.5630 - acc: 0.863 - ETA: 8:17 - loss: 0.5646 - acc: 0.862 - ETA: 8:14 - loss: 0.5645 - acc: 0.862 - ETA: 8:11 - loss: 0.5634 - acc: 0.862 - ETA: 8:08 - loss: 0.5657 - acc: 0.861 - ETA: 8:08 - loss: 0.5678 - acc: 0.860 - ETA: 8:10 - loss: 0.5659 - acc: 0.861 - ETA: 8:10 - loss: 0.5673 - acc: 0.860 - ETA: 8:09 - loss: 0.5691 - acc: 0.859 - ETA: 8:08 - loss: 0.5677 - acc: 0.860 - ETA: 8:08 - loss: 0.5691 - acc: 0.859 - ETA: 8:07 - loss: 0.5700 - acc: 0.859 - ETA: 8:06 - loss: 0.5695 - acc: 0.859 - ETA: 8:04 - loss: 0.5706 - acc: 0.859 - ETA: 8:06 - loss: 0.5684 - acc: 0.860 - ETA: 8:02 - loss: 0.5667 - acc: 0.860 - ETA: 7:59 - loss: 0.5673 - acc: 0.860 - ETA: 7:57 - loss: 0.5654 - acc: 0.860 - ETA: 7:54 - loss: 0.5649 - acc: 0.860 - ETA: 7:50 - loss: 0.5643 - acc: 0.860 - ETA: 7:47 - loss: 0.5667 - acc: 0.860 - ETA: 7:45 - loss: 0.5669 - acc: 0.860 - ETA: 7:41 - loss: 0.5663 - acc: 0.860 - ETA: 7:38 - loss: 0.5652 - acc: 0.860 - ETA: 7:34 - loss: 0.5628 - acc: 0.861 - ETA: 7:32 - loss: 0.5636 - acc: 0.860 - ETA: 7:30 - loss: 0.5646 - acc: 0.860 - ETA: 7:27 - loss: 0.5639 - acc: 0.860 - ETA: 7:24 - loss: 0.5623 - acc: 0.861 - ETA: 7:21 - loss: 0.5627 - acc: 0.861 - ETA: 7:17 - loss: 0.5618 - acc: 0.861 - ETA: 7:14 - loss: 0.5609 - acc: 0.861 - ETA: 7:11 - loss: 0.5633 - acc: 0.861 - ETA: 7:07 - loss: 0.5673 - acc: 0.860 - ETA: 7:04 - loss: 0.5659 - acc: 0.860 - ETA: 7:01 - loss: 0.5675 - acc: 0.859 - ETA: 6:57 - loss: 0.5665 - acc: 0.859 - ETA: 6:55 - loss: 0.5661 - acc: 0.859 - ETA: 6:51 - loss: 0.5659 - acc: 0.859 - ETA: 6:55 - loss: 0.5672 - acc: 0.859 - ETA: 6:53 - loss: 0.5680 - acc: 0.858 - ETA: 6:52 - loss: 0.5675 - acc: 0.858 - ETA: 6:53 - loss: 0.5666 - acc: 0.858 - ETA: 6:54 - loss: 0.5699 - acc: 0.857 - ETA: 6:52 - loss: 0.5677 - acc: 0.858 - ETA: 6:49 - loss: 0.5677 - acc: 0.858 - ETA: 6:46 - loss: 0.5662 - acc: 0.858 - ETA: 6:43 - loss: 0.5676 - acc: 0.857 - ETA: 6:39 - loss: 0.5675 - acc: 0.857 - ETA: 6:36 - loss: 0.5691 - acc: 0.8574"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6:33 - loss: 0.5674 - acc: 0.857 - ETA: 6:30 - loss: 0.5671 - acc: 0.857 - ETA: 6:26 - loss: 0.5692 - acc: 0.857 - ETA: 6:23 - loss: 0.5701 - acc: 0.857 - ETA: 6:20 - loss: 0.5716 - acc: 0.856 - ETA: 6:17 - loss: 0.5732 - acc: 0.856 - ETA: 6:14 - loss: 0.5726 - acc: 0.856 - ETA: 6:12 - loss: 0.5723 - acc: 0.856 - ETA: 6:08 - loss: 0.5720 - acc: 0.856 - ETA: 6:05 - loss: 0.5703 - acc: 0.856 - ETA: 6:02 - loss: 0.5717 - acc: 0.855 - ETA: 5:59 - loss: 0.5720 - acc: 0.855 - ETA: 5:55 - loss: 0.5731 - acc: 0.855 - ETA: 5:53 - loss: 0.5714 - acc: 0.855 - ETA: 5:52 - loss: 0.5716 - acc: 0.856 - ETA: 5:50 - loss: 0.5720 - acc: 0.855 - ETA: 5:47 - loss: 0.5713 - acc: 0.855 - ETA: 5:44 - loss: 0.5715 - acc: 0.855 - ETA: 5:40 - loss: 0.5719 - acc: 0.855 - ETA: 5:38 - loss: 0.5712 - acc: 0.856 - ETA: 5:35 - loss: 0.5739 - acc: 0.855 - ETA: 5:32 - loss: 0.5735 - acc: 0.855 - ETA: 5:29 - loss: 0.5741 - acc: 0.855 - ETA: 5:26 - loss: 0.5749 - acc: 0.855 - ETA: 5:22 - loss: 0.5740 - acc: 0.855 - ETA: 5:19 - loss: 0.5730 - acc: 0.855 - ETA: 5:16 - loss: 0.5747 - acc: 0.854 - ETA: 5:13 - loss: 0.5743 - acc: 0.854 - ETA: 5:10 - loss: 0.5728 - acc: 0.854 - ETA: 5:08 - loss: 0.5717 - acc: 0.855 - ETA: 5:05 - loss: 0.5730 - acc: 0.854 - ETA: 5:02 - loss: 0.5723 - acc: 0.854 - ETA: 4:58 - loss: 0.5741 - acc: 0.854 - ETA: 4:55 - loss: 0.5752 - acc: 0.853 - ETA: 4:52 - loss: 0.5743 - acc: 0.854 - ETA: 4:48 - loss: 0.5730 - acc: 0.854 - ETA: 4:45 - loss: 0.5716 - acc: 0.854 - ETA: 4:42 - loss: 0.5727 - acc: 0.853 - ETA: 4:39 - loss: 0.5724 - acc: 0.853 - ETA: 4:36 - loss: 0.5727 - acc: 0.853 - ETA: 4:33 - loss: 0.5753 - acc: 0.853 - ETA: 4:31 - loss: 0.5766 - acc: 0.853 - ETA: 4:28 - loss: 0.5771 - acc: 0.853 - ETA: 4:25 - loss: 0.5766 - acc: 0.853 - ETA: 4:22 - loss: 0.5766 - acc: 0.853 - ETA: 4:18 - loss: 0.5794 - acc: 0.852 - ETA: 4:15 - loss: 0.5794 - acc: 0.852 - ETA: 4:12 - loss: 0.5792 - acc: 0.852 - ETA: 4:09 - loss: 0.5793 - acc: 0.852 - ETA: 4:05 - loss: 0.5811 - acc: 0.852 - ETA: 4:02 - loss: 0.5813 - acc: 0.852 - ETA: 3:59 - loss: 0.5816 - acc: 0.851 - ETA: 3:56 - loss: 0.5802 - acc: 0.852 - ETA: 3:53 - loss: 0.5789 - acc: 0.852 - ETA: 3:50 - loss: 0.5805 - acc: 0.852 - ETA: 3:48 - loss: 0.5803 - acc: 0.852 - ETA: 3:45 - loss: 0.5806 - acc: 0.851 - ETA: 3:43 - loss: 0.5808 - acc: 0.851 - ETA: 3:40 - loss: 0.5814 - acc: 0.851 - ETA: 3:37 - loss: 0.5803 - acc: 0.851 - ETA: 3:34 - loss: 0.5810 - acc: 0.851 - ETA: 3:31 - loss: 0.5811 - acc: 0.851 - ETA: 3:28 - loss: 0.5820 - acc: 0.850 - ETA: 3:24 - loss: 0.5816 - acc: 0.850 - ETA: 3:21 - loss: 0.5836 - acc: 0.850 - ETA: 3:18 - loss: 0.5852 - acc: 0.849 - ETA: 3:15 - loss: 0.5869 - acc: 0.850 - ETA: 3:12 - loss: 0.5868 - acc: 0.849 - ETA: 3:09 - loss: 0.5882 - acc: 0.849 - ETA: 3:06 - loss: 0.5893 - acc: 0.849 - ETA: 3:04 - loss: 0.5899 - acc: 0.849 - ETA: 3:00 - loss: 0.5911 - acc: 0.849 - ETA: 2:57 - loss: 0.5902 - acc: 0.849 - ETA: 2:54 - loss: 0.5894 - acc: 0.849 - ETA: 2:51 - loss: 0.5902 - acc: 0.849 - ETA: 2:48 - loss: 0.5924 - acc: 0.848 - ETA: 2:45 - loss: 0.5917 - acc: 0.848 - ETA: 2:42 - loss: 0.5918 - acc: 0.848 - ETA: 2:39 - loss: 0.5914 - acc: 0.848 - ETA: 2:36 - loss: 0.5908 - acc: 0.848 - ETA: 2:33 - loss: 0.5910 - acc: 0.848 - ETA: 2:30 - loss: 0.5906 - acc: 0.849 - ETA: 2:27 - loss: 0.5900 - acc: 0.849 - ETA: 2:24 - loss: 0.5898 - acc: 0.849 - ETA: 2:21 - loss: 0.5902 - acc: 0.849 - ETA: 2:17 - loss: 0.5894 - acc: 0.849 - ETA: 2:14 - loss: 0.5882 - acc: 0.849 - ETA: 2:11 - loss: 0.5907 - acc: 0.849 - ETA: 2:08 - loss: 0.5893 - acc: 0.849 - ETA: 2:05 - loss: 0.5885 - acc: 0.849 - ETA: 2:01 - loss: 0.5879 - acc: 0.850 - ETA: 1:58 - loss: 0.5889 - acc: 0.849 - ETA: 1:55 - loss: 0.5908 - acc: 0.849 - ETA: 1:52 - loss: 0.5898 - acc: 0.849 - ETA: 1:49 - loss: 0.5895 - acc: 0.850 - ETA: 1:46 - loss: 0.5885 - acc: 0.850 - ETA: 1:43 - loss: 0.5897 - acc: 0.850 - ETA: 1:39 - loss: 0.5896 - acc: 0.849 - ETA: 1:36 - loss: 0.5909 - acc: 0.849 - ETA: 1:33 - loss: 0.5915 - acc: 0.849 - ETA: 1:30 - loss: 0.5931 - acc: 0.849 - ETA: 1:27 - loss: 0.5918 - acc: 0.849 - ETA: 1:24 - loss: 0.5920 - acc: 0.849 - ETA: 1:21 - loss: 0.5915 - acc: 0.849 - ETA: 1:18 - loss: 0.5917 - acc: 0.849 - ETA: 1:14 - loss: 0.5916 - acc: 0.849 - ETA: 1:11 - loss: 0.5919 - acc: 0.849 - ETA: 1:08 - loss: 0.5911 - acc: 0.849 - ETA: 1:05 - loss: 0.5904 - acc: 0.850 - ETA: 1:02 - loss: 0.5909 - acc: 0.849 - ETA: 59s - loss: 0.5947 - acc: 0.849 - ETA: 56s - loss: 0.5940 - acc: 0.84 - ETA: 53s - loss: 0.5936 - acc: 0.84 - ETA: 49s - loss: 0.5951 - acc: 0.84 - ETA: 46s - loss: 0.5962 - acc: 0.84 - ETA: 43s - loss: 0.5966 - acc: 0.84 - ETA: 40s - loss: 0.5964 - acc: 0.84 - ETA: 37s - loss: 0.5959 - acc: 0.84 - ETA: 34s - loss: 0.5959 - acc: 0.84 - ETA: 31s - loss: 0.5972 - acc: 0.84 - ETA: 27s - loss: 0.5980 - acc: 0.84 - ETA: 24s - loss: 0.5995 - acc: 0.84 - ETA: 21s - loss: 0.6016 - acc: 0.84 - ETA: 18s - loss: 0.6015 - acc: 0.84 - ETA: 15s - loss: 0.6029 - acc: 0.84 - ETA: 12s - loss: 0.6034 - acc: 0.84 - ETA: 9s - loss: 0.6038 - acc: 0.8459 - ETA: 6s - loss: 0.6028 - acc: 0.846 - ETA: 3s - loss: 0.6022 - acc: 0.846 - 1099s 165ms/step - loss: 0.6019 - acc: 0.8467 - val_loss: 6.1524 - val_acc: 0.0862\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 13:53 - loss: 0.0325 - acc: 1.00 - ETA: 16:04 - loss: 0.0254 - acc: 1.00 - ETA: 17:06 - loss: 0.0233 - acc: 1.00 - ETA: 16:32 - loss: 0.0226 - acc: 1.00 - ETA: 16:07 - loss: 0.0220 - acc: 1.00 - ETA: 15:05 - loss: 0.0195 - acc: 1.00 - ETA: 14:31 - loss: 0.0190 - acc: 1.00 - ETA: 16:55 - loss: 0.0186 - acc: 1.00 - ETA: 16:35 - loss: 0.0177 - acc: 1.00 - ETA: 16:01 - loss: 0.0179 - acc: 1.00 - ETA: 15:26 - loss: 0.0239 - acc: 0.99 - ETA: 15:02 - loss: 0.0243 - acc: 0.99 - ETA: 14:37 - loss: 0.0250 - acc: 0.99 - ETA: 14:20 - loss: 0.0731 - acc: 0.98 - ETA: 14:07 - loss: 0.0692 - acc: 0.99 - ETA: 13:54 - loss: 0.0686 - acc: 0.99 - ETA: 13:39 - loss: 0.0681 - acc: 0.98 - ETA: 13:24 - loss: 0.0662 - acc: 0.98 - ETA: 13:12 - loss: 0.0652 - acc: 0.98 - ETA: 13:16 - loss: 0.0636 - acc: 0.99 - ETA: 13:07 - loss: 0.0617 - acc: 0.99 - ETA: 13:02 - loss: 0.0606 - acc: 0.99 - ETA: 12:55 - loss: 0.0581 - acc: 0.99 - ETA: 12:45 - loss: 0.0586 - acc: 0.98 - ETA: 12:36 - loss: 0.0582 - acc: 0.99 - ETA: 13:02 - loss: 0.0564 - acc: 0.99 - ETA: 12:52 - loss: 0.0553 - acc: 0.99 - ETA: 12:42 - loss: 0.0538 - acc: 0.99 - ETA: 12:35 - loss: 0.0527 - acc: 0.99 - ETA: 12:29 - loss: 0.0513 - acc: 0.99 - ETA: 12:25 - loss: 0.0509 - acc: 0.99 - ETA: 12:50 - loss: 0.0498 - acc: 0.99 - ETA: 12:54 - loss: 0.0485 - acc: 0.99 - ETA: 12:45 - loss: 0.0478 - acc: 0.99 - ETA: 12:38 - loss: 0.0467 - acc: 0.99 - ETA: 12:30 - loss: 0.0458 - acc: 0.99 - ETA: 12:22 - loss: 0.0451 - acc: 0.99 - ETA: 12:16 - loss: 0.0442 - acc: 0.99 - ETA: 12:10 - loss: 0.0441 - acc: 0.99 - ETA: 12:04 - loss: 0.0434 - acc: 0.99 - ETA: 11:59 - loss: 0.0427 - acc: 0.99 - ETA: 11:55 - loss: 0.0431 - acc: 0.99 - ETA: 11:47 - loss: 0.0463 - acc: 0.99 - ETA: 11:43 - loss: 0.0455 - acc: 0.99 - ETA: 11:38 - loss: 0.0456 - acc: 0.99 - ETA: 11:33 - loss: 0.0452 - acc: 0.99 - ETA: 11:32 - loss: 0.0453 - acc: 0.99 - ETA: 11:32 - loss: 0.0605 - acc: 0.98 - ETA: 11:29 - loss: 0.0610 - acc: 0.98 - ETA: 11:30 - loss: 0.0611 - acc: 0.98 - ETA: 11:27 - loss: 0.0609 - acc: 0.98 - ETA: 11:23 - loss: 0.0601 - acc: 0.98 - ETA: 11:17 - loss: 0.0593 - acc: 0.98 - ETA: 11:13 - loss: 0.0583 - acc: 0.98 - ETA: 11:09 - loss: 0.0574 - acc: 0.98 - ETA: 11:08 - loss: 0.0573 - acc: 0.98 - ETA: 11:03 - loss: 0.0571 - acc: 0.98 - ETA: 11:00 - loss: 0.0573 - acc: 0.98 - ETA: 10:55 - loss: 0.0567 - acc: 0.98 - ETA: 10:51 - loss: 0.0558 - acc: 0.98 - ETA: 10:47 - loss: 0.0560 - acc: 0.98 - ETA: 10:43 - loss: 0.0560 - acc: 0.98 - ETA: 10:39 - loss: 0.0558 - acc: 0.98 - ETA: 10:36 - loss: 0.0586 - acc: 0.98 - ETA: 10:32 - loss: 0.0582 - acc: 0.98 - ETA: 10:28 - loss: 0.0574 - acc: 0.98 - ETA: 10:24 - loss: 0.0573 - acc: 0.98 - ETA: 10:21 - loss: 0.0686 - acc: 0.98 - ETA: 10:18 - loss: 0.0799 - acc: 0.98 - ETA: 10:14 - loss: 0.0796 - acc: 0.98 - ETA: 10:29 - loss: 0.0786 - acc: 0.98 - ETA: 10:29 - loss: 0.0777 - acc: 0.98 - ETA: 10:28 - loss: 0.0823 - acc: 0.98 - ETA: 10:32 - loss: 0.0818 - acc: 0.98 - ETA: 10:32 - loss: 0.0845 - acc: 0.98 - ETA: 10:35 - loss: 0.0859 - acc: 0.98 - ETA: 10:41 - loss: 0.0910 - acc: 0.98 - ETA: 10:53 - loss: 0.0922 - acc: 0.98 - ETA: 11:02 - loss: 0.0917 - acc: 0.98 - ETA: 11:04 - loss: 0.0909 - acc: 0.98 - ETA: 11:12 - loss: 0.0905 - acc: 0.98 - ETA: 11:22 - loss: 0.0958 - acc: 0.98 - ETA: 11:27 - loss: 0.0948 - acc: 0.98 - ETA: 11:35 - loss: 0.0939 - acc: 0.98 - ETA: 11:39 - loss: 0.0933 - acc: 0.98 - ETA: 11:35 - loss: 0.0924 - acc: 0.98 - ETA: 11:30 - loss: 0.0935 - acc: 0.98 - ETA: 11:27 - loss: 0.0934 - acc: 0.98 - ETA: 11:24 - loss: 0.0925 - acc: 0.98 - ETA: 11:21 - loss: 0.0915 - acc: 0.98 - ETA: 11:20 - loss: 0.0906 - acc: 0.98 - ETA: 11:17 - loss: 0.0899 - acc: 0.98 - ETA: 11:21 - loss: 0.0892 - acc: 0.98 - ETA: 11:18 - loss: 0.0884 - acc: 0.98 - ETA: 11:14 - loss: 0.0875 - acc: 0.98 - ETA: 11:09 - loss: 0.0867 - acc: 0.98 - ETA: 11:05 - loss: 0.0870 - acc: 0.98 - ETA: 11:02 - loss: 0.0885 - acc: 0.98 - ETA: 11:05 - loss: 0.0887 - acc: 0.98 - ETA: 11:08 - loss: 0.0903 - acc: 0.98 - ETA: 11:09 - loss: 0.0910 - acc: 0.98 - ETA: 11:11 - loss: 0.0908 - acc: 0.98 - ETA: 11:13 - loss: 0.0917 - acc: 0.98 - ETA: 11:16 - loss: 0.0914 - acc: 0.98 - ETA: 11:18 - loss: 0.0939 - acc: 0.98 - ETA: 11:19 - loss: 0.0946 - acc: 0.98 - ETA: 11:22 - loss: 0.0941 - acc: 0.98 - ETA: 11:18 - loss: 0.0943 - acc: 0.98 - ETA: 11:13 - loss: 0.0949 - acc: 0.97 - ETA: 11:21 - loss: 0.0946 - acc: 0.97 - ETA: 11:27 - loss: 0.0952 - acc: 0.97 - ETA: 11:33 - loss: 0.0947 - acc: 0.97 - ETA: 11:34 - loss: 0.0945 - acc: 0.97 - ETA: 11:34 - loss: 0.0944 - acc: 0.97 - ETA: 11:32 - loss: 0.0956 - acc: 0.97 - ETA: 11:28 - loss: 0.0952 - acc: 0.97 - ETA: 11:23 - loss: 0.0945 - acc: 0.97 - ETA: 11:18 - loss: 0.0939 - acc: 0.97 - ETA: 11:15 - loss: 0.0933 - acc: 0.97 - ETA: 11:11 - loss: 0.0933 - acc: 0.97 - ETA: 11:06 - loss: 0.0927 - acc: 0.97 - ETA: 11:01 - loss: 0.0930 - acc: 0.97 - ETA: 11:01 - loss: 0.0927 - acc: 0.97 - ETA: 10:57 - loss: 0.0921 - acc: 0.97 - ETA: 10:53 - loss: 0.0916 - acc: 0.97 - ETA: 10:50 - loss: 0.0910 - acc: 0.97 - ETA: 10:48 - loss: 0.0906 - acc: 0.97 - ETA: 10:44 - loss: 0.0900 - acc: 0.98 - ETA: 10:39 - loss: 0.0895 - acc: 0.98 - ETA: 10:35 - loss: 0.0890 - acc: 0.98 - ETA: 10:30 - loss: 0.0884 - acc: 0.98 - ETA: 10:26 - loss: 0.0878 - acc: 0.98 - ETA: 10:21 - loss: 0.0874 - acc: 0.98 - ETA: 10:17 - loss: 0.0869 - acc: 0.98 - ETA: 10:14 - loss: 0.0885 - acc: 0.98 - ETA: 10:11 - loss: 0.0987 - acc: 0.97 - ETA: 10:07 - loss: 0.0981 - acc: 0.97 - ETA: 10:04 - loss: 0.0976 - acc: 0.98 - ETA: 10:02 - loss: 0.0970 - acc: 0.98 - ETA: 9:58 - loss: 0.0987 - acc: 0.9796 - ETA: 9:54 - loss: 0.1010 - acc: 0.978 - ETA: 9:51 - loss: 0.1004 - acc: 0.978 - ETA: 9:53 - loss: 0.1006 - acc: 0.978 - ETA: 9:49 - loss: 0.1007 - acc: 0.978 - ETA: 9:44 - loss: 0.1003 - acc: 0.978 - ETA: 9:40 - loss: 0.1053 - acc: 0.978 - ETA: 9:36 - loss: 0.1047 - acc: 0.978 - ETA: 9:34 - loss: 0.1041 - acc: 0.978 - ETA: 9:30 - loss: 0.1036 - acc: 0.978 - ETA: 9:26 - loss: 0.1036 - acc: 0.978 - ETA: 9:29 - loss: 0.1044 - acc: 0.978 - ETA: 9:33 - loss: 0.1041 - acc: 0.978 - ETA: 9:43 - loss: 0.1045 - acc: 0.977 - ETA: 9:51 - loss: 0.1066 - acc: 0.976 - ETA: 9:53 - loss: 0.1165 - acc: 0.974 - ETA: 9:55 - loss: 0.1168 - acc: 0.974 - ETA: 9:56 - loss: 0.1169 - acc: 0.974 - ETA: 10:02 - loss: 0.1167 - acc: 0.97 - ETA: 10:04 - loss: 0.1180 - acc: 0.97 - ETA: 10:02 - loss: 0.1174 - acc: 0.97 - ETA: 10:00 - loss: 0.1176 - acc: 0.97 - ETA: 10:00 - loss: 0.1173 - acc: 0.97 - ETA: 10:00 - loss: 0.1216 - acc: 0.97 - ETA: 10:01 - loss: 0.1217 - acc: 0.97 - ETA: 10:02 - loss: 0.1211 - acc: 0.97 - ETA: 10:02 - loss: 0.1206 - acc: 0.97 - ETA: 10:02 - loss: 0.1199 - acc: 0.97 - ETA: 10:02 - loss: 0.1194 - acc: 0.97 - ETA: 10:00 - loss: 0.1212 - acc: 0.97 - ETA: 9:58 - loss: 0.1223 - acc: 0.9735 - ETA: 9:56 - loss: 0.1219 - acc: 0.973 - ETA: 9:53 - loss: 0.1220 - acc: 0.973 - ETA: 9:48 - loss: 0.1217 - acc: 0.973 - ETA: 9:43 - loss: 0.1213 - acc: 0.973 - ETA: 9:39 - loss: 0.1210 - acc: 0.973 - ETA: 9:35 - loss: 0.1212 - acc: 0.973 - ETA: 9:32 - loss: 0.1210 - acc: 0.973 - ETA: 9:29 - loss: 0.1204 - acc: 0.973 - ETA: 9:33 - loss: 0.1198 - acc: 0.974 - ETA: 9:32 - loss: 0.1195 - acc: 0.974 - ETA: 9:28 - loss: 0.1188 - acc: 0.974 - ETA: 9:28 - loss: 0.1197 - acc: 0.973 - ETA: 9:23 - loss: 0.1190 - acc: 0.974 - ETA: 9:18 - loss: 0.1189 - acc: 0.973 - ETA: 9:13 - loss: 0.1191 - acc: 0.973 - ETA: 9:08 - loss: 0.1191 - acc: 0.973 - ETA: 9:04 - loss: 0.1191 - acc: 0.973 - ETA: 8:59 - loss: 0.1185 - acc: 0.973 - ETA: 8:54 - loss: 0.1188 - acc: 0.973 - ETA: 8:50 - loss: 0.1188 - acc: 0.973 - ETA: 8:45 - loss: 0.1184 - acc: 0.973 - ETA: 8:41 - loss: 0.1188 - acc: 0.973 - ETA: 8:36 - loss: 0.1184 - acc: 0.973 - ETA: 8:31 - loss: 0.1179 - acc: 0.973 - ETA: 8:27 - loss: 0.1176 - acc: 0.973 - ETA: 8:22 - loss: 0.1179 - acc: 0.973 - ETA: 8:18 - loss: 0.1174 - acc: 0.973 - ETA: 8:14 - loss: 0.1178 - acc: 0.973 - ETA: 8:09 - loss: 0.1190 - acc: 0.972 - ETA: 8:05 - loss: 0.1190 - acc: 0.972 - ETA: 8:03 - loss: 0.1192 - acc: 0.972 - ETA: 7:59 - loss: 0.1220 - acc: 0.972 - ETA: 7:55 - loss: 0.1245 - acc: 0.971 - ETA: 7:52 - loss: 0.1244 - acc: 0.97186680/6680 [==============================] - ETA: 7:49 - loss: 0.1245 - acc: 0.971 - ETA: 7:45 - loss: 0.1252 - acc: 0.971 - ETA: 7:40 - loss: 0.1249 - acc: 0.971 - ETA: 7:36 - loss: 0.1257 - acc: 0.971 - ETA: 7:32 - loss: 0.1257 - acc: 0.971 - ETA: 7:27 - loss: 0.1252 - acc: 0.971 - ETA: 7:23 - loss: 0.1246 - acc: 0.971 - ETA: 7:19 - loss: 0.1245 - acc: 0.971 - ETA: 7:15 - loss: 0.1241 - acc: 0.971 - ETA: 7:12 - loss: 0.1236 - acc: 0.971 - ETA: 7:08 - loss: 0.1232 - acc: 0.971 - ETA: 7:04 - loss: 0.1233 - acc: 0.971 - ETA: 7:00 - loss: 0.1230 - acc: 0.971 - ETA: 6:56 - loss: 0.1232 - acc: 0.971 - ETA: 6:52 - loss: 0.1228 - acc: 0.971 - ETA: 6:47 - loss: 0.1224 - acc: 0.971 - ETA: 6:43 - loss: 0.1220 - acc: 0.971 - ETA: 6:39 - loss: 0.1229 - acc: 0.971 - ETA: 6:35 - loss: 0.1224 - acc: 0.971 - ETA: 6:32 - loss: 0.1219 - acc: 0.971 - ETA: 6:30 - loss: 0.1216 - acc: 0.972 - ETA: 6:27 - loss: 0.1211 - acc: 0.972 - ETA: 6:23 - loss: 0.1206 - acc: 0.972 - ETA: 6:19 - loss: 0.1202 - acc: 0.972 - ETA: 6:15 - loss: 0.1197 - acc: 0.972 - ETA: 6:10 - loss: 0.1193 - acc: 0.972 - ETA: 6:06 - loss: 0.1189 - acc: 0.972 - ETA: 6:02 - loss: 0.1186 - acc: 0.972 - ETA: 5:58 - loss: 0.1184 - acc: 0.972 - ETA: 5:54 - loss: 0.1180 - acc: 0.972 - ETA: 5:50 - loss: 0.1178 - acc: 0.973 - ETA: 5:46 - loss: 0.1174 - acc: 0.973 - ETA: 5:42 - loss: 0.1171 - acc: 0.973 - ETA: 5:38 - loss: 0.1168 - acc: 0.973 - ETA: 5:35 - loss: 0.1163 - acc: 0.973 - ETA: 5:32 - loss: 0.1161 - acc: 0.973 - ETA: 5:29 - loss: 0.1165 - acc: 0.973 - ETA: 5:26 - loss: 0.1161 - acc: 0.973 - ETA: 5:23 - loss: 0.1171 - acc: 0.972 - ETA: 5:19 - loss: 0.1173 - acc: 0.972 - ETA: 5:15 - loss: 0.1173 - acc: 0.972 - ETA: 5:11 - loss: 0.1183 - acc: 0.972 - ETA: 5:07 - loss: 0.1178 - acc: 0.972 - ETA: 5:03 - loss: 0.1175 - acc: 0.972 - ETA: 5:00 - loss: 0.1172 - acc: 0.972 - ETA: 4:57 - loss: 0.1170 - acc: 0.972 - ETA: 4:54 - loss: 0.1167 - acc: 0.972 - ETA: 4:50 - loss: 0.1166 - acc: 0.972 - ETA: 4:46 - loss: 0.1165 - acc: 0.972 - ETA: 4:42 - loss: 0.1163 - acc: 0.972 - ETA: 4:38 - loss: 0.1159 - acc: 0.972 - ETA: 4:34 - loss: 0.1160 - acc: 0.972 - ETA: 4:31 - loss: 0.1159 - acc: 0.972 - ETA: 4:27 - loss: 0.1161 - acc: 0.972 - ETA: 4:24 - loss: 0.1163 - acc: 0.972 - ETA: 4:20 - loss: 0.1162 - acc: 0.972 - ETA: 4:16 - loss: 0.1177 - acc: 0.972 - ETA: 4:13 - loss: 0.1184 - acc: 0.971 - ETA: 4:09 - loss: 0.1181 - acc: 0.972 - ETA: 4:07 - loss: 0.1179 - acc: 0.972 - ETA: 4:03 - loss: 0.1186 - acc: 0.972 - ETA: 3:59 - loss: 0.1196 - acc: 0.971 - ETA: 3:55 - loss: 0.1196 - acc: 0.971 - ETA: 3:52 - loss: 0.1199 - acc: 0.971 - ETA: 3:48 - loss: 0.1195 - acc: 0.971 - ETA: 3:44 - loss: 0.1193 - acc: 0.971 - ETA: 3:41 - loss: 0.1194 - acc: 0.971 - ETA: 3:37 - loss: 0.1190 - acc: 0.971 - ETA: 3:33 - loss: 0.1187 - acc: 0.971 - ETA: 3:29 - loss: 0.1195 - acc: 0.971 - ETA: 3:26 - loss: 0.1210 - acc: 0.971 - ETA: 3:22 - loss: 0.1214 - acc: 0.971 - ETA: 3:18 - loss: 0.1215 - acc: 0.970 - ETA: 3:15 - loss: 0.1218 - acc: 0.970 - ETA: 3:11 - loss: 0.1223 - acc: 0.970 - ETA: 3:08 - loss: 0.1219 - acc: 0.970 - ETA: 3:04 - loss: 0.1216 - acc: 0.970 - ETA: 3:01 - loss: 0.1212 - acc: 0.970 - ETA: 2:57 - loss: 0.1209 - acc: 0.970 - ETA: 2:54 - loss: 0.1207 - acc: 0.971 - ETA: 2:50 - loss: 0.1204 - acc: 0.971 - ETA: 2:46 - loss: 0.1211 - acc: 0.970 - ETA: 2:43 - loss: 0.1214 - acc: 0.970 - ETA: 2:39 - loss: 0.1214 - acc: 0.970 - ETA: 2:36 - loss: 0.1210 - acc: 0.970 - ETA: 2:32 - loss: 0.1209 - acc: 0.970 - ETA: 2:29 - loss: 0.1212 - acc: 0.970 - ETA: 2:25 - loss: 0.1211 - acc: 0.970 - ETA: 2:22 - loss: 0.1222 - acc: 0.969 - ETA: 2:19 - loss: 0.1219 - acc: 0.969 - ETA: 2:15 - loss: 0.1217 - acc: 0.969 - ETA: 2:12 - loss: 0.1214 - acc: 0.969 - ETA: 2:08 - loss: 0.1211 - acc: 0.970 - ETA: 2:05 - loss: 0.1207 - acc: 0.970 - ETA: 2:01 - loss: 0.1204 - acc: 0.970 - ETA: 1:58 - loss: 0.1201 - acc: 0.970 - ETA: 1:55 - loss: 0.1201 - acc: 0.970 - ETA: 1:51 - loss: 0.1197 - acc: 0.970 - ETA: 1:48 - loss: 0.1194 - acc: 0.970 - ETA: 1:44 - loss: 0.1192 - acc: 0.970 - ETA: 1:41 - loss: 0.1191 - acc: 0.970 - ETA: 1:37 - loss: 0.1188 - acc: 0.970 - ETA: 1:34 - loss: 0.1185 - acc: 0.970 - ETA: 1:30 - loss: 0.1184 - acc: 0.970 - ETA: 1:27 - loss: 0.1185 - acc: 0.970 - ETA: 1:24 - loss: 0.1181 - acc: 0.971 - ETA: 1:20 - loss: 0.1178 - acc: 0.971 - ETA: 1:17 - loss: 0.1177 - acc: 0.971 - ETA: 1:13 - loss: 0.1176 - acc: 0.971 - ETA: 1:10 - loss: 0.1173 - acc: 0.971 - ETA: 1:06 - loss: 0.1174 - acc: 0.971 - ETA: 1:02 - loss: 0.1173 - acc: 0.971 - ETA: 59s - loss: 0.1172 - acc: 0.971 - ETA: 55s - loss: 0.1176 - acc: 0.97 - ETA: 52s - loss: 0.1174 - acc: 0.97 - ETA: 48s - loss: 0.1184 - acc: 0.97 - ETA: 45s - loss: 0.1187 - acc: 0.97 - ETA: 41s - loss: 0.1187 - acc: 0.97 - ETA: 38s - loss: 0.1185 - acc: 0.97 - ETA: 34s - loss: 0.1182 - acc: 0.97 - ETA: 31s - loss: 0.1180 - acc: 0.97 - ETA: 27s - loss: 0.1179 - acc: 0.97 - ETA: 24s - loss: 0.1178 - acc: 0.97 - ETA: 20s - loss: 0.1178 - acc: 0.97 - ETA: 17s - loss: 0.1178 - acc: 0.97 - ETA: 13s - loss: 0.1175 - acc: 0.97 - ETA: 10s - loss: 0.1175 - acc: 0.97 - ETA: 6s - loss: 0.1172 - acc: 0.9705 - ETA: 3s - loss: 0.1171 - acc: 0.970 - 1218s 182ms/step - loss: 0.1176 - acc: 0.9704 - val_loss: 8.5136 - val_acc: 0.0886\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x259362c80f0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 8.9713%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 27:38 - loss: 16.1181 - acc: 0.0000e+0 - ETA: 14:06 - loss: 14.9964 - acc: 0.0000e+0 - ETA: 7:09 - loss: 15.3335 - acc: 0.0000e+0 - ETA: 5:49 - loss: 15.2813 - acc: 0.0000e+ - ETA: 4:11 - loss: 15.0513 - acc: 0.0143   - ETA: 3:42 - loss: 14.9929 - acc: 0.01 - ETA: 2:59 - loss: 15.0261 - acc: 0.01 - ETA: 2:44 - loss: 15.0156 - acc: 0.01 - ETA: 2:32 - loss: 14.9357 - acc: 0.01 - ETA: 2:11 - loss: 14.9631 - acc: 0.01 - ETA: 1:57 - loss: 14.8948 - acc: 0.01 - ETA: 1:44 - loss: 14.8587 - acc: 0.02 - ETA: 1:30 - loss: 14.7711 - acc: 0.02 - ETA: 1:15 - loss: 14.8058 - acc: 0.02 - ETA: 1:07 - loss: 14.7443 - acc: 0.02 - ETA: 1:00 - loss: 14.7623 - acc: 0.02 - ETA: 55s - loss: 14.6795 - acc: 0.0250 - ETA: 52s - loss: 14.6947 - acc: 0.025 - ETA: 48s - loss: 14.5437 - acc: 0.030 - ETA: 42s - loss: 14.5823 - acc: 0.029 - ETA: 39s - loss: 14.5009 - acc: 0.030 - ETA: 35s - loss: 14.3999 - acc: 0.033 - ETA: 33s - loss: 14.3601 - acc: 0.033 - ETA: 30s - loss: 14.3050 - acc: 0.035 - ETA: 28s - loss: 14.2456 - acc: 0.037 - ETA: 26s - loss: 14.1889 - acc: 0.039 - ETA: 25s - loss: 14.1547 - acc: 0.039 - ETA: 23s - loss: 14.1312 - acc: 0.042 - ETA: 22s - loss: 14.1168 - acc: 0.042 - ETA: 20s - loss: 14.0710 - acc: 0.042 - ETA: 19s - loss: 14.0579 - acc: 0.043 - ETA: 18s - loss: 14.0075 - acc: 0.045 - ETA: 17s - loss: 13.9368 - acc: 0.048 - ETA: 16s - loss: 13.9060 - acc: 0.048 - ETA: 16s - loss: 13.8424 - acc: 0.051 - ETA: 15s - loss: 13.8091 - acc: 0.051 - ETA: 14s - loss: 13.7608 - acc: 0.053 - ETA: 13s - loss: 13.7424 - acc: 0.055 - ETA: 13s - loss: 13.7310 - acc: 0.055 - ETA: 12s - loss: 13.6941 - acc: 0.057 - ETA: 12s - loss: 13.6008 - acc: 0.060 - ETA: 11s - loss: 13.5562 - acc: 0.062 - ETA: 11s - loss: 13.5218 - acc: 0.063 - ETA: 10s - loss: 13.4601 - acc: 0.065 - ETA: 10s - loss: 13.4130 - acc: 0.067 - ETA: 9s - loss: 13.3962 - acc: 0.068 - ETA: 9s - loss: 13.3990 - acc: 0.06 - ETA: 9s - loss: 13.3414 - acc: 0.06 - ETA: 8s - loss: 13.3147 - acc: 0.07 - ETA: 8s - loss: 13.2840 - acc: 0.07 - ETA: 8s - loss: 13.2680 - acc: 0.07 - ETA: 7s - loss: 13.2456 - acc: 0.07 - ETA: 7s - loss: 13.2062 - acc: 0.07 - ETA: 7s - loss: 13.1508 - acc: 0.07 - ETA: 6s - loss: 13.1041 - acc: 0.08 - ETA: 6s - loss: 13.0721 - acc: 0.08 - ETA: 6s - loss: 13.0508 - acc: 0.08 - ETA: 5s - loss: 13.0378 - acc: 0.08 - ETA: 5s - loss: 13.0107 - acc: 0.08 - ETA: 5s - loss: 12.9944 - acc: 0.08 - ETA: 4s - loss: 12.9634 - acc: 0.08 - ETA: 4s - loss: 12.9447 - acc: 0.08 - ETA: 4s - loss: 12.9359 - acc: 0.08 - ETA: 4s - loss: 12.9084 - acc: 0.08 - ETA: 4s - loss: 12.8757 - acc: 0.09 - ETA: 3s - loss: 12.8252 - acc: 0.09 - ETA: 3s - loss: 12.7999 - acc: 0.09 - ETA: 3s - loss: 12.7636 - acc: 0.09 - ETA: 3s - loss: 12.7391 - acc: 0.09 - ETA: 3s - loss: 12.6944 - acc: 0.10 - ETA: 2s - loss: 12.6643 - acc: 0.10 - ETA: 2s - loss: 12.6368 - acc: 0.10 - ETA: 2s - loss: 12.6185 - acc: 0.10 - ETA: 2s - loss: 12.5916 - acc: 0.10 - ETA: 2s - loss: 12.5706 - acc: 0.10 - ETA: 1s - loss: 12.5478 - acc: 0.10 - ETA: 1s - loss: 12.5265 - acc: 0.10 - ETA: 1s - loss: 12.4994 - acc: 0.10 - ETA: 1s - loss: 12.4666 - acc: 0.11 - ETA: 1s - loss: 12.4422 - acc: 0.11 - ETA: 1s - loss: 12.4208 - acc: 0.11 - ETA: 0s - loss: 12.3939 - acc: 0.11 - ETA: 0s - loss: 12.3870 - acc: 0.11 - ETA: 0s - loss: 12.3502 - acc: 0.11 - ETA: 0s - loss: 12.3348 - acc: 0.11 - ETA: 0s - loss: 12.3343 - acc: 0.11 - ETA: 0s - loss: 12.3105 - acc: 0.11 - ETA: 0s - loss: 12.2743 - acc: 0.12 - ETA: 0s - loss: 12.2471 - acc: 0.12 - 11s 2ms/step - loss: 12.2339 - acc: 0.1249 - val_loss: 10.7704 - val_acc: 0.2084\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 10.77045, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 8.1288 - acc: 0.450 - ETA: 3s - loss: 11.0556 - acc: 0.24 - ETA: 3s - loss: 10.3990 - acc: 0.27 - ETA: 3s - loss: 9.9190 - acc: 0.2906 - ETA: 3s - loss: 10.1074 - acc: 0.28 - ETA: 3s - loss: 9.9582 - acc: 0.2792 - ETA: 3s - loss: 10.0466 - acc: 0.27 - ETA: 3s - loss: 10.0373 - acc: 0.27 - ETA: 3s - loss: 9.8827 - acc: 0.2824 - ETA: 3s - loss: 9.9092 - acc: 0.276 - ETA: 3s - loss: 10.0337 - acc: 0.26 - ETA: 3s - loss: 10.0623 - acc: 0.26 - ETA: 3s - loss: 10.1687 - acc: 0.26 - ETA: 3s - loss: 10.1889 - acc: 0.26 - ETA: 3s - loss: 10.2028 - acc: 0.26 - ETA: 3s - loss: 10.0626 - acc: 0.27 - ETA: 3s - loss: 10.1397 - acc: 0.26 - ETA: 3s - loss: 10.1778 - acc: 0.26 - ETA: 3s - loss: 10.1832 - acc: 0.26 - ETA: 3s - loss: 10.1755 - acc: 0.26 - ETA: 3s - loss: 10.2194 - acc: 0.25 - ETA: 3s - loss: 10.1493 - acc: 0.26 - ETA: 2s - loss: 10.1900 - acc: 0.26 - ETA: 2s - loss: 10.1737 - acc: 0.26 - ETA: 2s - loss: 10.1237 - acc: 0.26 - ETA: 2s - loss: 10.1074 - acc: 0.26 - ETA: 2s - loss: 10.1422 - acc: 0.26 - ETA: 2s - loss: 10.1676 - acc: 0.26 - ETA: 2s - loss: 10.2042 - acc: 0.26 - ETA: 2s - loss: 10.2090 - acc: 0.26 - ETA: 2s - loss: 10.1840 - acc: 0.26 - ETA: 2s - loss: 10.1712 - acc: 0.26 - ETA: 2s - loss: 10.1889 - acc: 0.26 - ETA: 2s - loss: 10.1620 - acc: 0.27 - ETA: 2s - loss: 10.1528 - acc: 0.27 - ETA: 2s - loss: 10.1334 - acc: 0.27 - ETA: 2s - loss: 10.0988 - acc: 0.27 - ETA: 2s - loss: 10.1195 - acc: 0.27 - ETA: 2s - loss: 10.1032 - acc: 0.27 - ETA: 2s - loss: 10.0948 - acc: 0.27 - ETA: 2s - loss: 10.1118 - acc: 0.27 - ETA: 2s - loss: 10.0889 - acc: 0.27 - ETA: 1s - loss: 10.0741 - acc: 0.27 - ETA: 1s - loss: 10.0733 - acc: 0.27 - ETA: 1s - loss: 10.0229 - acc: 0.28 - ETA: 1s - loss: 10.0136 - acc: 0.28 - ETA: 1s - loss: 9.9926 - acc: 0.2840 - ETA: 1s - loss: 9.9794 - acc: 0.285 - ETA: 1s - loss: 9.9915 - acc: 0.284 - ETA: 1s - loss: 10.0161 - acc: 0.28 - ETA: 1s - loss: 10.0074 - acc: 0.28 - ETA: 1s - loss: 10.0139 - acc: 0.28 - ETA: 1s - loss: 10.0015 - acc: 0.28 - ETA: 1s - loss: 10.0020 - acc: 0.28 - ETA: 1s - loss: 10.0086 - acc: 0.28 - ETA: 1s - loss: 10.0045 - acc: 0.28 - ETA: 1s - loss: 9.9740 - acc: 0.2861 - ETA: 1s - loss: 10.0043 - acc: 0.28 - ETA: 1s - loss: 9.9922 - acc: 0.2852 - ETA: 1s - loss: 10.0098 - acc: 0.28 - ETA: 1s - loss: 10.0133 - acc: 0.28 - ETA: 0s - loss: 10.0063 - acc: 0.28 - ETA: 0s - loss: 9.9949 - acc: 0.2851 - ETA: 0s - loss: 9.9898 - acc: 0.285 - ETA: 0s - loss: 9.9699 - acc: 0.286 - ETA: 0s - loss: 9.9627 - acc: 0.287 - ETA: 0s - loss: 9.9688 - acc: 0.287 - ETA: 0s - loss: 9.9592 - acc: 0.288 - ETA: 0s - loss: 9.9648 - acc: 0.287 - ETA: 0s - loss: 9.9572 - acc: 0.288 - ETA: 0s - loss: 9.9564 - acc: 0.288 - ETA: 0s - loss: 9.9583 - acc: 0.287 - ETA: 0s - loss: 9.9490 - acc: 0.288 - ETA: 0s - loss: 9.9312 - acc: 0.289 - ETA: 0s - loss: 9.8970 - acc: 0.291 - ETA: 0s - loss: 9.9080 - acc: 0.291 - ETA: 0s - loss: 9.9008 - acc: 0.291 - ETA: 0s - loss: 9.8938 - acc: 0.292 - ETA: 0s - loss: 9.8974 - acc: 0.292 - 5s 684us/step - loss: 9.8894 - acc: 0.2928 - val_loss: 9.8057 - val_acc: 0.2743\n",
      "\n",
      "Epoch 00002: val_loss improved from 10.77045 to 9.80571, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 10s - loss: 10.3868 - acc: 0.300 - ETA: 4s - loss: 8.0545 - acc: 0.4167  - ETA: 4s - loss: 8.4988 - acc: 0.386 - ETA: 4s - loss: 8.6555 - acc: 0.376 - ETA: 4s - loss: 8.5445 - acc: 0.385 - ETA: 4s - loss: 8.6449 - acc: 0.372 - ETA: 4s - loss: 8.7817 - acc: 0.366 - ETA: 3s - loss: 8.8927 - acc: 0.362 - ETA: 3s - loss: 9.0274 - acc: 0.354 - ETA: 3s - loss: 9.1663 - acc: 0.348 - ETA: 3s - loss: 9.1497 - acc: 0.347 - ETA: 3s - loss: 9.1204 - acc: 0.351 - ETA: 3s - loss: 9.0918 - acc: 0.354 - ETA: 3s - loss: 9.1267 - acc: 0.353 - ETA: 3s - loss: 9.0827 - acc: 0.356 - ETA: 3s - loss: 9.1023 - acc: 0.354 - ETA: 3s - loss: 9.1186 - acc: 0.353 - ETA: 3s - loss: 9.0798 - acc: 0.353 - ETA: 3s - loss: 9.1261 - acc: 0.353 - ETA: 3s - loss: 9.2009 - acc: 0.348 - ETA: 2s - loss: 9.1335 - acc: 0.350 - ETA: 2s - loss: 9.1758 - acc: 0.347 - ETA: 2s - loss: 9.2223 - acc: 0.345 - ETA: 2s - loss: 9.2125 - acc: 0.344 - ETA: 2s - loss: 9.2514 - acc: 0.344 - ETA: 2s - loss: 9.2258 - acc: 0.345 - ETA: 2s - loss: 9.2168 - acc: 0.345 - ETA: 2s - loss: 9.2457 - acc: 0.345 - ETA: 2s - loss: 9.2533 - acc: 0.344 - ETA: 2s - loss: 9.2075 - acc: 0.346 - ETA: 2s - loss: 9.1953 - acc: 0.346 - ETA: 2s - loss: 9.2227 - acc: 0.344 - ETA: 2s - loss: 9.2346 - acc: 0.343 - ETA: 2s - loss: 9.2134 - acc: 0.344 - ETA: 2s - loss: 9.2486 - acc: 0.343 - ETA: 2s - loss: 9.2263 - acc: 0.343 - ETA: 2s - loss: 9.2364 - acc: 0.344 - ETA: 1s - loss: 9.2363 - acc: 0.343 - ETA: 1s - loss: 9.2209 - acc: 0.344 - ETA: 1s - loss: 9.2282 - acc: 0.344 - ETA: 1s - loss: 9.2545 - acc: 0.342 - ETA: 1s - loss: 9.2375 - acc: 0.344 - ETA: 1s - loss: 9.2574 - acc: 0.343 - ETA: 1s - loss: 9.2452 - acc: 0.344 - ETA: 1s - loss: 9.2480 - acc: 0.344 - ETA: 1s - loss: 9.2472 - acc: 0.345 - ETA: 1s - loss: 9.2277 - acc: 0.345 - ETA: 1s - loss: 9.1943 - acc: 0.347 - ETA: 1s - loss: 9.1859 - acc: 0.348 - ETA: 1s - loss: 9.1693 - acc: 0.349 - ETA: 1s - loss: 9.1912 - acc: 0.348 - ETA: 1s - loss: 9.1851 - acc: 0.349 - ETA: 1s - loss: 9.1665 - acc: 0.350 - ETA: 1s - loss: 9.1721 - acc: 0.350 - ETA: 1s - loss: 9.1772 - acc: 0.351 - ETA: 1s - loss: 9.1848 - acc: 0.351 - ETA: 1s - loss: 9.1832 - acc: 0.351 - ETA: 1s - loss: 9.2071 - acc: 0.350 - ETA: 0s - loss: 9.1923 - acc: 0.351 - ETA: 0s - loss: 9.1886 - acc: 0.352 - ETA: 0s - loss: 9.1926 - acc: 0.353 - ETA: 0s - loss: 9.1992 - acc: 0.352 - ETA: 0s - loss: 9.1906 - acc: 0.353 - ETA: 0s - loss: 9.2122 - acc: 0.352 - ETA: 0s - loss: 9.2019 - acc: 0.353 - ETA: 0s - loss: 9.2194 - acc: 0.351 - ETA: 0s - loss: 9.1916 - acc: 0.352 - ETA: 0s - loss: 9.1755 - acc: 0.353 - ETA: 0s - loss: 9.1660 - acc: 0.354 - ETA: 0s - loss: 9.1567 - acc: 0.354 - ETA: 0s - loss: 9.1470 - acc: 0.355 - ETA: 0s - loss: 9.1191 - acc: 0.357 - ETA: 0s - loss: 9.1246 - acc: 0.356 - ETA: 0s - loss: 9.1396 - acc: 0.356 - ETA: 0s - loss: 9.1310 - acc: 0.356 - ETA: 0s - loss: 9.1184 - acc: 0.357 - ETA: 0s - loss: 9.1028 - acc: 0.358 - 5s 677us/step - loss: 9.0902 - acc: 0.3590 - val_loss: 9.3581 - val_acc: 0.3090\n",
      "\n",
      "Epoch 00003: val_loss improved from 9.80571 to 9.35809, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 8.3175 - acc: 0.400 - ETA: 3s - loss: 8.9903 - acc: 0.400 - ETA: 3s - loss: 9.2460 - acc: 0.390 - ETA: 3s - loss: 8.9657 - acc: 0.403 - ETA: 3s - loss: 9.2893 - acc: 0.386 - ETA: 3s - loss: 9.1532 - acc: 0.393 - ETA: 3s - loss: 8.9520 - acc: 0.403 - ETA: 3s - loss: 8.9916 - acc: 0.401 - ETA: 3s - loss: 9.1751 - acc: 0.389 - ETA: 3s - loss: 9.0075 - acc: 0.394 - ETA: 3s - loss: 8.9502 - acc: 0.397 - ETA: 3s - loss: 8.9549 - acc: 0.395 - ETA: 3s - loss: 8.9475 - acc: 0.397 - ETA: 3s - loss: 8.9662 - acc: 0.393 - ETA: 3s - loss: 8.9428 - acc: 0.396 - ETA: 3s - loss: 8.8868 - acc: 0.400 - ETA: 3s - loss: 8.8838 - acc: 0.400 - ETA: 3s - loss: 8.8726 - acc: 0.400 - ETA: 3s - loss: 8.9604 - acc: 0.395 - ETA: 3s - loss: 8.9019 - acc: 0.398 - ETA: 3s - loss: 8.8981 - acc: 0.398 - ETA: 3s - loss: 8.9100 - acc: 0.397 - ETA: 2s - loss: 8.9614 - acc: 0.393 - ETA: 2s - loss: 8.9511 - acc: 0.393 - ETA: 2s - loss: 8.9706 - acc: 0.391 - ETA: 2s - loss: 8.9829 - acc: 0.389 - ETA: 2s - loss: 8.9527 - acc: 0.391 - ETA: 2s - loss: 8.8914 - acc: 0.396 - ETA: 2s - loss: 8.8776 - acc: 0.396 - ETA: 2s - loss: 8.8824 - acc: 0.394 - ETA: 2s - loss: 8.8647 - acc: 0.396 - ETA: 2s - loss: 8.8816 - acc: 0.393 - ETA: 2s - loss: 8.8623 - acc: 0.395 - ETA: 2s - loss: 8.8553 - acc: 0.396 - ETA: 2s - loss: 8.8237 - acc: 0.398 - ETA: 2s - loss: 8.8159 - acc: 0.399 - ETA: 2s - loss: 8.8345 - acc: 0.398 - ETA: 2s - loss: 8.8429 - acc: 0.398 - ETA: 2s - loss: 8.8469 - acc: 0.397 - ETA: 2s - loss: 8.8312 - acc: 0.398 - ETA: 1s - loss: 8.8571 - acc: 0.396 - ETA: 1s - loss: 8.7976 - acc: 0.401 - ETA: 1s - loss: 8.7738 - acc: 0.401 - ETA: 1s - loss: 8.7756 - acc: 0.401 - ETA: 1s - loss: 8.7849 - acc: 0.401 - ETA: 1s - loss: 8.7884 - acc: 0.401 - ETA: 1s - loss: 8.7865 - acc: 0.401 - ETA: 1s - loss: 8.7588 - acc: 0.402 - ETA: 1s - loss: 8.7621 - acc: 0.401 - ETA: 1s - loss: 8.7864 - acc: 0.399 - ETA: 1s - loss: 8.7678 - acc: 0.399 - ETA: 1s - loss: 8.7902 - acc: 0.399 - ETA: 1s - loss: 8.7501 - acc: 0.401 - ETA: 1s - loss: 8.7611 - acc: 0.400 - ETA: 1s - loss: 8.7580 - acc: 0.400 - ETA: 1s - loss: 8.7448 - acc: 0.402 - ETA: 1s - loss: 8.7460 - acc: 0.401 - ETA: 1s - loss: 8.7333 - acc: 0.401 - ETA: 0s - loss: 8.7186 - acc: 0.402 - ETA: 0s - loss: 8.7032 - acc: 0.403 - ETA: 0s - loss: 8.6949 - acc: 0.403 - ETA: 0s - loss: 8.6601 - acc: 0.405 - ETA: 0s - loss: 8.6530 - acc: 0.405 - ETA: 0s - loss: 8.6532 - acc: 0.405 - ETA: 0s - loss: 8.6561 - acc: 0.405 - ETA: 0s - loss: 8.6592 - acc: 0.404 - ETA: 0s - loss: 8.6580 - acc: 0.404 - ETA: 0s - loss: 8.6361 - acc: 0.405 - ETA: 0s - loss: 8.6478 - acc: 0.405 - ETA: 0s - loss: 8.6267 - acc: 0.406 - ETA: 0s - loss: 8.6222 - acc: 0.406 - ETA: 0s - loss: 8.6265 - acc: 0.405 - ETA: 0s - loss: 8.6443 - acc: 0.404 - ETA: 0s - loss: 8.6403 - acc: 0.404 - 4s 633us/step - loss: 8.6230 - acc: 0.4052 - val_loss: 8.8051 - val_acc: 0.3509\n",
      "\n",
      "Epoch 00004: val_loss improved from 9.35809 to 8.80514, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 3s - loss: 9.0402 - acc: 0.400 - ETA: 3s - loss: 8.7073 - acc: 0.385 - ETA: 3s - loss: 8.5262 - acc: 0.411 - ETA: 2s - loss: 8.3165 - acc: 0.431 - ETA: 2s - loss: 8.4594 - acc: 0.428 - ETA: 2s - loss: 8.5520 - acc: 0.411 - ETA: 2s - loss: 8.4846 - acc: 0.416 - ETA: 2s - loss: 8.6306 - acc: 0.408 - ETA: 2s - loss: 8.6854 - acc: 0.405 - ETA: 2s - loss: 8.5427 - acc: 0.416 - ETA: 2s - loss: 8.5027 - acc: 0.415 - ETA: 2s - loss: 8.5667 - acc: 0.412 - ETA: 2s - loss: 8.5066 - acc: 0.416 - ETA: 2s - loss: 8.4126 - acc: 0.425 - ETA: 2s - loss: 8.4450 - acc: 0.423 - ETA: 2s - loss: 8.4757 - acc: 0.423 - ETA: 2s - loss: 8.4981 - acc: 0.422 - ETA: 2s - loss: 8.4586 - acc: 0.422 - ETA: 2s - loss: 8.4219 - acc: 0.422 - ETA: 2s - loss: 8.3996 - acc: 0.424 - ETA: 2s - loss: 8.4446 - acc: 0.418 - ETA: 1s - loss: 8.3745 - acc: 0.423 - ETA: 1s - loss: 8.3705 - acc: 0.423 - ETA: 1s - loss: 8.3089 - acc: 0.425 - ETA: 1s - loss: 8.2721 - acc: 0.427 - ETA: 1s - loss: 8.2522 - acc: 0.429 - ETA: 1s - loss: 8.2479 - acc: 0.430 - ETA: 1s - loss: 8.2653 - acc: 0.428 - ETA: 1s - loss: 8.2494 - acc: 0.430 - ETA: 1s - loss: 8.2106 - acc: 0.433 - ETA: 1s - loss: 8.1751 - acc: 0.437 - ETA: 1s - loss: 8.1871 - acc: 0.436 - ETA: 1s - loss: 8.1485 - acc: 0.438 - ETA: 1s - loss: 8.1605 - acc: 0.438 - ETA: 1s - loss: 8.1811 - acc: 0.437 - ETA: 1s - loss: 8.1661 - acc: 0.437 - ETA: 1s - loss: 8.1519 - acc: 0.437 - ETA: 1s - loss: 8.1504 - acc: 0.437 - ETA: 1s - loss: 8.1538 - acc: 0.436 - ETA: 0s - loss: 8.1678 - acc: 0.435 - ETA: 0s - loss: 8.1529 - acc: 0.436 - ETA: 0s - loss: 8.1196 - acc: 0.439 - ETA: 0s - loss: 8.1387 - acc: 0.438 - ETA: 0s - loss: 8.1240 - acc: 0.438 - ETA: 0s - loss: 8.1250 - acc: 0.438 - ETA: 0s - loss: 8.1035 - acc: 0.439 - ETA: 0s - loss: 8.1183 - acc: 0.438 - ETA: 0s - loss: 8.1115 - acc: 0.438 - ETA: 0s - loss: 8.1103 - acc: 0.438 - ETA: 0s - loss: 8.1050 - acc: 0.439 - ETA: 0s - loss: 8.1168 - acc: 0.438 - ETA: 0s - loss: 8.1123 - acc: 0.438 - ETA: 0s - loss: 8.1249 - acc: 0.438 - ETA: 0s - loss: 8.1344 - acc: 0.438 - ETA: 0s - loss: 8.1273 - acc: 0.438 - ETA: 0s - loss: 8.1209 - acc: 0.438 - ETA: 0s - loss: 8.1074 - acc: 0.439 - ETA: 0s - loss: 8.1082 - acc: 0.438 - ETA: 0s - loss: 8.0994 - acc: 0.439 - 4s 529us/step - loss: 8.0948 - acc: 0.4395 - val_loss: 8.4809 - val_acc: 0.3880\n",
      "\n",
      "Epoch 00005: val_loss improved from 8.80514 to 8.48091, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 7.6986 - acc: 0.350 - ETA: 3s - loss: 6.8379 - acc: 0.508 - ETA: 3s - loss: 7.4664 - acc: 0.468 - ETA: 3s - loss: 7.4811 - acc: 0.480 - ETA: 3s - loss: 7.5601 - acc: 0.477 - ETA: 3s - loss: 7.4149 - acc: 0.488 - ETA: 3s - loss: 7.7080 - acc: 0.473 - ETA: 3s - loss: 7.8250 - acc: 0.467 - ETA: 3s - loss: 7.7653 - acc: 0.472 - ETA: 3s - loss: 7.7900 - acc: 0.471 - ETA: 3s - loss: 7.8090 - acc: 0.470 - ETA: 2s - loss: 7.8118 - acc: 0.470 - ETA: 2s - loss: 7.7578 - acc: 0.472 - ETA: 2s - loss: 7.7163 - acc: 0.475 - ETA: 2s - loss: 7.6514 - acc: 0.480 - ETA: 2s - loss: 7.5589 - acc: 0.487 - ETA: 2s - loss: 7.5886 - acc: 0.484 - ETA: 2s - loss: 7.5457 - acc: 0.486 - ETA: 2s - loss: 7.5219 - acc: 0.488 - ETA: 2s - loss: 7.5318 - acc: 0.486 - ETA: 2s - loss: 7.5379 - acc: 0.485 - ETA: 2s - loss: 7.4523 - acc: 0.491 - ETA: 2s - loss: 7.5056 - acc: 0.489 - ETA: 2s - loss: 7.4896 - acc: 0.490 - ETA: 2s - loss: 7.5065 - acc: 0.491 - ETA: 2s - loss: 7.5387 - acc: 0.489 - ETA: 2s - loss: 7.6119 - acc: 0.485 - ETA: 2s - loss: 7.6505 - acc: 0.483 - ETA: 1s - loss: 7.6802 - acc: 0.481 - ETA: 1s - loss: 7.7143 - acc: 0.479 - ETA: 1s - loss: 7.7266 - acc: 0.478 - ETA: 1s - loss: 7.7467 - acc: 0.477 - ETA: 1s - loss: 7.7480 - acc: 0.477 - ETA: 1s - loss: 7.8116 - acc: 0.473 - ETA: 1s - loss: 7.8191 - acc: 0.473 - ETA: 1s - loss: 7.8131 - acc: 0.472 - ETA: 1s - loss: 7.8758 - acc: 0.468 - ETA: 1s - loss: 7.8604 - acc: 0.469 - ETA: 1s - loss: 7.9218 - acc: 0.465 - ETA: 1s - loss: 7.9151 - acc: 0.465 - ETA: 1s - loss: 7.9023 - acc: 0.467 - ETA: 1s - loss: 7.9097 - acc: 0.467 - ETA: 1s - loss: 7.8918 - acc: 0.468 - ETA: 1s - loss: 7.9092 - acc: 0.467 - ETA: 1s - loss: 7.8737 - acc: 0.469 - ETA: 1s - loss: 7.8415 - acc: 0.471 - ETA: 0s - loss: 7.8061 - acc: 0.473 - ETA: 0s - loss: 7.7942 - acc: 0.472 - ETA: 0s - loss: 7.7966 - acc: 0.473 - ETA: 0s - loss: 7.7994 - acc: 0.473 - ETA: 0s - loss: 7.8272 - acc: 0.471 - ETA: 0s - loss: 7.8323 - acc: 0.471 - ETA: 0s - loss: 7.8248 - acc: 0.471 - ETA: 0s - loss: 7.8260 - acc: 0.471 - ETA: 0s - loss: 7.8384 - acc: 0.471 - ETA: 0s - loss: 7.8453 - acc: 0.470 - ETA: 0s - loss: 7.8411 - acc: 0.470 - ETA: 0s - loss: 7.8324 - acc: 0.471 - ETA: 0s - loss: 7.8259 - acc: 0.471 - ETA: 0s - loss: 7.8262 - acc: 0.471 - ETA: 0s - loss: 7.8336 - acc: 0.471 - ETA: 0s - loss: 7.8214 - acc: 0.472 - ETA: 0s - loss: 7.8088 - acc: 0.473 - 4s 548us/step - loss: 7.7974 - acc: 0.4740 - val_loss: 8.3444 - val_acc: 0.3952\n",
      "\n",
      "Epoch 00006: val_loss improved from 8.48091 to 8.34437, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 3s - loss: 9.0644 - acc: 0.350 - ETA: 3s - loss: 7.9524 - acc: 0.458 - ETA: 3s - loss: 8.3052 - acc: 0.445 - ETA: 3s - loss: 8.0913 - acc: 0.459 - ETA: 3s - loss: 7.9786 - acc: 0.469 - ETA: 3s - loss: 7.7189 - acc: 0.486 - ETA: 3s - loss: 7.6956 - acc: 0.491 - ETA: 3s - loss: 7.8381 - acc: 0.484 - ETA: 3s - loss: 7.8821 - acc: 0.482 - ETA: 3s - loss: 7.8092 - acc: 0.486 - ETA: 3s - loss: 7.8300 - acc: 0.483 - ETA: 3s - loss: 7.8052 - acc: 0.483 - ETA: 3s - loss: 7.8156 - acc: 0.483 - ETA: 3s - loss: 7.7951 - acc: 0.485 - ETA: 3s - loss: 7.7654 - acc: 0.487 - ETA: 2s - loss: 7.8237 - acc: 0.485 - ETA: 2s - loss: 7.7887 - acc: 0.486 - ETA: 2s - loss: 7.7386 - acc: 0.488 - ETA: 2s - loss: 7.7840 - acc: 0.486 - ETA: 2s - loss: 7.7913 - acc: 0.485 - ETA: 2s - loss: 7.8005 - acc: 0.486 - ETA: 2s - loss: 7.8115 - acc: 0.485 - ETA: 2s - loss: 7.8644 - acc: 0.482 - ETA: 2s - loss: 7.8314 - acc: 0.482 - ETA: 2s - loss: 7.8297 - acc: 0.481 - ETA: 2s - loss: 7.8474 - acc: 0.480 - ETA: 2s - loss: 7.8461 - acc: 0.480 - ETA: 2s - loss: 7.8133 - acc: 0.481 - ETA: 2s - loss: 7.8238 - acc: 0.481 - ETA: 2s - loss: 7.7695 - acc: 0.485 - ETA: 2s - loss: 7.7656 - acc: 0.484 - ETA: 2s - loss: 7.7691 - acc: 0.483 - ETA: 2s - loss: 7.7776 - acc: 0.483 - ETA: 2s - loss: 7.7749 - acc: 0.483 - ETA: 1s - loss: 7.7529 - acc: 0.484 - ETA: 1s - loss: 7.7705 - acc: 0.483 - ETA: 1s - loss: 7.7585 - acc: 0.482 - ETA: 1s - loss: 7.7366 - acc: 0.484 - ETA: 1s - loss: 7.7514 - acc: 0.482 - ETA: 1s - loss: 7.7427 - acc: 0.484 - ETA: 1s - loss: 7.7440 - acc: 0.484 - ETA: 1s - loss: 7.7646 - acc: 0.482 - ETA: 1s - loss: 7.7608 - acc: 0.482 - ETA: 1s - loss: 7.7553 - acc: 0.482 - ETA: 1s - loss: 7.7592 - acc: 0.482 - ETA: 1s - loss: 7.7556 - acc: 0.482 - ETA: 1s - loss: 7.7279 - acc: 0.483 - ETA: 1s - loss: 7.7146 - acc: 0.484 - ETA: 1s - loss: 7.7141 - acc: 0.484 - ETA: 1s - loss: 7.6908 - acc: 0.486 - ETA: 1s - loss: 7.6894 - acc: 0.486 - ETA: 1s - loss: 7.7033 - acc: 0.485 - ETA: 0s - loss: 7.7286 - acc: 0.483 - ETA: 0s - loss: 7.7198 - acc: 0.484 - ETA: 0s - loss: 7.7171 - acc: 0.485 - ETA: 0s - loss: 7.7115 - acc: 0.485 - ETA: 0s - loss: 7.7269 - acc: 0.484 - ETA: 0s - loss: 7.7125 - acc: 0.485 - ETA: 0s - loss: 7.7234 - acc: 0.484 - ETA: 0s - loss: 7.7142 - acc: 0.485 - ETA: 0s - loss: 7.7130 - acc: 0.485 - ETA: 0s - loss: 7.7146 - acc: 0.484 - ETA: 0s - loss: 7.6957 - acc: 0.486 - ETA: 0s - loss: 7.6768 - acc: 0.486 - ETA: 0s - loss: 7.6915 - acc: 0.486 - ETA: 0s - loss: 7.6726 - acc: 0.487 - ETA: 0s - loss: 7.6605 - acc: 0.488 - ETA: 0s - loss: 7.6632 - acc: 0.488 - ETA: 0s - loss: 7.6571 - acc: 0.489 - ETA: 0s - loss: 7.6403 - acc: 0.489 - 4s 615us/step - loss: 7.6437 - acc: 0.4892 - val_loss: 8.3132 - val_acc: 0.3988\n",
      "\n",
      "Epoch 00007: val_loss improved from 8.34437 to 8.31317, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 5.7839 - acc: 0.600 - ETA: 3s - loss: 7.4853 - acc: 0.525 - ETA: 3s - loss: 7.9948 - acc: 0.490 - ETA: 3s - loss: 7.5235 - acc: 0.521 - ETA: 3s - loss: 7.3236 - acc: 0.535 - ETA: 3s - loss: 7.2175 - acc: 0.534 - ETA: 3s - loss: 7.1336 - acc: 0.537 - ETA: 3s - loss: 7.2059 - acc: 0.533 - ETA: 3s - loss: 7.3138 - acc: 0.527 - ETA: 3s - loss: 7.2558 - acc: 0.531 - ETA: 3s - loss: 7.2205 - acc: 0.532 - ETA: 3s - loss: 7.3379 - acc: 0.526 - ETA: 3s - loss: 7.3675 - acc: 0.523 - ETA: 3s - loss: 7.2973 - acc: 0.525 - ETA: 3s - loss: 7.3253 - acc: 0.525 - ETA: 3s - loss: 7.3951 - acc: 0.522 - ETA: 3s - loss: 7.3888 - acc: 0.520 - ETA: 3s - loss: 7.2582 - acc: 0.525 - ETA: 3s - loss: 7.3560 - acc: 0.519 - ETA: 3s - loss: 7.4267 - acc: 0.514 - ETA: 3s - loss: 7.3673 - acc: 0.517 - ETA: 3s - loss: 7.3524 - acc: 0.518 - ETA: 3s - loss: 7.4109 - acc: 0.514 - ETA: 2s - loss: 7.4880 - acc: 0.510 - ETA: 2s - loss: 7.5108 - acc: 0.509 - ETA: 2s - loss: 7.4705 - acc: 0.510 - ETA: 2s - loss: 7.4815 - acc: 0.509 - ETA: 2s - loss: 7.4786 - acc: 0.510 - ETA: 2s - loss: 7.4747 - acc: 0.510 - ETA: 2s - loss: 7.5042 - acc: 0.508 - ETA: 2s - loss: 7.5310 - acc: 0.506 - ETA: 2s - loss: 7.5462 - acc: 0.506 - ETA: 2s - loss: 7.5953 - acc: 0.503 - ETA: 2s - loss: 7.6439 - acc: 0.500 - ETA: 2s - loss: 7.6692 - acc: 0.498 - ETA: 2s - loss: 7.6533 - acc: 0.499 - ETA: 2s - loss: 7.6716 - acc: 0.498 - ETA: 2s - loss: 7.6554 - acc: 0.499 - ETA: 2s - loss: 7.6549 - acc: 0.498 - ETA: 2s - loss: 7.6671 - acc: 0.497 - ETA: 2s - loss: 7.6860 - acc: 0.496 - ETA: 2s - loss: 7.6806 - acc: 0.495 - ETA: 1s - loss: 7.6679 - acc: 0.496 - ETA: 1s - loss: 7.6535 - acc: 0.496 - ETA: 1s - loss: 7.6350 - acc: 0.498 - ETA: 1s - loss: 7.6435 - acc: 0.497 - ETA: 1s - loss: 7.6579 - acc: 0.496 - ETA: 1s - loss: 7.6821 - acc: 0.494 - ETA: 1s - loss: 7.6632 - acc: 0.495 - ETA: 1s - loss: 7.6566 - acc: 0.495 - ETA: 1s - loss: 7.6498 - acc: 0.496 - ETA: 1s - loss: 7.6241 - acc: 0.498 - ETA: 1s - loss: 7.6140 - acc: 0.499 - ETA: 1s - loss: 7.6086 - acc: 0.499 - ETA: 1s - loss: 7.6289 - acc: 0.498 - ETA: 1s - loss: 7.6160 - acc: 0.499 - ETA: 1s - loss: 7.6210 - acc: 0.499 - ETA: 1s - loss: 7.6044 - acc: 0.500 - ETA: 1s - loss: 7.5999 - acc: 0.500 - ETA: 1s - loss: 7.6043 - acc: 0.501 - ETA: 1s - loss: 7.6077 - acc: 0.501 - ETA: 1s - loss: 7.6240 - acc: 0.500 - ETA: 0s - loss: 7.5998 - acc: 0.502 - ETA: 0s - loss: 7.5980 - acc: 0.502 - ETA: 0s - loss: 7.5774 - acc: 0.503 - ETA: 0s - loss: 7.5733 - acc: 0.503 - ETA: 0s - loss: 7.5818 - acc: 0.502 - ETA: 0s - loss: 7.5794 - acc: 0.502 - ETA: 0s - loss: 7.5764 - acc: 0.503 - ETA: 0s - loss: 7.5626 - acc: 0.503 - ETA: 0s - loss: 7.5678 - acc: 0.503 - ETA: 0s - loss: 7.5588 - acc: 0.503 - ETA: 0s - loss: 7.5637 - acc: 0.503 - ETA: 0s - loss: 7.5514 - acc: 0.503 - ETA: 0s - loss: 7.5442 - acc: 0.504 - ETA: 0s - loss: 7.5345 - acc: 0.504 - ETA: 0s - loss: 7.5309 - acc: 0.504 - ETA: 0s - loss: 7.5453 - acc: 0.504 - ETA: 0s - loss: 7.5327 - acc: 0.504 - ETA: 0s - loss: 7.5206 - acc: 0.505 - 5s 680us/step - loss: 7.5147 - acc: 0.5055 - val_loss: 8.2135 - val_acc: 0.4132\n",
      "\n",
      "Epoch 00008: val_loss improved from 8.31317 to 8.21352, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 9/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 7.3846 - acc: 0.500 - ETA: 4s - loss: 7.7538 - acc: 0.491 - ETA: 4s - loss: 8.5304 - acc: 0.445 - ETA: 3s - loss: 8.4322 - acc: 0.453 - ETA: 3s - loss: 7.7840 - acc: 0.497 - ETA: 3s - loss: 7.5927 - acc: 0.513 - ETA: 3s - loss: 7.5798 - acc: 0.513 - ETA: 3s - loss: 7.3513 - acc: 0.528 - ETA: 3s - loss: 7.2828 - acc: 0.533 - ETA: 3s - loss: 7.2836 - acc: 0.532 - ETA: 3s - loss: 7.3124 - acc: 0.528 - ETA: 3s - loss: 7.2456 - acc: 0.531 - ETA: 3s - loss: 7.1159 - acc: 0.539 - ETA: 3s - loss: 7.0744 - acc: 0.543 - ETA: 3s - loss: 7.1148 - acc: 0.538 - ETA: 3s - loss: 7.1202 - acc: 0.538 - ETA: 3s - loss: 7.0930 - acc: 0.540 - ETA: 3s - loss: 7.1501 - acc: 0.536 - ETA: 3s - loss: 7.1693 - acc: 0.534 - ETA: 3s - loss: 7.2180 - acc: 0.531 - ETA: 2s - loss: 7.1797 - acc: 0.534 - ETA: 2s - loss: 7.2591 - acc: 0.529 - ETA: 2s - loss: 7.2788 - acc: 0.528 - ETA: 2s - loss: 7.3816 - acc: 0.522 - ETA: 2s - loss: 7.4218 - acc: 0.519 - ETA: 2s - loss: 7.4391 - acc: 0.518 - ETA: 2s - loss: 7.3949 - acc: 0.520 - ETA: 2s - loss: 7.4032 - acc: 0.521 - ETA: 2s - loss: 7.3665 - acc: 0.523 - ETA: 2s - loss: 7.3920 - acc: 0.521 - ETA: 2s - loss: 7.3779 - acc: 0.522 - ETA: 2s - loss: 7.4301 - acc: 0.519 - ETA: 2s - loss: 7.3925 - acc: 0.520 - ETA: 2s - loss: 7.3971 - acc: 0.520 - ETA: 2s - loss: 7.4072 - acc: 0.519 - ETA: 2s - loss: 7.4070 - acc: 0.519 - ETA: 2s - loss: 7.4313 - acc: 0.517 - ETA: 2s - loss: 7.4367 - acc: 0.516 - ETA: 2s - loss: 7.4292 - acc: 0.516 - ETA: 2s - loss: 7.4223 - acc: 0.517 - ETA: 1s - loss: 7.4304 - acc: 0.516 - ETA: 1s - loss: 7.4078 - acc: 0.517 - ETA: 1s - loss: 7.4441 - acc: 0.515 - ETA: 1s - loss: 7.4375 - acc: 0.515 - ETA: 1s - loss: 7.4505 - acc: 0.514 - ETA: 1s - loss: 7.4317 - acc: 0.515 - ETA: 1s - loss: 7.3786 - acc: 0.518 - ETA: 1s - loss: 7.3760 - acc: 0.518 - ETA: 1s - loss: 7.3648 - acc: 0.519 - ETA: 1s - loss: 7.3603 - acc: 0.519 - ETA: 1s - loss: 7.3736 - acc: 0.519 - ETA: 1s - loss: 7.3876 - acc: 0.518 - ETA: 1s - loss: 7.3728 - acc: 0.518 - ETA: 1s - loss: 7.3867 - acc: 0.518 - ETA: 1s - loss: 7.3921 - acc: 0.517 - ETA: 1s - loss: 7.4006 - acc: 0.517 - ETA: 1s - loss: 7.3984 - acc: 0.517 - ETA: 1s - loss: 7.3914 - acc: 0.517 - ETA: 1s - loss: 7.3944 - acc: 0.517 - ETA: 0s - loss: 7.4097 - acc: 0.516 - ETA: 0s - loss: 7.3987 - acc: 0.516 - ETA: 0s - loss: 7.4120 - acc: 0.516 - ETA: 0s - loss: 7.4274 - acc: 0.515 - ETA: 0s - loss: 7.4451 - acc: 0.514 - ETA: 0s - loss: 7.4553 - acc: 0.514 - ETA: 0s - loss: 7.4626 - acc: 0.513 - ETA: 0s - loss: 7.4697 - acc: 0.512 - ETA: 0s - loss: 7.4702 - acc: 0.512 - ETA: 0s - loss: 7.4571 - acc: 0.513 - ETA: 0s - loss: 7.4691 - acc: 0.512 - ETA: 0s - loss: 7.4610 - acc: 0.513 - ETA: 0s - loss: 7.4540 - acc: 0.513 - ETA: 0s - loss: 7.4273 - acc: 0.514 - ETA: 0s - loss: 7.4132 - acc: 0.515 - ETA: 0s - loss: 7.4213 - acc: 0.514 - ETA: 0s - loss: 7.4397 - acc: 0.513 - ETA: 0s - loss: 7.4178 - acc: 0.514 - 4s 664us/step - loss: 7.4246 - acc: 0.5142 - val_loss: 8.1497 - val_acc: 0.4132\n",
      "\n",
      "Epoch 00009: val_loss improved from 8.21352 to 8.14966, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 8s - loss: 11.2851 - acc: 0.30 - ETA: 4s - loss: 7.7227 - acc: 0.4917 - ETA: 4s - loss: 7.2640 - acc: 0.520 - ETA: 3s - loss: 7.5969 - acc: 0.500 - ETA: 3s - loss: 7.3666 - acc: 0.520 - ETA: 3s - loss: 7.3460 - acc: 0.526 - ETA: 3s - loss: 7.2821 - acc: 0.531 - ETA: 3s - loss: 7.4077 - acc: 0.525 - ETA: 3s - loss: 7.3923 - acc: 0.527 - ETA: 3s - loss: 7.4156 - acc: 0.527 - ETA: 3s - loss: 7.5100 - acc: 0.521 - ETA: 3s - loss: 7.5984 - acc: 0.516 - ETA: 3s - loss: 7.6827 - acc: 0.512 - ETA: 3s - loss: 7.6457 - acc: 0.514 - ETA: 3s - loss: 7.6061 - acc: 0.516 - ETA: 3s - loss: 7.6306 - acc: 0.515 - ETA: 3s - loss: 7.6177 - acc: 0.514 - ETA: 3s - loss: 7.5774 - acc: 0.516 - ETA: 2s - loss: 7.5966 - acc: 0.516 - ETA: 2s - loss: 7.5538 - acc: 0.516 - ETA: 2s - loss: 7.5120 - acc: 0.518 - ETA: 2s - loss: 7.5441 - acc: 0.516 - ETA: 2s - loss: 7.5462 - acc: 0.516 - ETA: 2s - loss: 7.5009 - acc: 0.519 - ETA: 2s - loss: 7.4393 - acc: 0.523 - ETA: 2s - loss: 7.4223 - acc: 0.524 - ETA: 2s - loss: 7.4022 - acc: 0.524 - ETA: 2s - loss: 7.3979 - acc: 0.524 - ETA: 2s - loss: 7.4312 - acc: 0.523 - ETA: 2s - loss: 7.4562 - acc: 0.522 - ETA: 2s - loss: 7.4681 - acc: 0.521 - ETA: 2s - loss: 7.4613 - acc: 0.522 - ETA: 2s - loss: 7.4546 - acc: 0.522 - ETA: 2s - loss: 7.4387 - acc: 0.522 - ETA: 2s - loss: 7.4705 - acc: 0.520 - ETA: 2s - loss: 7.4913 - acc: 0.519 - ETA: 1s - loss: 7.4637 - acc: 0.521 - ETA: 1s - loss: 7.4463 - acc: 0.522 - ETA: 1s - loss: 7.4078 - acc: 0.524 - ETA: 1s - loss: 7.4314 - acc: 0.523 - ETA: 1s - loss: 7.4277 - acc: 0.523 - ETA: 1s - loss: 7.4474 - acc: 0.521 - ETA: 1s - loss: 7.4425 - acc: 0.521 - ETA: 1s - loss: 7.4005 - acc: 0.524 - ETA: 1s - loss: 7.4147 - acc: 0.523 - ETA: 1s - loss: 7.4087 - acc: 0.524 - ETA: 1s - loss: 7.3652 - acc: 0.526 - ETA: 1s - loss: 7.3844 - acc: 0.525 - ETA: 1s - loss: 7.3746 - acc: 0.526 - ETA: 1s - loss: 7.3757 - acc: 0.526 - ETA: 1s - loss: 7.3590 - acc: 0.526 - ETA: 1s - loss: 7.3670 - acc: 0.525 - ETA: 1s - loss: 7.3915 - acc: 0.524 - ETA: 0s - loss: 7.3772 - acc: 0.525 - ETA: 0s - loss: 7.3675 - acc: 0.526 - ETA: 0s - loss: 7.3717 - acc: 0.526 - ETA: 0s - loss: 7.3652 - acc: 0.526 - ETA: 0s - loss: 7.3394 - acc: 0.527 - ETA: 0s - loss: 7.3381 - acc: 0.528 - ETA: 0s - loss: 7.3289 - acc: 0.527 - ETA: 0s - loss: 7.3340 - acc: 0.527 - ETA: 0s - loss: 7.3251 - acc: 0.527 - ETA: 0s - loss: 7.3051 - acc: 0.529 - ETA: 0s - loss: 7.3270 - acc: 0.527 - ETA: 0s - loss: 7.3194 - acc: 0.527 - ETA: 0s - loss: 7.3275 - acc: 0.527 - ETA: 0s - loss: 7.3143 - acc: 0.528 - ETA: 0s - loss: 7.3145 - acc: 0.528 - ETA: 0s - loss: 7.3050 - acc: 0.528 - ETA: 0s - loss: 7.3103 - acc: 0.528 - ETA: 0s - loss: 7.3079 - acc: 0.528 - ETA: 0s - loss: 7.3176 - acc: 0.527 - ETA: 0s - loss: 7.3276 - acc: 0.526 - 4s 634us/step - loss: 7.3252 - acc: 0.5265 - val_loss: 7.9484 - val_acc: 0.4144\n",
      "\n",
      "Epoch 00010: val_loss improved from 8.14966 to 7.94842, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - ETA: 8s - loss: 8.0872 - acc: 0.500 - ETA: 4s - loss: 7.3674 - acc: 0.541 - ETA: 4s - loss: 6.9614 - acc: 0.563 - ETA: 3s - loss: 7.3900 - acc: 0.528 - ETA: 3s - loss: 7.5142 - acc: 0.514 - ETA: 3s - loss: 7.7491 - acc: 0.501 - ETA: 3s - loss: 7.8057 - acc: 0.495 - ETA: 3s - loss: 7.5873 - acc: 0.502 - ETA: 3s - loss: 7.5938 - acc: 0.502 - ETA: 3s - loss: 7.4933 - acc: 0.510 - ETA: 3s - loss: 7.3925 - acc: 0.518 - ETA: 3s - loss: 7.3270 - acc: 0.524 - ETA: 3s - loss: 7.2752 - acc: 0.528 - ETA: 3s - loss: 7.1747 - acc: 0.534 - ETA: 3s - loss: 7.1545 - acc: 0.536 - ETA: 3s - loss: 7.2837 - acc: 0.530 - ETA: 3s - loss: 7.2470 - acc: 0.530 - ETA: 3s - loss: 7.1774 - acc: 0.534 - ETA: 2s - loss: 7.2618 - acc: 0.529 - ETA: 2s - loss: 7.2945 - acc: 0.526 - ETA: 2s - loss: 7.3090 - acc: 0.525 - ETA: 2s - loss: 7.2789 - acc: 0.525 - ETA: 2s - loss: 7.2481 - acc: 0.527 - ETA: 2s - loss: 7.3012 - acc: 0.524 - ETA: 2s - loss: 7.3162 - acc: 0.522 - ETA: 2s - loss: 7.2991 - acc: 0.522 - ETA: 2s - loss: 7.2374 - acc: 0.525 - ETA: 2s - loss: 7.2150 - acc: 0.526 - ETA: 2s - loss: 7.2344 - acc: 0.525 - ETA: 2s - loss: 7.1867 - acc: 0.528 - ETA: 2s - loss: 7.1760 - acc: 0.529 - ETA: 2s - loss: 7.1817 - acc: 0.529 - ETA: 2s - loss: 7.1734 - acc: 0.530 - ETA: 2s - loss: 7.1484 - acc: 0.532 - ETA: 2s - loss: 7.1263 - acc: 0.533 - ETA: 2s - loss: 7.1520 - acc: 0.532 - ETA: 2s - loss: 7.1706 - acc: 0.531 - ETA: 2s - loss: 7.1699 - acc: 0.531 - ETA: 1s - loss: 7.1479 - acc: 0.533 - ETA: 1s - loss: 7.1429 - acc: 0.533 - ETA: 1s - loss: 7.1257 - acc: 0.534 - ETA: 1s - loss: 7.1213 - acc: 0.534 - ETA: 1s - loss: 7.1344 - acc: 0.533 - ETA: 1s - loss: 7.1347 - acc: 0.533 - ETA: 1s - loss: 7.1476 - acc: 0.532 - ETA: 1s - loss: 7.1631 - acc: 0.531 - ETA: 1s - loss: 7.1477 - acc: 0.532 - ETA: 1s - loss: 7.1876 - acc: 0.530 - ETA: 1s - loss: 7.1523 - acc: 0.532 - ETA: 1s - loss: 7.1487 - acc: 0.532 - ETA: 1s - loss: 7.1575 - acc: 0.532 - ETA: 1s - loss: 7.1496 - acc: 0.533 - ETA: 1s - loss: 7.1402 - acc: 0.533 - ETA: 1s - loss: 7.1398 - acc: 0.533 - ETA: 1s - loss: 7.1440 - acc: 0.533 - ETA: 1s - loss: 7.1371 - acc: 0.533 - ETA: 1s - loss: 7.1527 - acc: 0.532 - ETA: 0s - loss: 7.1334 - acc: 0.533 - ETA: 0s - loss: 7.1373 - acc: 0.533 - ETA: 0s - loss: 7.1487 - acc: 0.533 - ETA: 0s - loss: 7.1508 - acc: 0.533 - ETA: 0s - loss: 7.1330 - acc: 0.534 - ETA: 0s - loss: 7.1360 - acc: 0.534 - ETA: 0s - loss: 7.1400 - acc: 0.534 - ETA: 0s - loss: 7.1192 - acc: 0.536 - ETA: 0s - loss: 7.1179 - acc: 0.536 - ETA: 0s - loss: 7.1236 - acc: 0.535 - ETA: 0s - loss: 7.0978 - acc: 0.537 - ETA: 0s - loss: 7.0923 - acc: 0.537 - ETA: 0s - loss: 7.0820 - acc: 0.538 - ETA: 0s - loss: 7.0794 - acc: 0.538 - ETA: 0s - loss: 7.0632 - acc: 0.540 - ETA: 0s - loss: 7.0712 - acc: 0.539 - ETA: 0s - loss: 7.0668 - acc: 0.540 - ETA: 0s - loss: 7.1085 - acc: 0.537 - 4s 657us/step - loss: 7.1078 - acc: 0.5376 - val_loss: 7.9042 - val_acc: 0.4311\n",
      "\n",
      "Epoch 00011: val_loss improved from 7.94842 to 7.90421, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 7s - loss: 8.0634 - acc: 0.500 - ETA: 4s - loss: 7.5404 - acc: 0.525 - ETA: 4s - loss: 7.8985 - acc: 0.490 - ETA: 4s - loss: 7.7465 - acc: 0.500 - ETA: 3s - loss: 7.5297 - acc: 0.515 - ETA: 3s - loss: 7.3729 - acc: 0.525 - ETA: 3s - loss: 7.1315 - acc: 0.543 - ETA: 3s - loss: 7.0265 - acc: 0.550 - ETA: 3s - loss: 7.0969 - acc: 0.545 - ETA: 3s - loss: 7.2537 - acc: 0.535 - ETA: 3s - loss: 7.2445 - acc: 0.534 - ETA: 3s - loss: 7.1506 - acc: 0.539 - ETA: 3s - loss: 7.2421 - acc: 0.533 - ETA: 3s - loss: 7.2376 - acc: 0.533 - ETA: 3s - loss: 7.1823 - acc: 0.536 - ETA: 3s - loss: 7.1189 - acc: 0.542 - ETA: 3s - loss: 7.0425 - acc: 0.547 - ETA: 3s - loss: 6.9779 - acc: 0.551 - ETA: 3s - loss: 6.9641 - acc: 0.552 - ETA: 2s - loss: 6.9859 - acc: 0.550 - ETA: 2s - loss: 7.0140 - acc: 0.547 - ETA: 2s - loss: 6.9784 - acc: 0.550 - ETA: 2s - loss: 6.9655 - acc: 0.550 - ETA: 2s - loss: 7.0236 - acc: 0.546 - ETA: 2s - loss: 7.1052 - acc: 0.541 - ETA: 2s - loss: 7.0913 - acc: 0.541 - ETA: 2s - loss: 7.0533 - acc: 0.543 - ETA: 2s - loss: 7.0279 - acc: 0.545 - ETA: 2s - loss: 7.0257 - acc: 0.545 - ETA: 2s - loss: 7.0167 - acc: 0.546 - ETA: 2s - loss: 7.0052 - acc: 0.547 - ETA: 2s - loss: 6.9997 - acc: 0.547 - ETA: 2s - loss: 7.0190 - acc: 0.546 - ETA: 2s - loss: 6.9972 - acc: 0.547 - ETA: 2s - loss: 6.9875 - acc: 0.548 - ETA: 1s - loss: 7.0077 - acc: 0.546 - ETA: 1s - loss: 7.0360 - acc: 0.544 - ETA: 1s - loss: 7.0044 - acc: 0.545 - ETA: 1s - loss: 6.9835 - acc: 0.547 - ETA: 1s - loss: 6.9938 - acc: 0.547 - ETA: 1s - loss: 6.9810 - acc: 0.547 - ETA: 1s - loss: 7.0047 - acc: 0.546 - ETA: 1s - loss: 6.9983 - acc: 0.547 - ETA: 1s - loss: 6.9833 - acc: 0.548 - ETA: 1s - loss: 6.9837 - acc: 0.548 - ETA: 1s - loss: 6.9625 - acc: 0.549 - ETA: 1s - loss: 6.9728 - acc: 0.548 - ETA: 1s - loss: 6.9690 - acc: 0.548 - ETA: 1s - loss: 6.9465 - acc: 0.550 - ETA: 1s - loss: 6.9330 - acc: 0.551 - ETA: 1s - loss: 6.9405 - acc: 0.550 - ETA: 1s - loss: 6.9260 - acc: 0.550 - ETA: 1s - loss: 6.9073 - acc: 0.551 - ETA: 1s - loss: 6.9252 - acc: 0.550 - ETA: 0s - loss: 6.9208 - acc: 0.551 - ETA: 0s - loss: 6.9433 - acc: 0.550 - ETA: 0s - loss: 6.9306 - acc: 0.551 - ETA: 0s - loss: 6.9274 - acc: 0.551 - ETA: 0s - loss: 6.9460 - acc: 0.550 - ETA: 0s - loss: 6.9352 - acc: 0.550 - ETA: 0s - loss: 6.9434 - acc: 0.550 - ETA: 0s - loss: 6.9492 - acc: 0.549 - ETA: 0s - loss: 6.9733 - acc: 0.548 - ETA: 0s - loss: 6.9649 - acc: 0.548 - ETA: 0s - loss: 6.9844 - acc: 0.547 - ETA: 0s - loss: 6.9746 - acc: 0.548 - ETA: 0s - loss: 6.9741 - acc: 0.548 - ETA: 0s - loss: 6.9634 - acc: 0.549 - ETA: 0s - loss: 6.9744 - acc: 0.548 - ETA: 0s - loss: 6.9951 - acc: 0.547 - ETA: 0s - loss: 6.9886 - acc: 0.547 - ETA: 0s - loss: 6.9957 - acc: 0.547 - ETA: 0s - loss: 7.0072 - acc: 0.546 - 4s 642us/step - loss: 7.0195 - acc: 0.5458 - val_loss: 7.7700 - val_acc: 0.4323\n",
      "\n",
      "Epoch 00012: val_loss improved from 7.90421 to 7.76996, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 13/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 6.4775 - acc: 0.600 - ETA: 4s - loss: 5.8860 - acc: 0.610 - ETA: 4s - loss: 5.8712 - acc: 0.622 - ETA: 4s - loss: 6.6072 - acc: 0.580 - ETA: 4s - loss: 6.9015 - acc: 0.564 - ETA: 3s - loss: 6.7255 - acc: 0.577 - ETA: 3s - loss: 6.7188 - acc: 0.576 - ETA: 3s - loss: 6.5513 - acc: 0.586 - ETA: 3s - loss: 6.5304 - acc: 0.583 - ETA: 3s - loss: 6.6927 - acc: 0.573 - ETA: 3s - loss: 6.7883 - acc: 0.567 - ETA: 3s - loss: 6.9066 - acc: 0.561 - ETA: 3s - loss: 6.9265 - acc: 0.560 - ETA: 3s - loss: 6.8787 - acc: 0.564 - ETA: 3s - loss: 6.8749 - acc: 0.565 - ETA: 3s - loss: 6.9545 - acc: 0.560 - ETA: 3s - loss: 6.9157 - acc: 0.563 - ETA: 3s - loss: 6.8828 - acc: 0.565 - ETA: 3s - loss: 6.9133 - acc: 0.563 - ETA: 3s - loss: 6.8315 - acc: 0.568 - ETA: 3s - loss: 6.8810 - acc: 0.565 - ETA: 3s - loss: 6.8642 - acc: 0.565 - ETA: 3s - loss: 6.8576 - acc: 0.565 - ETA: 3s - loss: 6.8549 - acc: 0.565 - ETA: 2s - loss: 6.8562 - acc: 0.564 - ETA: 2s - loss: 6.8720 - acc: 0.563 - ETA: 2s - loss: 6.8597 - acc: 0.563 - ETA: 2s - loss: 6.8506 - acc: 0.563 - ETA: 2s - loss: 6.8595 - acc: 0.562 - ETA: 2s - loss: 6.8627 - acc: 0.563 - ETA: 2s - loss: 6.8192 - acc: 0.566 - ETA: 2s - loss: 6.8178 - acc: 0.566 - ETA: 2s - loss: 6.8592 - acc: 0.564 - ETA: 2s - loss: 6.8572 - acc: 0.564 - ETA: 2s - loss: 6.8178 - acc: 0.566 - ETA: 2s - loss: 6.8688 - acc: 0.563 - ETA: 2s - loss: 6.7999 - acc: 0.567 - ETA: 2s - loss: 6.8925 - acc: 0.561 - ETA: 2s - loss: 6.8986 - acc: 0.561 - ETA: 1s - loss: 6.8975 - acc: 0.562 - ETA: 1s - loss: 6.8882 - acc: 0.562 - ETA: 1s - loss: 6.8943 - acc: 0.562 - ETA: 1s - loss: 6.8954 - acc: 0.562 - ETA: 1s - loss: 6.9096 - acc: 0.561 - ETA: 1s - loss: 6.9307 - acc: 0.559 - ETA: 1s - loss: 6.9279 - acc: 0.559 - ETA: 1s - loss: 6.9345 - acc: 0.559 - ETA: 1s - loss: 6.9361 - acc: 0.559 - ETA: 1s - loss: 6.9384 - acc: 0.559 - ETA: 1s - loss: 6.9423 - acc: 0.558 - ETA: 1s - loss: 6.9411 - acc: 0.558 - ETA: 1s - loss: 6.9282 - acc: 0.559 - ETA: 1s - loss: 6.9227 - acc: 0.559 - ETA: 1s - loss: 6.9228 - acc: 0.559 - ETA: 1s - loss: 6.9330 - acc: 0.558 - ETA: 1s - loss: 6.9476 - acc: 0.557 - ETA: 1s - loss: 6.9526 - acc: 0.557 - ETA: 0s - loss: 6.9415 - acc: 0.558 - ETA: 0s - loss: 6.9212 - acc: 0.559 - ETA: 0s - loss: 6.9299 - acc: 0.558 - ETA: 0s - loss: 6.9579 - acc: 0.557 - ETA: 0s - loss: 6.9448 - acc: 0.557 - ETA: 0s - loss: 6.9398 - acc: 0.558 - ETA: 0s - loss: 6.9541 - acc: 0.557 - ETA: 0s - loss: 6.9640 - acc: 0.557 - ETA: 0s - loss: 6.9646 - acc: 0.556 - ETA: 0s - loss: 6.9528 - acc: 0.557 - ETA: 0s - loss: 6.9364 - acc: 0.558 - ETA: 0s - loss: 6.9406 - acc: 0.558 - ETA: 0s - loss: 6.9457 - acc: 0.558 - ETA: 0s - loss: 6.9563 - acc: 0.557 - ETA: 0s - loss: 6.9689 - acc: 0.556 - ETA: 0s - loss: 6.9770 - acc: 0.556 - ETA: 0s - loss: 6.9839 - acc: 0.556 - ETA: 0s - loss: 6.9837 - acc: 0.555 - ETA: 0s - loss: 6.9637 - acc: 0.556 - 4s 657us/step - loss: 6.9623 - acc: 0.5569 - val_loss: 7.8714 - val_acc: 0.4216\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 4.8727 - acc: 0.650 - ETA: 4s - loss: 7.1359 - acc: 0.550 - ETA: 3s - loss: 7.1170 - acc: 0.554 - ETA: 3s - loss: 7.1695 - acc: 0.550 - ETA: 3s - loss: 7.4317 - acc: 0.532 - ETA: 3s - loss: 7.3645 - acc: 0.538 - ETA: 3s - loss: 7.2400 - acc: 0.545 - ETA: 3s - loss: 7.0813 - acc: 0.555 - ETA: 3s - loss: 7.0388 - acc: 0.557 - ETA: 3s - loss: 7.0463 - acc: 0.556 - ETA: 3s - loss: 7.0132 - acc: 0.559 - ETA: 3s - loss: 7.1121 - acc: 0.551 - ETA: 3s - loss: 7.0251 - acc: 0.557 - ETA: 3s - loss: 7.0827 - acc: 0.554 - ETA: 3s - loss: 7.0824 - acc: 0.553 - ETA: 3s - loss: 7.0271 - acc: 0.557 - ETA: 3s - loss: 7.0006 - acc: 0.559 - ETA: 3s - loss: 6.9417 - acc: 0.562 - ETA: 2s - loss: 7.0002 - acc: 0.557 - ETA: 2s - loss: 6.9679 - acc: 0.559 - ETA: 2s - loss: 6.9493 - acc: 0.561 - ETA: 2s - loss: 6.9414 - acc: 0.560 - ETA: 2s - loss: 6.9538 - acc: 0.558 - ETA: 2s - loss: 6.9641 - acc: 0.556 - ETA: 2s - loss: 6.9495 - acc: 0.557 - ETA: 2s - loss: 6.9879 - acc: 0.555 - ETA: 2s - loss: 6.9777 - acc: 0.556 - ETA: 2s - loss: 6.9930 - acc: 0.554 - ETA: 2s - loss: 7.0163 - acc: 0.552 - ETA: 2s - loss: 7.0550 - acc: 0.550 - ETA: 2s - loss: 7.0778 - acc: 0.548 - ETA: 2s - loss: 7.1093 - acc: 0.546 - ETA: 2s - loss: 7.1599 - acc: 0.543 - ETA: 2s - loss: 7.1949 - acc: 0.541 - ETA: 2s - loss: 7.1644 - acc: 0.543 - ETA: 2s - loss: 7.1630 - acc: 0.543 - ETA: 2s - loss: 7.1604 - acc: 0.543 - ETA: 1s - loss: 7.0981 - acc: 0.547 - ETA: 1s - loss: 7.0787 - acc: 0.549 - ETA: 1s - loss: 7.0853 - acc: 0.548 - ETA: 1s - loss: 7.0785 - acc: 0.549 - ETA: 1s - loss: 7.0784 - acc: 0.548 - ETA: 1s - loss: 7.0742 - acc: 0.549 - ETA: 1s - loss: 7.0819 - acc: 0.548 - ETA: 1s - loss: 7.0665 - acc: 0.549 - ETA: 1s - loss: 7.0392 - acc: 0.550 - ETA: 1s - loss: 7.0007 - acc: 0.552 - ETA: 1s - loss: 7.0179 - acc: 0.551 - ETA: 1s - loss: 6.9985 - acc: 0.552 - ETA: 1s - loss: 6.9714 - acc: 0.554 - ETA: 1s - loss: 6.9743 - acc: 0.554 - ETA: 1s - loss: 6.9642 - acc: 0.555 - ETA: 1s - loss: 6.9742 - acc: 0.554 - ETA: 1s - loss: 6.9739 - acc: 0.554 - ETA: 1s - loss: 6.9622 - acc: 0.554 - ETA: 1s - loss: 6.9599 - acc: 0.554 - ETA: 1s - loss: 6.9714 - acc: 0.554 - ETA: 0s - loss: 6.9511 - acc: 0.555 - ETA: 0s - loss: 6.9484 - acc: 0.555 - ETA: 0s - loss: 6.9655 - acc: 0.554 - ETA: 0s - loss: 6.9685 - acc: 0.554 - ETA: 0s - loss: 6.9539 - acc: 0.554 - ETA: 0s - loss: 6.9507 - acc: 0.554 - ETA: 0s - loss: 6.9615 - acc: 0.554 - ETA: 0s - loss: 6.9747 - acc: 0.553 - ETA: 0s - loss: 6.9683 - acc: 0.553 - ETA: 0s - loss: 6.9695 - acc: 0.553 - ETA: 0s - loss: 6.9403 - acc: 0.555 - ETA: 0s - loss: 6.9276 - acc: 0.556 - ETA: 0s - loss: 6.9336 - acc: 0.556 - ETA: 0s - loss: 6.9330 - acc: 0.556 - ETA: 0s - loss: 6.9085 - acc: 0.557 - ETA: 0s - loss: 6.8950 - acc: 0.558 - ETA: 0s - loss: 6.8874 - acc: 0.559 - ETA: 0s - loss: 6.8949 - acc: 0.558 - ETA: 0s - loss: 6.9069 - acc: 0.558 - ETA: 0s - loss: 6.9039 - acc: 0.558 - ETA: 0s - loss: 6.9000 - acc: 0.558 - 5s 676us/step - loss: 6.8962 - acc: 0.5590 - val_loss: 7.8451 - val_acc: 0.4383\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - ETA: 10s - loss: 5.2067 - acc: 0.65 - ETA: 5s - loss: 5.8467 - acc: 0.6250 - ETA: 4s - loss: 5.7673 - acc: 0.635 - ETA: 4s - loss: 6.0802 - acc: 0.617 - ETA: 4s - loss: 6.2974 - acc: 0.605 - ETA: 4s - loss: 6.2896 - acc: 0.606 - ETA: 4s - loss: 6.2240 - acc: 0.609 - ETA: 4s - loss: 6.4222 - acc: 0.596 - ETA: 4s - loss: 6.5439 - acc: 0.589 - ETA: 4s - loss: 6.6824 - acc: 0.581 - ETA: 4s - loss: 6.6811 - acc: 0.581 - ETA: 4s - loss: 6.6399 - acc: 0.583 - ETA: 3s - loss: 6.6595 - acc: 0.581 - ETA: 3s - loss: 6.6454 - acc: 0.581 - ETA: 3s - loss: 6.7295 - acc: 0.576 - ETA: 3s - loss: 6.8018 - acc: 0.569 - ETA: 3s - loss: 6.7805 - acc: 0.571 - ETA: 3s - loss: 6.7416 - acc: 0.572 - ETA: 3s - loss: 6.7559 - acc: 0.572 - ETA: 3s - loss: 6.7693 - acc: 0.571 - ETA: 3s - loss: 6.7733 - acc: 0.572 - ETA: 3s - loss: 6.7675 - acc: 0.572 - ETA: 3s - loss: 6.7013 - acc: 0.576 - ETA: 3s - loss: 6.7395 - acc: 0.572 - ETA: 3s - loss: 6.7637 - acc: 0.571 - ETA: 3s - loss: 6.7592 - acc: 0.572 - ETA: 3s - loss: 6.7383 - acc: 0.572 - ETA: 3s - loss: 6.7658 - acc: 0.570 - ETA: 3s - loss: 6.7267 - acc: 0.573 - ETA: 2s - loss: 6.7414 - acc: 0.572 - ETA: 2s - loss: 6.7452 - acc: 0.572 - ETA: 2s - loss: 6.7109 - acc: 0.574 - ETA: 2s - loss: 6.7153 - acc: 0.574 - ETA: 2s - loss: 6.7018 - acc: 0.575 - ETA: 2s - loss: 6.7181 - acc: 0.574 - ETA: 2s - loss: 6.6995 - acc: 0.575 - ETA: 2s - loss: 6.7038 - acc: 0.575 - ETA: 2s - loss: 6.7092 - acc: 0.575 - ETA: 2s - loss: 6.7116 - acc: 0.575 - ETA: 2s - loss: 6.6756 - acc: 0.577 - ETA: 2s - loss: 6.6600 - acc: 0.578 - ETA: 2s - loss: 6.6845 - acc: 0.576 - ETA: 2s - loss: 6.6849 - acc: 0.576 - ETA: 2s - loss: 6.6844 - acc: 0.576 - ETA: 2s - loss: 6.6641 - acc: 0.577 - ETA: 2s - loss: 6.6864 - acc: 0.576 - ETA: 2s - loss: 6.6988 - acc: 0.575 - ETA: 1s - loss: 6.6894 - acc: 0.576 - ETA: 1s - loss: 6.7011 - acc: 0.575 - ETA: 1s - loss: 6.6961 - acc: 0.576 - ETA: 1s - loss: 6.6915 - acc: 0.576 - ETA: 1s - loss: 6.7272 - acc: 0.574 - ETA: 1s - loss: 6.7303 - acc: 0.574 - ETA: 1s - loss: 6.7517 - acc: 0.572 - ETA: 1s - loss: 6.7530 - acc: 0.572 - ETA: 1s - loss: 6.7622 - acc: 0.572 - ETA: 1s - loss: 6.7642 - acc: 0.571 - ETA: 1s - loss: 6.7786 - acc: 0.570 - ETA: 1s - loss: 6.7981 - acc: 0.569 - ETA: 1s - loss: 6.8327 - acc: 0.567 - ETA: 1s - loss: 6.8198 - acc: 0.568 - ETA: 1s - loss: 6.8403 - acc: 0.567 - ETA: 1s - loss: 6.8280 - acc: 0.567 - ETA: 1s - loss: 6.8381 - acc: 0.567 - ETA: 0s - loss: 6.8448 - acc: 0.566 - ETA: 0s - loss: 6.8481 - acc: 0.566 - ETA: 0s - loss: 6.8523 - acc: 0.565 - ETA: 0s - loss: 6.8255 - acc: 0.567 - ETA: 0s - loss: 6.8146 - acc: 0.568 - ETA: 0s - loss: 6.8181 - acc: 0.568 - ETA: 0s - loss: 6.7977 - acc: 0.569 - ETA: 0s - loss: 6.8004 - acc: 0.568 - ETA: 0s - loss: 6.7902 - acc: 0.569 - ETA: 0s - loss: 6.7769 - acc: 0.570 - ETA: 0s - loss: 6.7885 - acc: 0.570 - ETA: 0s - loss: 6.7794 - acc: 0.570 - ETA: 0s - loss: 6.7933 - acc: 0.569 - ETA: 0s - loss: 6.8181 - acc: 0.567 - ETA: 0s - loss: 6.8216 - acc: 0.567 - ETA: 0s - loss: 6.8126 - acc: 0.568 - ETA: 0s - loss: 6.8274 - acc: 0.567 - ETA: 0s - loss: 6.8255 - acc: 0.567 - 5s 719us/step - loss: 6.8253 - acc: 0.5672 - val_loss: 7.7027 - val_acc: 0.4503\n",
      "\n",
      "Epoch 00015: val_loss improved from 7.76996 to 7.70272, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 4.8356 - acc: 0.700 - ETA: 3s - loss: 6.7169 - acc: 0.583 - ETA: 3s - loss: 6.8920 - acc: 0.572 - ETA: 3s - loss: 7.1724 - acc: 0.553 - ETA: 3s - loss: 7.0623 - acc: 0.560 - ETA: 3s - loss: 7.4046 - acc: 0.539 - ETA: 3s - loss: 7.2677 - acc: 0.548 - ETA: 3s - loss: 7.2905 - acc: 0.547 - ETA: 3s - loss: 6.9901 - acc: 0.565 - ETA: 3s - loss: 6.8871 - acc: 0.570 - ETA: 3s - loss: 6.8309 - acc: 0.572 - ETA: 3s - loss: 6.8718 - acc: 0.569 - ETA: 3s - loss: 6.8810 - acc: 0.569 - ETA: 3s - loss: 6.8983 - acc: 0.566 - ETA: 3s - loss: 6.8711 - acc: 0.568 - ETA: 3s - loss: 6.7506 - acc: 0.576 - ETA: 3s - loss: 6.8080 - acc: 0.572 - ETA: 3s - loss: 6.7874 - acc: 0.573 - ETA: 3s - loss: 6.8106 - acc: 0.572 - ETA: 2s - loss: 6.6951 - acc: 0.579 - ETA: 2s - loss: 6.6325 - acc: 0.582 - ETA: 2s - loss: 6.6951 - acc: 0.577 - ETA: 2s - loss: 6.7042 - acc: 0.576 - ETA: 2s - loss: 6.7315 - acc: 0.574 - ETA: 2s - loss: 6.6931 - acc: 0.576 - ETA: 2s - loss: 6.6783 - acc: 0.577 - ETA: 2s - loss: 6.6689 - acc: 0.578 - ETA: 2s - loss: 6.6626 - acc: 0.578 - ETA: 2s - loss: 6.7056 - acc: 0.575 - ETA: 2s - loss: 6.7022 - acc: 0.576 - ETA: 2s - loss: 6.7054 - acc: 0.575 - ETA: 2s - loss: 6.6668 - acc: 0.578 - ETA: 2s - loss: 6.6782 - acc: 0.577 - ETA: 2s - loss: 6.7173 - acc: 0.574 - ETA: 2s - loss: 6.7445 - acc: 0.573 - ETA: 2s - loss: 6.7678 - acc: 0.571 - ETA: 2s - loss: 6.7899 - acc: 0.570 - ETA: 2s - loss: 6.8171 - acc: 0.568 - ETA: 1s - loss: 6.7972 - acc: 0.569 - ETA: 1s - loss: 6.7467 - acc: 0.571 - ETA: 1s - loss: 6.7897 - acc: 0.568 - ETA: 1s - loss: 6.7655 - acc: 0.570 - ETA: 1s - loss: 6.7700 - acc: 0.570 - ETA: 1s - loss: 6.7371 - acc: 0.572 - ETA: 1s - loss: 6.7191 - acc: 0.573 - ETA: 1s - loss: 6.7126 - acc: 0.574 - ETA: 1s - loss: 6.7071 - acc: 0.574 - ETA: 1s - loss: 6.7079 - acc: 0.573 - ETA: 1s - loss: 6.7071 - acc: 0.574 - ETA: 1s - loss: 6.7180 - acc: 0.573 - ETA: 1s - loss: 6.7263 - acc: 0.572 - ETA: 1s - loss: 6.7134 - acc: 0.573 - ETA: 1s - loss: 6.6832 - acc: 0.575 - ETA: 1s - loss: 6.6595 - acc: 0.576 - ETA: 1s - loss: 6.6573 - acc: 0.576 - ETA: 1s - loss: 6.6471 - acc: 0.577 - ETA: 1s - loss: 6.6602 - acc: 0.576 - ETA: 0s - loss: 6.6720 - acc: 0.576 - ETA: 0s - loss: 6.6691 - acc: 0.576 - ETA: 0s - loss: 6.6827 - acc: 0.575 - ETA: 0s - loss: 6.6916 - acc: 0.574 - ETA: 0s - loss: 6.7049 - acc: 0.574 - ETA: 0s - loss: 6.7105 - acc: 0.573 - ETA: 0s - loss: 6.7197 - acc: 0.573 - ETA: 0s - loss: 6.7240 - acc: 0.572 - ETA: 0s - loss: 6.7328 - acc: 0.572 - ETA: 0s - loss: 6.7447 - acc: 0.570 - ETA: 0s - loss: 6.7376 - acc: 0.570 - ETA: 0s - loss: 6.7585 - acc: 0.569 - ETA: 0s - loss: 6.7718 - acc: 0.568 - ETA: 0s - loss: 6.7750 - acc: 0.567 - ETA: 0s - loss: 6.7704 - acc: 0.568 - ETA: 0s - loss: 6.7672 - acc: 0.568 - ETA: 0s - loss: 6.7797 - acc: 0.567 - ETA: 0s - loss: 6.7776 - acc: 0.567 - 4s 654us/step - loss: 6.7805 - acc: 0.5674 - val_loss: 7.5795 - val_acc: 0.4443\n",
      "\n",
      "Epoch 00016: val_loss improved from 7.70272 to 7.57946, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 17/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 5.6435 - acc: 0.650 - ETA: 4s - loss: 6.6307 - acc: 0.575 - ETA: 4s - loss: 6.6448 - acc: 0.575 - ETA: 3s - loss: 7.1368 - acc: 0.546 - ETA: 3s - loss: 6.7375 - acc: 0.573 - ETA: 3s - loss: 6.8554 - acc: 0.567 - ETA: 3s - loss: 6.8881 - acc: 0.564 - ETA: 3s - loss: 6.8054 - acc: 0.571 - ETA: 3s - loss: 6.8255 - acc: 0.568 - ETA: 3s - loss: 6.8490 - acc: 0.567 - ETA: 3s - loss: 6.7375 - acc: 0.575 - ETA: 3s - loss: 6.7687 - acc: 0.573 - ETA: 3s - loss: 6.7277 - acc: 0.575 - ETA: 3s - loss: 6.7379 - acc: 0.574 - ETA: 3s - loss: 6.8277 - acc: 0.567 - ETA: 3s - loss: 6.8038 - acc: 0.569 - ETA: 3s - loss: 6.7313 - acc: 0.573 - ETA: 3s - loss: 6.7115 - acc: 0.574 - ETA: 3s - loss: 6.6980 - acc: 0.574 - ETA: 3s - loss: 6.5678 - acc: 0.582 - ETA: 3s - loss: 6.6403 - acc: 0.578 - ETA: 3s - loss: 6.5587 - acc: 0.582 - ETA: 3s - loss: 6.5889 - acc: 0.580 - ETA: 2s - loss: 6.5927 - acc: 0.580 - ETA: 2s - loss: 6.6145 - acc: 0.579 - ETA: 2s - loss: 6.5756 - acc: 0.581 - ETA: 2s - loss: 6.6527 - acc: 0.576 - ETA: 2s - loss: 6.6416 - acc: 0.576 - ETA: 2s - loss: 6.6433 - acc: 0.576 - ETA: 2s - loss: 6.6102 - acc: 0.578 - ETA: 2s - loss: 6.6115 - acc: 0.578 - ETA: 2s - loss: 6.6083 - acc: 0.578 - ETA: 2s - loss: 6.6215 - acc: 0.577 - ETA: 2s - loss: 6.6738 - acc: 0.574 - ETA: 2s - loss: 6.6695 - acc: 0.574 - ETA: 2s - loss: 6.6249 - acc: 0.577 - ETA: 2s - loss: 6.6372 - acc: 0.576 - ETA: 2s - loss: 6.6250 - acc: 0.577 - ETA: 2s - loss: 6.6107 - acc: 0.578 - ETA: 2s - loss: 6.6270 - acc: 0.577 - ETA: 2s - loss: 6.6538 - acc: 0.575 - ETA: 2s - loss: 6.6680 - acc: 0.575 - ETA: 1s - loss: 6.6677 - acc: 0.575 - ETA: 1s - loss: 6.6863 - acc: 0.573 - ETA: 1s - loss: 6.6794 - acc: 0.574 - ETA: 1s - loss: 6.7076 - acc: 0.573 - ETA: 1s - loss: 6.6819 - acc: 0.574 - ETA: 1s - loss: 6.6703 - acc: 0.575 - ETA: 1s - loss: 6.6812 - acc: 0.574 - ETA: 1s - loss: 6.6737 - acc: 0.574 - ETA: 1s - loss: 6.6740 - acc: 0.574 - ETA: 1s - loss: 6.6513 - acc: 0.576 - ETA: 1s - loss: 6.6582 - acc: 0.575 - ETA: 1s - loss: 6.6424 - acc: 0.576 - ETA: 1s - loss: 6.6409 - acc: 0.576 - ETA: 1s - loss: 6.6189 - acc: 0.576 - ETA: 1s - loss: 6.6124 - acc: 0.577 - ETA: 1s - loss: 6.5917 - acc: 0.578 - ETA: 1s - loss: 6.5886 - acc: 0.578 - ETA: 0s - loss: 6.5982 - acc: 0.577 - ETA: 0s - loss: 6.6025 - acc: 0.577 - ETA: 0s - loss: 6.5988 - acc: 0.577 - ETA: 0s - loss: 6.6166 - acc: 0.575 - ETA: 0s - loss: 6.6315 - acc: 0.574 - ETA: 0s - loss: 6.6372 - acc: 0.574 - ETA: 0s - loss: 6.6574 - acc: 0.572 - ETA: 0s - loss: 6.6724 - acc: 0.572 - ETA: 0s - loss: 6.6578 - acc: 0.573 - ETA: 0s - loss: 6.6799 - acc: 0.571 - ETA: 0s - loss: 6.6630 - acc: 0.572 - ETA: 0s - loss: 6.6735 - acc: 0.571 - ETA: 0s - loss: 6.6727 - acc: 0.571 - ETA: 0s - loss: 6.6963 - acc: 0.570 - ETA: 0s - loss: 6.6911 - acc: 0.570 - ETA: 0s - loss: 6.6826 - acc: 0.571 - 4s 645us/step - loss: 6.6772 - acc: 0.5713 - val_loss: 7.5558 - val_acc: 0.4491\n",
      "\n",
      "Epoch 00017: val_loss improved from 7.57946 to 7.55584, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 3s - loss: 8.0599 - acc: 0.500 - ETA: 3s - loss: 6.8588 - acc: 0.566 - ETA: 3s - loss: 6.9391 - acc: 0.554 - ETA: 3s - loss: 6.8253 - acc: 0.563 - ETA: 3s - loss: 6.9582 - acc: 0.557 - ETA: 3s - loss: 6.9672 - acc: 0.554 - ETA: 3s - loss: 6.7940 - acc: 0.562 - ETA: 3s - loss: 6.6314 - acc: 0.572 - ETA: 3s - loss: 6.3066 - acc: 0.593 - ETA: 3s - loss: 6.4016 - acc: 0.588 - ETA: 3s - loss: 6.5111 - acc: 0.582 - ETA: 3s - loss: 6.4502 - acc: 0.585 - ETA: 3s - loss: 6.4215 - acc: 0.588 - ETA: 3s - loss: 6.4475 - acc: 0.586 - ETA: 3s - loss: 6.4353 - acc: 0.588 - ETA: 3s - loss: 6.5182 - acc: 0.584 - ETA: 3s - loss: 6.6030 - acc: 0.579 - ETA: 3s - loss: 6.5548 - acc: 0.582 - ETA: 3s - loss: 6.6192 - acc: 0.579 - ETA: 3s - loss: 6.6188 - acc: 0.579 - ETA: 3s - loss: 6.6210 - acc: 0.579 - ETA: 2s - loss: 6.6246 - acc: 0.579 - ETA: 2s - loss: 6.6175 - acc: 0.579 - ETA: 2s - loss: 6.6044 - acc: 0.580 - ETA: 2s - loss: 6.6157 - acc: 0.579 - ETA: 2s - loss: 6.5384 - acc: 0.583 - ETA: 2s - loss: 6.5997 - acc: 0.579 - ETA: 2s - loss: 6.6017 - acc: 0.579 - ETA: 2s - loss: 6.6445 - acc: 0.577 - ETA: 2s - loss: 6.6706 - acc: 0.576 - ETA: 2s - loss: 6.6360 - acc: 0.577 - ETA: 2s - loss: 6.6562 - acc: 0.576 - ETA: 2s - loss: 6.6158 - acc: 0.578 - ETA: 2s - loss: 6.5775 - acc: 0.581 - ETA: 2s - loss: 6.5723 - acc: 0.581 - ETA: 2s - loss: 6.5597 - acc: 0.581 - ETA: 2s - loss: 6.5885 - acc: 0.579 - ETA: 2s - loss: 6.5682 - acc: 0.579 - ETA: 2s - loss: 6.5797 - acc: 0.578 - ETA: 2s - loss: 6.5823 - acc: 0.578 - ETA: 2s - loss: 6.5689 - acc: 0.578 - ETA: 1s - loss: 6.5656 - acc: 0.579 - ETA: 1s - loss: 6.5543 - acc: 0.580 - ETA: 1s - loss: 6.5318 - acc: 0.581 - ETA: 1s - loss: 6.5530 - acc: 0.580 - ETA: 1s - loss: 6.5664 - acc: 0.579 - ETA: 1s - loss: 6.5388 - acc: 0.581 - ETA: 1s - loss: 6.5340 - acc: 0.581 - ETA: 1s - loss: 6.5598 - acc: 0.580 - ETA: 1s - loss: 6.5729 - acc: 0.579 - ETA: 1s - loss: 6.5480 - acc: 0.581 - ETA: 1s - loss: 6.5245 - acc: 0.582 - ETA: 1s - loss: 6.5452 - acc: 0.581 - ETA: 1s - loss: 6.5399 - acc: 0.581 - ETA: 1s - loss: 6.5724 - acc: 0.580 - ETA: 1s - loss: 6.5698 - acc: 0.579 - ETA: 1s - loss: 6.5831 - acc: 0.579 - ETA: 1s - loss: 6.5746 - acc: 0.579 - ETA: 1s - loss: 6.5647 - acc: 0.580 - ETA: 1s - loss: 6.5662 - acc: 0.580 - ETA: 0s - loss: 6.5678 - acc: 0.579 - ETA: 0s - loss: 6.5805 - acc: 0.579 - ETA: 0s - loss: 6.5829 - acc: 0.579 - ETA: 0s - loss: 6.5834 - acc: 0.579 - ETA: 0s - loss: 6.5836 - acc: 0.579 - ETA: 0s - loss: 6.5979 - acc: 0.578 - ETA: 0s - loss: 6.5903 - acc: 0.578 - ETA: 0s - loss: 6.5856 - acc: 0.578 - ETA: 0s - loss: 6.5816 - acc: 0.579 - ETA: 0s - loss: 6.5767 - acc: 0.579 - ETA: 0s - loss: 6.5665 - acc: 0.580 - ETA: 0s - loss: 6.5842 - acc: 0.579 - ETA: 0s - loss: 6.5944 - acc: 0.578 - ETA: 0s - loss: 6.5873 - acc: 0.579 - ETA: 0s - loss: 6.5759 - acc: 0.579 - ETA: 0s - loss: 6.5790 - acc: 0.579 - ETA: 0s - loss: 6.5923 - acc: 0.578 - 4s 656us/step - loss: 6.5944 - acc: 0.5787 - val_loss: 7.5805 - val_acc: 0.4479\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 6.4496 - acc: 0.600 - ETA: 3s - loss: 6.2114 - acc: 0.600 - ETA: 3s - loss: 6.3423 - acc: 0.595 - ETA: 3s - loss: 6.0841 - acc: 0.612 - ETA: 3s - loss: 6.3180 - acc: 0.600 - ETA: 3s - loss: 6.4742 - acc: 0.591 - ETA: 3s - loss: 6.7566 - acc: 0.574 - ETA: 3s - loss: 6.6876 - acc: 0.579 - ETA: 3s - loss: 6.7525 - acc: 0.575 - ETA: 3s - loss: 6.7623 - acc: 0.575 - ETA: 3s - loss: 6.7911 - acc: 0.572 - ETA: 3s - loss: 6.7317 - acc: 0.577 - ETA: 3s - loss: 6.8170 - acc: 0.571 - ETA: 3s - loss: 6.8701 - acc: 0.567 - ETA: 3s - loss: 6.8624 - acc: 0.568 - ETA: 3s - loss: 6.8152 - acc: 0.571 - ETA: 3s - loss: 6.7731 - acc: 0.574 - ETA: 3s - loss: 6.7586 - acc: 0.573 - ETA: 3s - loss: 6.8032 - acc: 0.571 - ETA: 3s - loss: 6.7677 - acc: 0.573 - ETA: 3s - loss: 6.7783 - acc: 0.572 - ETA: 2s - loss: 6.8052 - acc: 0.570 - ETA: 2s - loss: 6.8164 - acc: 0.569 - ETA: 2s - loss: 6.8124 - acc: 0.569 - ETA: 2s - loss: 6.8241 - acc: 0.568 - ETA: 2s - loss: 6.8070 - acc: 0.569 - ETA: 2s - loss: 6.8019 - acc: 0.570 - ETA: 2s - loss: 6.8306 - acc: 0.568 - ETA: 2s - loss: 6.8323 - acc: 0.568 - ETA: 2s - loss: 6.8521 - acc: 0.567 - ETA: 2s - loss: 6.8468 - acc: 0.567 - ETA: 2s - loss: 6.8332 - acc: 0.568 - ETA: 2s - loss: 6.8224 - acc: 0.569 - ETA: 2s - loss: 6.8151 - acc: 0.570 - ETA: 2s - loss: 6.7710 - acc: 0.573 - ETA: 2s - loss: 6.7970 - acc: 0.571 - ETA: 2s - loss: 6.8086 - acc: 0.570 - ETA: 2s - loss: 6.8330 - acc: 0.569 - ETA: 2s - loss: 6.8074 - acc: 0.570 - ETA: 2s - loss: 6.8207 - acc: 0.569 - ETA: 1s - loss: 6.8078 - acc: 0.570 - ETA: 1s - loss: 6.7917 - acc: 0.571 - ETA: 1s - loss: 6.7978 - acc: 0.571 - ETA: 1s - loss: 6.7819 - acc: 0.572 - ETA: 1s - loss: 6.7860 - acc: 0.572 - ETA: 1s - loss: 6.7590 - acc: 0.573 - ETA: 1s - loss: 6.7488 - acc: 0.574 - ETA: 1s - loss: 6.7532 - acc: 0.573 - ETA: 1s - loss: 6.7582 - acc: 0.573 - ETA: 1s - loss: 6.7495 - acc: 0.573 - ETA: 1s - loss: 6.7153 - acc: 0.575 - ETA: 1s - loss: 6.7069 - acc: 0.576 - ETA: 1s - loss: 6.6919 - acc: 0.577 - ETA: 1s - loss: 6.6633 - acc: 0.578 - ETA: 1s - loss: 6.6360 - acc: 0.580 - ETA: 1s - loss: 6.6265 - acc: 0.581 - ETA: 1s - loss: 6.6010 - acc: 0.582 - ETA: 1s - loss: 6.6042 - acc: 0.582 - ETA: 1s - loss: 6.6054 - acc: 0.582 - ETA: 0s - loss: 6.6214 - acc: 0.581 - ETA: 0s - loss: 6.6163 - acc: 0.581 - ETA: 0s - loss: 6.6319 - acc: 0.580 - ETA: 0s - loss: 6.6077 - acc: 0.581 - ETA: 0s - loss: 6.6115 - acc: 0.581 - ETA: 0s - loss: 6.5718 - acc: 0.583 - ETA: 0s - loss: 6.5594 - acc: 0.584 - ETA: 0s - loss: 6.5454 - acc: 0.584 - ETA: 0s - loss: 6.5614 - acc: 0.583 - ETA: 0s - loss: 6.5316 - acc: 0.585 - ETA: 0s - loss: 6.5269 - acc: 0.585 - ETA: 0s - loss: 6.5047 - acc: 0.587 - ETA: 0s - loss: 6.5007 - acc: 0.587 - ETA: 0s - loss: 6.5012 - acc: 0.586 - ETA: 0s - loss: 6.5189 - acc: 0.585 - ETA: 0s - loss: 6.5402 - acc: 0.583 - ETA: 0s - loss: 6.5367 - acc: 0.583 - ETA: 0s - loss: 6.5304 - acc: 0.584 - 4s 661us/step - loss: 6.5490 - acc: 0.5829 - val_loss: 7.4177 - val_acc: 0.4515\n",
      "\n",
      "Epoch 00019: val_loss improved from 7.55584 to 7.41765, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 8.8899 - acc: 0.450 - ETA: 4s - loss: 7.0069 - acc: 0.558 - ETA: 3s - loss: 6.1670 - acc: 0.613 - ETA: 3s - loss: 6.4609 - acc: 0.590 - ETA: 3s - loss: 6.4236 - acc: 0.592 - ETA: 3s - loss: 6.4530 - acc: 0.588 - ETA: 3s - loss: 6.2653 - acc: 0.601 - ETA: 3s - loss: 6.6866 - acc: 0.575 - ETA: 3s - loss: 6.5772 - acc: 0.583 - ETA: 3s - loss: 6.6361 - acc: 0.578 - ETA: 3s - loss: 6.4243 - acc: 0.592 - ETA: 3s - loss: 6.4152 - acc: 0.593 - ETA: 3s - loss: 6.4562 - acc: 0.589 - ETA: 3s - loss: 6.4208 - acc: 0.591 - ETA: 3s - loss: 6.4713 - acc: 0.589 - ETA: 3s - loss: 6.4384 - acc: 0.591 - ETA: 3s - loss: 6.3869 - acc: 0.595 - ETA: 3s - loss: 6.3799 - acc: 0.596 - ETA: 2s - loss: 6.3859 - acc: 0.595 - ETA: 2s - loss: 6.3798 - acc: 0.596 - ETA: 2s - loss: 6.4000 - acc: 0.595 - ETA: 2s - loss: 6.4602 - acc: 0.592 - ETA: 2s - loss: 6.4837 - acc: 0.591 - ETA: 2s - loss: 6.4450 - acc: 0.592 - ETA: 2s - loss: 6.4311 - acc: 0.593 - ETA: 2s - loss: 6.4460 - acc: 0.593 - ETA: 2s - loss: 6.4836 - acc: 0.590 - ETA: 2s - loss: 6.4907 - acc: 0.589 - ETA: 2s - loss: 6.4992 - acc: 0.588 - ETA: 2s - loss: 6.4917 - acc: 0.589 - ETA: 2s - loss: 6.5206 - acc: 0.587 - ETA: 2s - loss: 6.5352 - acc: 0.586 - ETA: 2s - loss: 6.5328 - acc: 0.586 - ETA: 2s - loss: 6.5571 - acc: 0.585 - ETA: 2s - loss: 6.5334 - acc: 0.586 - ETA: 2s - loss: 6.5061 - acc: 0.588 - ETA: 2s - loss: 6.5350 - acc: 0.585 - ETA: 2s - loss: 6.5485 - acc: 0.585 - ETA: 1s - loss: 6.5368 - acc: 0.586 - ETA: 1s - loss: 6.5581 - acc: 0.584 - ETA: 1s - loss: 6.5627 - acc: 0.584 - ETA: 1s - loss: 6.5775 - acc: 0.583 - ETA: 1s - loss: 6.5817 - acc: 0.583 - ETA: 1s - loss: 6.5749 - acc: 0.583 - ETA: 1s - loss: 6.5931 - acc: 0.581 - ETA: 1s - loss: 6.5903 - acc: 0.581 - ETA: 1s - loss: 6.5607 - acc: 0.583 - ETA: 1s - loss: 6.5747 - acc: 0.582 - ETA: 1s - loss: 6.5728 - acc: 0.582 - ETA: 1s - loss: 6.5561 - acc: 0.583 - ETA: 1s - loss: 6.5542 - acc: 0.583 - ETA: 1s - loss: 6.5253 - acc: 0.585 - ETA: 1s - loss: 6.5240 - acc: 0.585 - ETA: 1s - loss: 6.5258 - acc: 0.585 - ETA: 1s - loss: 6.4885 - acc: 0.587 - ETA: 1s - loss: 6.5026 - acc: 0.586 - ETA: 1s - loss: 6.5082 - acc: 0.585 - ETA: 0s - loss: 6.4997 - acc: 0.586 - ETA: 0s - loss: 6.4937 - acc: 0.586 - ETA: 0s - loss: 6.4658 - acc: 0.588 - ETA: 0s - loss: 6.4826 - acc: 0.587 - ETA: 0s - loss: 6.4792 - acc: 0.587 - ETA: 0s - loss: 6.4714 - acc: 0.587 - ETA: 0s - loss: 6.4951 - acc: 0.586 - ETA: 0s - loss: 6.4555 - acc: 0.588 - ETA: 0s - loss: 6.4490 - acc: 0.589 - ETA: 0s - loss: 6.4506 - acc: 0.588 - ETA: 0s - loss: 6.4224 - acc: 0.590 - ETA: 0s - loss: 6.4211 - acc: 0.590 - ETA: 0s - loss: 6.4164 - acc: 0.590 - ETA: 0s - loss: 6.4194 - acc: 0.590 - ETA: 0s - loss: 6.4090 - acc: 0.591 - ETA: 0s - loss: 6.4286 - acc: 0.589 - ETA: 0s - loss: 6.4301 - acc: 0.589 - ETA: 0s - loss: 6.4387 - acc: 0.589 - ETA: 0s - loss: 6.4373 - acc: 0.588 - 4s 661us/step - loss: 6.4179 - acc: 0.5897 - val_loss: 7.4060 - val_acc: 0.4611\n",
      "\n",
      "Epoch 00020: val_loss improved from 7.41765 to 7.40601, saving model to saved_models/weights.best.VGG16.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x259238ed7f0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 46.2919%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogResnet50Data.npz')\n",
    "train_Resnet50 = bottleneck_features['train']\n",
    "valid_Resnet50 = bottleneck_features['valid']\n",
    "test_Resnet50 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "   __Answer:__ Step 1) I used global average pooling layer to reduce the dimentionality. It takes feature map as input and calculates the average value of the node for each map in the stack.\n",
    "   Step 2) Then I added output layer with softmax function which provides the value between 0 and 1.\n",
    "\n",
    "Instead of constructing a CNN from scratch, I am passing my input to a pre-trained neural network that is Resnet50 which is microsoft research network. It is a kind of VGG, the largest of which has a groundbreaking 152 layers. Resnet added skipped layer so the gradient signal has to travel a shorter route, which makes this pre trained network suitable for the current problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_2 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "Resnet50_model = Sequential()\n",
    "Resnet50_model.add(GlobalAveragePooling2D(input_shape=train_Resnet50.shape[1:]))\n",
    "Resnet50_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "Resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "Resnet50_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 47:42 - loss: 5.3800 - acc: 0.0000e+ - ETA: 9:29 - loss: 6.0495 - acc: 0.0000e+00 - ETA: 5:14 - loss: 5.9182 - acc: 0.0222    - ETA: 3:36 - loss: 5.6079 - acc: 0.061 - ETA: 2:44 - loss: 5.3933 - acc: 0.055 - ETA: 2:19 - loss: 5.2315 - acc: 0.067 - ETA: 1:50 - loss: 5.0037 - acc: 0.090 - ETA: 1:34 - loss: 4.8415 - acc: 0.105 - ETA: 1:22 - loss: 4.6606 - acc: 0.133 - ETA: 1:13 - loss: 4.5042 - acc: 0.148 - ETA: 1:05 - loss: 4.3389 - acc: 0.164 - ETA: 59s - loss: 4.2196 - acc: 0.175 - ETA: 54s - loss: 4.0680 - acc: 0.19 - ETA: 49s - loss: 3.9643 - acc: 0.21 - ETA: 45s - loss: 3.8311 - acc: 0.23 - ETA: 42s - loss: 3.7361 - acc: 0.24 - ETA: 39s - loss: 3.6353 - acc: 0.26 - ETA: 37s - loss: 3.5559 - acc: 0.27 - ETA: 35s - loss: 3.4879 - acc: 0.28 - ETA: 32s - loss: 3.4074 - acc: 0.29 - ETA: 31s - loss: 3.3490 - acc: 0.30 - ETA: 29s - loss: 3.2753 - acc: 0.31 - ETA: 27s - loss: 3.2168 - acc: 0.32 - ETA: 26s - loss: 3.1547 - acc: 0.33 - ETA: 25s - loss: 3.0895 - acc: 0.34 - ETA: 23s - loss: 3.0368 - acc: 0.35 - ETA: 22s - loss: 2.9828 - acc: 0.36 - ETA: 21s - loss: 2.9361 - acc: 0.37 - ETA: 20s - loss: 2.8913 - acc: 0.38 - ETA: 19s - loss: 2.8412 - acc: 0.38 - ETA: 19s - loss: 2.7902 - acc: 0.39 - ETA: 18s - loss: 2.7376 - acc: 0.40 - ETA: 17s - loss: 2.6987 - acc: 0.41 - ETA: 16s - loss: 2.6594 - acc: 0.41 - ETA: 15s - loss: 2.6179 - acc: 0.42 - ETA: 15s - loss: 2.5722 - acc: 0.43 - ETA: 14s - loss: 2.5302 - acc: 0.44 - ETA: 14s - loss: 2.5041 - acc: 0.44 - ETA: 13s - loss: 2.4656 - acc: 0.45 - ETA: 12s - loss: 2.4351 - acc: 0.46 - ETA: 12s - loss: 2.4099 - acc: 0.46 - ETA: 11s - loss: 2.3866 - acc: 0.46 - ETA: 11s - loss: 2.3469 - acc: 0.47 - ETA: 10s - loss: 2.3237 - acc: 0.47 - ETA: 10s - loss: 2.3041 - acc: 0.47 - ETA: 10s - loss: 2.2749 - acc: 0.48 - ETA: 9s - loss: 2.2470 - acc: 0.4872 - ETA: 9s - loss: 2.2192 - acc: 0.492 - ETA: 8s - loss: 2.1971 - acc: 0.496 - ETA: 8s - loss: 2.1788 - acc: 0.499 - ETA: 8s - loss: 2.1557 - acc: 0.503 - ETA: 7s - loss: 2.1308 - acc: 0.508 - ETA: 7s - loss: 2.1130 - acc: 0.508 - ETA: 7s - loss: 2.0961 - acc: 0.510 - ETA: 6s - loss: 2.0782 - acc: 0.514 - ETA: 6s - loss: 2.0591 - acc: 0.518 - ETA: 6s - loss: 2.0384 - acc: 0.523 - ETA: 5s - loss: 2.0187 - acc: 0.526 - ETA: 5s - loss: 2.0014 - acc: 0.529 - ETA: 5s - loss: 1.9791 - acc: 0.535 - ETA: 5s - loss: 1.9642 - acc: 0.537 - ETA: 4s - loss: 1.9479 - acc: 0.540 - ETA: 4s - loss: 1.9313 - acc: 0.544 - ETA: 4s - loss: 1.9057 - acc: 0.550 - ETA: 4s - loss: 1.8915 - acc: 0.552 - ETA: 3s - loss: 1.8742 - acc: 0.556 - ETA: 3s - loss: 1.8581 - acc: 0.559 - ETA: 3s - loss: 1.8432 - acc: 0.562 - ETA: 3s - loss: 1.8336 - acc: 0.564 - ETA: 2s - loss: 1.8185 - acc: 0.567 - ETA: 2s - loss: 1.8036 - acc: 0.569 - ETA: 2s - loss: 1.7887 - acc: 0.571 - ETA: 2s - loss: 1.7822 - acc: 0.572 - ETA: 2s - loss: 1.7726 - acc: 0.573 - ETA: 1s - loss: 1.7584 - acc: 0.577 - ETA: 1s - loss: 1.7483 - acc: 0.579 - ETA: 1s - loss: 1.7377 - acc: 0.580 - ETA: 1s - loss: 1.7245 - acc: 0.582 - ETA: 1s - loss: 1.7108 - acc: 0.585 - ETA: 1s - loss: 1.6978 - acc: 0.588 - ETA: 0s - loss: 1.6853 - acc: 0.589 - ETA: 0s - loss: 1.6775 - acc: 0.590 - ETA: 0s - loss: 1.6652 - acc: 0.593 - ETA: 0s - loss: 1.6528 - acc: 0.596 - ETA: 0s - loss: 1.6477 - acc: 0.596 - ETA: 0s - loss: 1.6349 - acc: 0.599 - 13s 2ms/step - loss: 1.6264 - acc: 0.6006 - val_loss: 0.8486 - val_acc: 0.7521\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.84862, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.2935 - acc: 0.950 - ETA: 4s - loss: 0.3486 - acc: 0.880 - ETA: 4s - loss: 0.3758 - acc: 0.894 - ETA: 4s - loss: 0.4311 - acc: 0.892 - ETA: 4s - loss: 0.4488 - acc: 0.883 - ETA: 4s - loss: 0.4520 - acc: 0.878 - ETA: 4s - loss: 0.4534 - acc: 0.880 - ETA: 4s - loss: 0.4638 - acc: 0.870 - ETA: 4s - loss: 0.4665 - acc: 0.866 - ETA: 4s - loss: 0.4563 - acc: 0.867 - ETA: 4s - loss: 0.4605 - acc: 0.864 - ETA: 4s - loss: 0.4570 - acc: 0.865 - ETA: 4s - loss: 0.4485 - acc: 0.864 - ETA: 3s - loss: 0.4498 - acc: 0.864 - ETA: 3s - loss: 0.4471 - acc: 0.866 - ETA: 3s - loss: 0.4419 - acc: 0.865 - ETA: 3s - loss: 0.4456 - acc: 0.865 - ETA: 3s - loss: 0.4523 - acc: 0.863 - ETA: 3s - loss: 0.4441 - acc: 0.865 - ETA: 3s - loss: 0.4402 - acc: 0.867 - ETA: 3s - loss: 0.4447 - acc: 0.867 - ETA: 3s - loss: 0.4427 - acc: 0.867 - ETA: 3s - loss: 0.4415 - acc: 0.866 - ETA: 3s - loss: 0.4429 - acc: 0.865 - ETA: 3s - loss: 0.4426 - acc: 0.866 - ETA: 3s - loss: 0.4420 - acc: 0.867 - ETA: 3s - loss: 0.4358 - acc: 0.870 - ETA: 3s - loss: 0.4361 - acc: 0.870 - ETA: 3s - loss: 0.4303 - acc: 0.873 - ETA: 3s - loss: 0.4337 - acc: 0.871 - ETA: 2s - loss: 0.4302 - acc: 0.873 - ETA: 2s - loss: 0.4325 - acc: 0.872 - ETA: 2s - loss: 0.4331 - acc: 0.871 - ETA: 2s - loss: 0.4366 - acc: 0.869 - ETA: 2s - loss: 0.4364 - acc: 0.868 - ETA: 2s - loss: 0.4320 - acc: 0.870 - ETA: 2s - loss: 0.4361 - acc: 0.868 - ETA: 2s - loss: 0.4440 - acc: 0.865 - ETA: 2s - loss: 0.4422 - acc: 0.866 - ETA: 2s - loss: 0.4451 - acc: 0.864 - ETA: 2s - loss: 0.4458 - acc: 0.864 - ETA: 2s - loss: 0.4414 - acc: 0.866 - ETA: 2s - loss: 0.4389 - acc: 0.866 - ETA: 2s - loss: 0.4383 - acc: 0.867 - ETA: 2s - loss: 0.4355 - acc: 0.867 - ETA: 2s - loss: 0.4340 - acc: 0.868 - ETA: 2s - loss: 0.4338 - acc: 0.867 - ETA: 2s - loss: 0.4370 - acc: 0.865 - ETA: 2s - loss: 0.4390 - acc: 0.865 - ETA: 1s - loss: 0.4365 - acc: 0.865 - ETA: 1s - loss: 0.4412 - acc: 0.864 - ETA: 1s - loss: 0.4397 - acc: 0.864 - ETA: 1s - loss: 0.4434 - acc: 0.863 - ETA: 1s - loss: 0.4443 - acc: 0.862 - ETA: 1s - loss: 0.4422 - acc: 0.863 - ETA: 1s - loss: 0.4414 - acc: 0.863 - ETA: 1s - loss: 0.4468 - acc: 0.861 - ETA: 1s - loss: 0.4453 - acc: 0.862 - ETA: 1s - loss: 0.4445 - acc: 0.862 - ETA: 1s - loss: 0.4440 - acc: 0.862 - ETA: 1s - loss: 0.4455 - acc: 0.862 - ETA: 1s - loss: 0.4442 - acc: 0.862 - ETA: 1s - loss: 0.4427 - acc: 0.862 - ETA: 1s - loss: 0.4433 - acc: 0.862 - ETA: 1s - loss: 0.4449 - acc: 0.861 - ETA: 1s - loss: 0.4431 - acc: 0.862 - ETA: 1s - loss: 0.4417 - acc: 0.862 - ETA: 0s - loss: 0.4456 - acc: 0.861 - ETA: 0s - loss: 0.4462 - acc: 0.860 - ETA: 0s - loss: 0.4453 - acc: 0.861 - ETA: 0s - loss: 0.4444 - acc: 0.861 - ETA: 0s - loss: 0.4440 - acc: 0.860 - ETA: 0s - loss: 0.4439 - acc: 0.860 - ETA: 0s - loss: 0.4456 - acc: 0.860 - ETA: 0s - loss: 0.4489 - acc: 0.859 - ETA: 0s - loss: 0.4508 - acc: 0.859 - ETA: 0s - loss: 0.4503 - acc: 0.858 - ETA: 0s - loss: 0.4507 - acc: 0.858 - ETA: 0s - loss: 0.4507 - acc: 0.857 - ETA: 0s - loss: 0.4496 - acc: 0.857 - ETA: 0s - loss: 0.4474 - acc: 0.858 - ETA: 0s - loss: 0.4472 - acc: 0.857 - ETA: 0s - loss: 0.4460 - acc: 0.858 - ETA: 0s - loss: 0.4461 - acc: 0.858 - ETA: 0s - loss: 0.4454 - acc: 0.857 - ETA: 0s - loss: 0.4458 - acc: 0.858 - 5s 719us/step - loss: 0.4445 - acc: 0.8585 - val_loss: 0.7121 - val_acc: 0.7832\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.84862 to 0.71208, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "Epoch 3/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.6400 - acc: 0.800 - ETA: 4s - loss: 0.3193 - acc: 0.875 - ETA: 4s - loss: 0.3277 - acc: 0.890 - ETA: 4s - loss: 0.3276 - acc: 0.892 - ETA: 4s - loss: 0.3106 - acc: 0.900 - ETA: 4s - loss: 0.2886 - acc: 0.906 - ETA: 4s - loss: 0.2728 - acc: 0.911 - ETA: 4s - loss: 0.2702 - acc: 0.911 - ETA: 4s - loss: 0.2685 - acc: 0.911 - ETA: 3s - loss: 0.2702 - acc: 0.911 - ETA: 3s - loss: 0.2577 - acc: 0.917 - ETA: 3s - loss: 0.2558 - acc: 0.916 - ETA: 3s - loss: 0.2517 - acc: 0.918 - ETA: 3s - loss: 0.2461 - acc: 0.922 - ETA: 3s - loss: 0.2614 - acc: 0.917 - ETA: 3s - loss: 0.2558 - acc: 0.919 - ETA: 3s - loss: 0.2584 - acc: 0.917 - ETA: 3s - loss: 0.2631 - acc: 0.917 - ETA: 3s - loss: 0.2646 - acc: 0.915 - ETA: 3s - loss: 0.2627 - acc: 0.915 - ETA: 3s - loss: 0.2588 - acc: 0.916 - ETA: 3s - loss: 0.2602 - acc: 0.917 - ETA: 3s - loss: 0.2569 - acc: 0.918 - ETA: 3s - loss: 0.2535 - acc: 0.917 - ETA: 3s - loss: 0.2486 - acc: 0.919 - ETA: 3s - loss: 0.2523 - acc: 0.919 - ETA: 3s - loss: 0.2536 - acc: 0.918 - ETA: 3s - loss: 0.2525 - acc: 0.919 - ETA: 2s - loss: 0.2585 - acc: 0.917 - ETA: 2s - loss: 0.2591 - acc: 0.917 - ETA: 2s - loss: 0.2563 - acc: 0.918 - ETA: 2s - loss: 0.2570 - acc: 0.918 - ETA: 2s - loss: 0.2580 - acc: 0.917 - ETA: 2s - loss: 0.2597 - acc: 0.917 - ETA: 2s - loss: 0.2561 - acc: 0.919 - ETA: 2s - loss: 0.2549 - acc: 0.919 - ETA: 2s - loss: 0.2528 - acc: 0.920 - ETA: 2s - loss: 0.2550 - acc: 0.920 - ETA: 2s - loss: 0.2572 - acc: 0.919 - ETA: 2s - loss: 0.2564 - acc: 0.920 - ETA: 2s - loss: 0.2566 - acc: 0.919 - ETA: 2s - loss: 0.2608 - acc: 0.918 - ETA: 2s - loss: 0.2597 - acc: 0.919 - ETA: 2s - loss: 0.2596 - acc: 0.918 - ETA: 2s - loss: 0.2567 - acc: 0.919 - ETA: 2s - loss: 0.2591 - acc: 0.918 - ETA: 2s - loss: 0.2637 - acc: 0.916 - ETA: 2s - loss: 0.2620 - acc: 0.917 - ETA: 2s - loss: 0.2609 - acc: 0.917 - ETA: 1s - loss: 0.2626 - acc: 0.916 - ETA: 1s - loss: 0.2603 - acc: 0.917 - ETA: 1s - loss: 0.2632 - acc: 0.916 - ETA: 1s - loss: 0.2647 - acc: 0.916 - ETA: 1s - loss: 0.2648 - acc: 0.916 - ETA: 1s - loss: 0.2678 - acc: 0.916 - ETA: 1s - loss: 0.2687 - acc: 0.916 - ETA: 1s - loss: 0.2703 - acc: 0.915 - ETA: 1s - loss: 0.2711 - acc: 0.915 - ETA: 1s - loss: 0.2697 - acc: 0.915 - ETA: 1s - loss: 0.2699 - acc: 0.915 - ETA: 1s - loss: 0.2697 - acc: 0.915 - ETA: 1s - loss: 0.2679 - acc: 0.915 - ETA: 1s - loss: 0.2664 - acc: 0.916 - ETA: 1s - loss: 0.2661 - acc: 0.916 - ETA: 1s - loss: 0.2670 - acc: 0.916 - ETA: 1s - loss: 0.2683 - acc: 0.916 - ETA: 1s - loss: 0.2697 - acc: 0.915 - ETA: 0s - loss: 0.2722 - acc: 0.913 - ETA: 0s - loss: 0.2728 - acc: 0.913 - ETA: 0s - loss: 0.2726 - acc: 0.913 - ETA: 0s - loss: 0.2733 - acc: 0.913 - ETA: 0s - loss: 0.2724 - acc: 0.913 - ETA: 0s - loss: 0.2716 - acc: 0.914 - ETA: 0s - loss: 0.2706 - acc: 0.914 - ETA: 0s - loss: 0.2699 - acc: 0.914 - ETA: 0s - loss: 0.2698 - acc: 0.914 - ETA: 0s - loss: 0.2688 - acc: 0.915 - ETA: 0s - loss: 0.2688 - acc: 0.915 - ETA: 0s - loss: 0.2670 - acc: 0.915 - ETA: 0s - loss: 0.2674 - acc: 0.915 - ETA: 0s - loss: 0.2677 - acc: 0.915 - ETA: 0s - loss: 0.2673 - acc: 0.915 - ETA: 0s - loss: 0.2691 - acc: 0.914 - ETA: 0s - loss: 0.2690 - acc: 0.914 - 5s 715us/step - loss: 0.2684 - acc: 0.9150 - val_loss: 0.6726 - val_acc: 0.8036\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.71208 to 0.67264, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 0.1142 - acc: 0.950 - ETA: 3s - loss: 0.1599 - acc: 0.958 - ETA: 4s - loss: 0.1326 - acc: 0.970 - ETA: 4s - loss: 0.1299 - acc: 0.971 - ETA: 4s - loss: 0.1424 - acc: 0.963 - ETA: 4s - loss: 0.1431 - acc: 0.965 - ETA: 4s - loss: 0.1356 - acc: 0.963 - ETA: 4s - loss: 0.1410 - acc: 0.960 - ETA: 4s - loss: 0.1495 - acc: 0.957 - ETA: 4s - loss: 0.1454 - acc: 0.959 - ETA: 3s - loss: 0.1463 - acc: 0.958 - ETA: 3s - loss: 0.1524 - acc: 0.955 - ETA: 3s - loss: 0.1580 - acc: 0.951 - ETA: 3s - loss: 0.1563 - acc: 0.950 - ETA: 3s - loss: 0.1503 - acc: 0.953 - ETA: 3s - loss: 0.1488 - acc: 0.953 - ETA: 3s - loss: 0.1486 - acc: 0.953 - ETA: 3s - loss: 0.1492 - acc: 0.952 - ETA: 3s - loss: 0.1509 - acc: 0.952 - ETA: 3s - loss: 0.1497 - acc: 0.953 - ETA: 3s - loss: 0.1518 - acc: 0.951 - ETA: 3s - loss: 0.1536 - acc: 0.952 - ETA: 3s - loss: 0.1509 - acc: 0.953 - ETA: 3s - loss: 0.1563 - acc: 0.951 - ETA: 3s - loss: 0.1571 - acc: 0.951 - ETA: 3s - loss: 0.1616 - acc: 0.951 - ETA: 3s - loss: 0.1592 - acc: 0.951 - ETA: 3s - loss: 0.1611 - acc: 0.950 - ETA: 3s - loss: 0.1604 - acc: 0.950 - ETA: 3s - loss: 0.1595 - acc: 0.951 - ETA: 2s - loss: 0.1626 - acc: 0.950 - ETA: 2s - loss: 0.1668 - acc: 0.948 - ETA: 2s - loss: 0.1680 - acc: 0.948 - ETA: 2s - loss: 0.1659 - acc: 0.950 - ETA: 2s - loss: 0.1646 - acc: 0.950 - ETA: 2s - loss: 0.1674 - acc: 0.949 - ETA: 2s - loss: 0.1721 - acc: 0.947 - ETA: 2s - loss: 0.1714 - acc: 0.947 - ETA: 2s - loss: 0.1714 - acc: 0.947 - ETA: 2s - loss: 0.1704 - acc: 0.947 - ETA: 2s - loss: 0.1710 - acc: 0.946 - ETA: 2s - loss: 0.1694 - acc: 0.947 - ETA: 2s - loss: 0.1695 - acc: 0.947 - ETA: 2s - loss: 0.1736 - acc: 0.946 - ETA: 2s - loss: 0.1724 - acc: 0.946 - ETA: 2s - loss: 0.1733 - acc: 0.946 - ETA: 2s - loss: 0.1736 - acc: 0.945 - ETA: 2s - loss: 0.1752 - acc: 0.945 - ETA: 1s - loss: 0.1750 - acc: 0.945 - ETA: 1s - loss: 0.1753 - acc: 0.945 - ETA: 1s - loss: 0.1768 - acc: 0.945 - ETA: 1s - loss: 0.1768 - acc: 0.945 - ETA: 1s - loss: 0.1764 - acc: 0.945 - ETA: 1s - loss: 0.1752 - acc: 0.946 - ETA: 1s - loss: 0.1763 - acc: 0.945 - ETA: 1s - loss: 0.1776 - acc: 0.945 - ETA: 1s - loss: 0.1794 - acc: 0.945 - ETA: 1s - loss: 0.1810 - acc: 0.945 - ETA: 1s - loss: 0.1809 - acc: 0.944 - ETA: 1s - loss: 0.1800 - acc: 0.945 - ETA: 1s - loss: 0.1801 - acc: 0.944 - ETA: 1s - loss: 0.1790 - acc: 0.944 - ETA: 1s - loss: 0.1785 - acc: 0.944 - ETA: 1s - loss: 0.1774 - acc: 0.944 - ETA: 1s - loss: 0.1781 - acc: 0.944 - ETA: 1s - loss: 0.1792 - acc: 0.944 - ETA: 0s - loss: 0.1795 - acc: 0.944 - ETA: 0s - loss: 0.1789 - acc: 0.944 - ETA: 0s - loss: 0.1799 - acc: 0.944 - ETA: 0s - loss: 0.1798 - acc: 0.944 - ETA: 0s - loss: 0.1822 - acc: 0.943 - ETA: 0s - loss: 0.1823 - acc: 0.944 - ETA: 0s - loss: 0.1827 - acc: 0.944 - ETA: 0s - loss: 0.1820 - acc: 0.944 - ETA: 0s - loss: 0.1821 - acc: 0.944 - ETA: 0s - loss: 0.1820 - acc: 0.943 - ETA: 0s - loss: 0.1823 - acc: 0.943 - ETA: 0s - loss: 0.1814 - acc: 0.943 - ETA: 0s - loss: 0.1820 - acc: 0.942 - ETA: 0s - loss: 0.1831 - acc: 0.942 - ETA: 0s - loss: 0.1831 - acc: 0.942 - ETA: 0s - loss: 0.1819 - acc: 0.943 - ETA: 0s - loss: 0.1817 - acc: 0.942 - 5s 703us/step - loss: 0.1815 - acc: 0.9430 - val_loss: 0.6257 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.67264 to 0.62575, saving model to saved_models/weights.best.Resnet50.hdf5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 0.0887 - acc: 0.950 - ETA: 4s - loss: 0.1221 - acc: 0.950 - ETA: 4s - loss: 0.0900 - acc: 0.972 - ETA: 4s - loss: 0.0945 - acc: 0.965 - ETA: 4s - loss: 0.0942 - acc: 0.967 - ETA: 4s - loss: 0.0919 - acc: 0.971 - ETA: 4s - loss: 0.0990 - acc: 0.970 - ETA: 4s - loss: 0.1096 - acc: 0.960 - ETA: 4s - loss: 0.1162 - acc: 0.954 - ETA: 4s - loss: 0.1119 - acc: 0.956 - ETA: 3s - loss: 0.1127 - acc: 0.958 - ETA: 3s - loss: 0.1122 - acc: 0.958 - ETA: 3s - loss: 0.1119 - acc: 0.961 - ETA: 3s - loss: 0.1096 - acc: 0.962 - ETA: 3s - loss: 0.1074 - acc: 0.964 - ETA: 3s - loss: 0.1072 - acc: 0.965 - ETA: 3s - loss: 0.1087 - acc: 0.963 - ETA: 3s - loss: 0.1096 - acc: 0.963 - ETA: 3s - loss: 0.1093 - acc: 0.963 - ETA: 3s - loss: 0.1117 - acc: 0.963 - ETA: 3s - loss: 0.1093 - acc: 0.964 - ETA: 3s - loss: 0.1086 - acc: 0.965 - ETA: 3s - loss: 0.1090 - acc: 0.965 - ETA: 3s - loss: 0.1081 - acc: 0.966 - ETA: 3s - loss: 0.1075 - acc: 0.966 - ETA: 3s - loss: 0.1103 - acc: 0.966 - ETA: 3s - loss: 0.1090 - acc: 0.966 - ETA: 3s - loss: 0.1093 - acc: 0.966 - ETA: 2s - loss: 0.1098 - acc: 0.966 - ETA: 2s - loss: 0.1091 - acc: 0.966 - ETA: 2s - loss: 0.1127 - acc: 0.965 - ETA: 2s - loss: 0.1137 - acc: 0.964 - ETA: 2s - loss: 0.1151 - acc: 0.964 - ETA: 2s - loss: 0.1157 - acc: 0.964 - ETA: 2s - loss: 0.1138 - acc: 0.965 - ETA: 2s - loss: 0.1137 - acc: 0.965 - ETA: 2s - loss: 0.1116 - acc: 0.966 - ETA: 2s - loss: 0.1114 - acc: 0.965 - ETA: 2s - loss: 0.1125 - acc: 0.964 - ETA: 2s - loss: 0.1110 - acc: 0.965 - ETA: 2s - loss: 0.1103 - acc: 0.965 - ETA: 2s - loss: 0.1120 - acc: 0.964 - ETA: 2s - loss: 0.1108 - acc: 0.965 - ETA: 2s - loss: 0.1101 - acc: 0.965 - ETA: 2s - loss: 0.1123 - acc: 0.965 - ETA: 2s - loss: 0.1123 - acc: 0.965 - ETA: 2s - loss: 0.1141 - acc: 0.964 - ETA: 1s - loss: 0.1173 - acc: 0.963 - ETA: 1s - loss: 0.1165 - acc: 0.963 - ETA: 1s - loss: 0.1163 - acc: 0.963 - ETA: 1s - loss: 0.1166 - acc: 0.963 - ETA: 1s - loss: 0.1181 - acc: 0.963 - ETA: 1s - loss: 0.1177 - acc: 0.963 - ETA: 1s - loss: 0.1181 - acc: 0.963 - ETA: 1s - loss: 0.1175 - acc: 0.963 - ETA: 1s - loss: 0.1177 - acc: 0.963 - ETA: 1s - loss: 0.1165 - acc: 0.963 - ETA: 1s - loss: 0.1170 - acc: 0.963 - ETA: 1s - loss: 0.1161 - acc: 0.963 - ETA: 1s - loss: 0.1165 - acc: 0.963 - ETA: 1s - loss: 0.1180 - acc: 0.962 - ETA: 1s - loss: 0.1175 - acc: 0.962 - ETA: 1s - loss: 0.1183 - acc: 0.962 - ETA: 1s - loss: 0.1179 - acc: 0.962 - ETA: 1s - loss: 0.1212 - acc: 0.961 - ETA: 1s - loss: 0.1219 - acc: 0.961 - ETA: 0s - loss: 0.1243 - acc: 0.960 - ETA: 0s - loss: 0.1249 - acc: 0.960 - ETA: 0s - loss: 0.1253 - acc: 0.960 - ETA: 0s - loss: 0.1265 - acc: 0.960 - ETA: 0s - loss: 0.1270 - acc: 0.960 - ETA: 0s - loss: 0.1268 - acc: 0.960 - ETA: 0s - loss: 0.1277 - acc: 0.959 - ETA: 0s - loss: 0.1269 - acc: 0.960 - ETA: 0s - loss: 0.1264 - acc: 0.960 - ETA: 0s - loss: 0.1272 - acc: 0.960 - ETA: 0s - loss: 0.1268 - acc: 0.960 - ETA: 0s - loss: 0.1266 - acc: 0.960 - ETA: 0s - loss: 0.1264 - acc: 0.960 - ETA: 0s - loss: 0.1263 - acc: 0.960 - ETA: 0s - loss: 0.1267 - acc: 0.960 - ETA: 0s - loss: 0.1279 - acc: 0.960 - ETA: 0s - loss: 0.1279 - acc: 0.960 - ETA: 0s - loss: 0.1285 - acc: 0.959 - 5s 701us/step - loss: 0.1286 - acc: 0.9594 - val_loss: 0.6506 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.2102 - acc: 0.900 - ETA: 4s - loss: 0.0650 - acc: 0.980 - ETA: 4s - loss: 0.0487 - acc: 0.988 - ETA: 4s - loss: 0.0626 - acc: 0.980 - ETA: 4s - loss: 0.0593 - acc: 0.979 - ETA: 4s - loss: 0.0646 - acc: 0.981 - ETA: 4s - loss: 0.0635 - acc: 0.982 - ETA: 3s - loss: 0.0612 - acc: 0.982 - ETA: 3s - loss: 0.0586 - acc: 0.983 - ETA: 3s - loss: 0.0626 - acc: 0.983 - ETA: 3s - loss: 0.0598 - acc: 0.984 - ETA: 3s - loss: 0.0626 - acc: 0.982 - ETA: 3s - loss: 0.0638 - acc: 0.981 - ETA: 3s - loss: 0.0628 - acc: 0.983 - ETA: 3s - loss: 0.0632 - acc: 0.982 - ETA: 3s - loss: 0.0627 - acc: 0.982 - ETA: 3s - loss: 0.0614 - acc: 0.983 - ETA: 3s - loss: 0.0620 - acc: 0.981 - ETA: 3s - loss: 0.0670 - acc: 0.979 - ETA: 3s - loss: 0.0671 - acc: 0.979 - ETA: 3s - loss: 0.0672 - acc: 0.979 - ETA: 3s - loss: 0.0695 - acc: 0.977 - ETA: 3s - loss: 0.0694 - acc: 0.978 - ETA: 3s - loss: 0.0707 - acc: 0.977 - ETA: 3s - loss: 0.0703 - acc: 0.977 - ETA: 3s - loss: 0.0753 - acc: 0.976 - ETA: 3s - loss: 0.0743 - acc: 0.976 - ETA: 3s - loss: 0.0774 - acc: 0.975 - ETA: 3s - loss: 0.0774 - acc: 0.975 - ETA: 2s - loss: 0.0757 - acc: 0.976 - ETA: 2s - loss: 0.0765 - acc: 0.976 - ETA: 2s - loss: 0.0758 - acc: 0.976 - ETA: 2s - loss: 0.0787 - acc: 0.976 - ETA: 2s - loss: 0.0780 - acc: 0.976 - ETA: 2s - loss: 0.0777 - acc: 0.976 - ETA: 2s - loss: 0.0772 - acc: 0.976 - ETA: 2s - loss: 0.0780 - acc: 0.976 - ETA: 2s - loss: 0.0790 - acc: 0.976 - ETA: 2s - loss: 0.0779 - acc: 0.976 - ETA: 2s - loss: 0.0779 - acc: 0.976 - ETA: 2s - loss: 0.0778 - acc: 0.976 - ETA: 2s - loss: 0.0780 - acc: 0.976 - ETA: 2s - loss: 0.0767 - acc: 0.976 - ETA: 2s - loss: 0.0777 - acc: 0.976 - ETA: 2s - loss: 0.0791 - acc: 0.975 - ETA: 2s - loss: 0.0790 - acc: 0.975 - ETA: 2s - loss: 0.0788 - acc: 0.975 - ETA: 1s - loss: 0.0788 - acc: 0.975 - ETA: 1s - loss: 0.0805 - acc: 0.974 - ETA: 1s - loss: 0.0807 - acc: 0.974 - ETA: 1s - loss: 0.0827 - acc: 0.973 - ETA: 1s - loss: 0.0828 - acc: 0.973 - ETA: 1s - loss: 0.0825 - acc: 0.973 - ETA: 1s - loss: 0.0816 - acc: 0.973 - ETA: 1s - loss: 0.0810 - acc: 0.974 - ETA: 1s - loss: 0.0810 - acc: 0.974 - ETA: 1s - loss: 0.0807 - acc: 0.974 - ETA: 1s - loss: 0.0801 - acc: 0.974 - ETA: 1s - loss: 0.0805 - acc: 0.974 - ETA: 1s - loss: 0.0797 - acc: 0.974 - ETA: 1s - loss: 0.0807 - acc: 0.974 - ETA: 1s - loss: 0.0822 - acc: 0.973 - ETA: 1s - loss: 0.0829 - acc: 0.973 - ETA: 1s - loss: 0.0830 - acc: 0.973 - ETA: 1s - loss: 0.0824 - acc: 0.973 - ETA: 1s - loss: 0.0850 - acc: 0.972 - ETA: 0s - loss: 0.0868 - acc: 0.972 - ETA: 0s - loss: 0.0885 - acc: 0.971 - ETA: 0s - loss: 0.0882 - acc: 0.971 - ETA: 0s - loss: 0.0895 - acc: 0.971 - ETA: 0s - loss: 0.0889 - acc: 0.971 - ETA: 0s - loss: 0.0887 - acc: 0.971 - ETA: 0s - loss: 0.0881 - acc: 0.971 - ETA: 0s - loss: 0.0875 - acc: 0.971 - ETA: 0s - loss: 0.0870 - acc: 0.972 - ETA: 0s - loss: 0.0868 - acc: 0.972 - ETA: 0s - loss: 0.0866 - acc: 0.972 - ETA: 0s - loss: 0.0864 - acc: 0.972 - ETA: 0s - loss: 0.0887 - acc: 0.972 - ETA: 0s - loss: 0.0882 - acc: 0.972 - ETA: 0s - loss: 0.0900 - acc: 0.971 - ETA: 0s - loss: 0.0891 - acc: 0.971 - ETA: 0s - loss: 0.0901 - acc: 0.971 - ETA: 0s - loss: 0.0897 - acc: 0.971 - 5s 693us/step - loss: 0.0895 - acc: 0.9716 - val_loss: 0.6833 - val_acc: 0.8084\n",
      "\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 7/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0141 - acc: 1.000 - ETA: 4s - loss: 0.0397 - acc: 0.990 - ETA: 4s - loss: 0.0747 - acc: 0.980 - ETA: 4s - loss: 0.0655 - acc: 0.982 - ETA: 4s - loss: 0.0584 - acc: 0.983 - ETA: 4s - loss: 0.0499 - acc: 0.986 - ETA: 3s - loss: 0.0573 - acc: 0.984 - ETA: 3s - loss: 0.0594 - acc: 0.985 - ETA: 3s - loss: 0.0542 - acc: 0.986 - ETA: 3s - loss: 0.0514 - acc: 0.988 - ETA: 3s - loss: 0.0495 - acc: 0.988 - ETA: 3s - loss: 0.0496 - acc: 0.988 - ETA: 3s - loss: 0.0476 - acc: 0.989 - ETA: 3s - loss: 0.0494 - acc: 0.988 - ETA: 3s - loss: 0.0506 - acc: 0.988 - ETA: 3s - loss: 0.0488 - acc: 0.989 - ETA: 3s - loss: 0.0473 - acc: 0.990 - ETA: 3s - loss: 0.0463 - acc: 0.990 - ETA: 3s - loss: 0.0445 - acc: 0.990 - ETA: 3s - loss: 0.0484 - acc: 0.989 - ETA: 3s - loss: 0.0484 - acc: 0.989 - ETA: 3s - loss: 0.0490 - acc: 0.989 - ETA: 3s - loss: 0.0483 - acc: 0.989 - ETA: 3s - loss: 0.0489 - acc: 0.989 - ETA: 3s - loss: 0.0492 - acc: 0.988 - ETA: 2s - loss: 0.0508 - acc: 0.988 - ETA: 2s - loss: 0.0548 - acc: 0.986 - ETA: 2s - loss: 0.0567 - acc: 0.985 - ETA: 2s - loss: 0.0563 - acc: 0.985 - ETA: 2s - loss: 0.0557 - acc: 0.986 - ETA: 2s - loss: 0.0584 - acc: 0.985 - ETA: 2s - loss: 0.0609 - acc: 0.984 - ETA: 2s - loss: 0.0605 - acc: 0.985 - ETA: 2s - loss: 0.0631 - acc: 0.984 - ETA: 2s - loss: 0.0632 - acc: 0.984 - ETA: 2s - loss: 0.0629 - acc: 0.984 - ETA: 2s - loss: 0.0632 - acc: 0.983 - ETA: 2s - loss: 0.0623 - acc: 0.983 - ETA: 2s - loss: 0.0611 - acc: 0.984 - ETA: 2s - loss: 0.0604 - acc: 0.984 - ETA: 2s - loss: 0.0604 - acc: 0.984 - ETA: 2s - loss: 0.0612 - acc: 0.984 - ETA: 2s - loss: 0.0611 - acc: 0.984 - ETA: 2s - loss: 0.0611 - acc: 0.984 - ETA: 1s - loss: 0.0623 - acc: 0.983 - ETA: 1s - loss: 0.0620 - acc: 0.983 - ETA: 1s - loss: 0.0611 - acc: 0.984 - ETA: 1s - loss: 0.0613 - acc: 0.983 - ETA: 1s - loss: 0.0615 - acc: 0.983 - ETA: 1s - loss: 0.0622 - acc: 0.983 - ETA: 1s - loss: 0.0623 - acc: 0.983 - ETA: 1s - loss: 0.0622 - acc: 0.982 - ETA: 1s - loss: 0.0617 - acc: 0.983 - ETA: 1s - loss: 0.0625 - acc: 0.982 - ETA: 1s - loss: 0.0634 - acc: 0.982 - ETA: 1s - loss: 0.0637 - acc: 0.982 - ETA: 1s - loss: 0.0633 - acc: 0.982 - ETA: 1s - loss: 0.0630 - acc: 0.982 - ETA: 1s - loss: 0.0626 - acc: 0.982 - ETA: 1s - loss: 0.0622 - acc: 0.982 - ETA: 1s - loss: 0.0616 - acc: 0.982 - ETA: 1s - loss: 0.0620 - acc: 0.982 - ETA: 1s - loss: 0.0614 - acc: 0.982 - ETA: 1s - loss: 0.0614 - acc: 0.982 - ETA: 0s - loss: 0.0612 - acc: 0.982 - ETA: 0s - loss: 0.0615 - acc: 0.982 - ETA: 0s - loss: 0.0612 - acc: 0.982 - ETA: 0s - loss: 0.0606 - acc: 0.982 - ETA: 0s - loss: 0.0609 - acc: 0.982 - ETA: 0s - loss: 0.0616 - acc: 0.982 - ETA: 0s - loss: 0.0627 - acc: 0.982 - ETA: 0s - loss: 0.0631 - acc: 0.981 - ETA: 0s - loss: 0.0628 - acc: 0.982 - ETA: 0s - loss: 0.0638 - acc: 0.981 - ETA: 0s - loss: 0.0638 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0633 - acc: 0.981 - ETA: 0s - loss: 0.0632 - acc: 0.981 - ETA: 0s - loss: 0.0629 - acc: 0.981 - ETA: 0s - loss: 0.0631 - acc: 0.981 - ETA: 0s - loss: 0.0635 - acc: 0.981 - ETA: 0s - loss: 0.0640 - acc: 0.981 - ETA: 0s - loss: 0.0648 - acc: 0.980 - 5s 691us/step - loss: 0.0647 - acc: 0.9810 - val_loss: 0.7388 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00007: val_loss did not improve\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0304 - acc: 1.000 - ETA: 4s - loss: 0.0493 - acc: 0.990 - ETA: 4s - loss: 0.0352 - acc: 0.993 - ETA: 4s - loss: 0.0324 - acc: 0.995 - ETA: 4s - loss: 0.0322 - acc: 0.993 - ETA: 4s - loss: 0.0333 - acc: 0.992 - ETA: 4s - loss: 0.0303 - acc: 0.993 - ETA: 4s - loss: 0.0314 - acc: 0.991 - ETA: 4s - loss: 0.0361 - acc: 0.990 - ETA: 4s - loss: 0.0352 - acc: 0.991 - ETA: 4s - loss: 0.0378 - acc: 0.990 - ETA: 3s - loss: 0.0392 - acc: 0.988 - ETA: 3s - loss: 0.0383 - acc: 0.988 - ETA: 3s - loss: 0.0396 - acc: 0.988 - ETA: 3s - loss: 0.0375 - acc: 0.989 - ETA: 3s - loss: 0.0359 - acc: 0.990 - ETA: 3s - loss: 0.0341 - acc: 0.990 - ETA: 3s - loss: 0.0362 - acc: 0.990 - ETA: 3s - loss: 0.0363 - acc: 0.990 - ETA: 3s - loss: 0.0354 - acc: 0.990 - ETA: 3s - loss: 0.0346 - acc: 0.991 - ETA: 3s - loss: 0.0335 - acc: 0.991 - ETA: 3s - loss: 0.0345 - acc: 0.990 - ETA: 3s - loss: 0.0347 - acc: 0.990 - ETA: 3s - loss: 0.0345 - acc: 0.991 - ETA: 3s - loss: 0.0343 - acc: 0.991 - ETA: 3s - loss: 0.0354 - acc: 0.990 - ETA: 3s - loss: 0.0360 - acc: 0.990 - ETA: 3s - loss: 0.0362 - acc: 0.990 - ETA: 2s - loss: 0.0361 - acc: 0.990 - ETA: 2s - loss: 0.0359 - acc: 0.991 - ETA: 2s - loss: 0.0363 - acc: 0.991 - ETA: 2s - loss: 0.0358 - acc: 0.991 - ETA: 2s - loss: 0.0352 - acc: 0.991 - ETA: 2s - loss: 0.0346 - acc: 0.991 - ETA: 2s - loss: 0.0344 - acc: 0.991 - ETA: 2s - loss: 0.0342 - acc: 0.991 - ETA: 2s - loss: 0.0347 - acc: 0.991 - ETA: 2s - loss: 0.0357 - acc: 0.990 - ETA: 2s - loss: 0.0358 - acc: 0.990 - ETA: 2s - loss: 0.0364 - acc: 0.990 - ETA: 2s - loss: 0.0368 - acc: 0.990 - ETA: 2s - loss: 0.0434 - acc: 0.988 - ETA: 2s - loss: 0.0438 - acc: 0.988 - ETA: 2s - loss: 0.0447 - acc: 0.987 - ETA: 2s - loss: 0.0440 - acc: 0.987 - ETA: 2s - loss: 0.0440 - acc: 0.987 - ETA: 2s - loss: 0.0437 - acc: 0.988 - ETA: 1s - loss: 0.0442 - acc: 0.987 - ETA: 1s - loss: 0.0437 - acc: 0.988 - ETA: 1s - loss: 0.0436 - acc: 0.988 - ETA: 1s - loss: 0.0433 - acc: 0.988 - ETA: 1s - loss: 0.0428 - acc: 0.988 - ETA: 1s - loss: 0.0427 - acc: 0.988 - ETA: 1s - loss: 0.0430 - acc: 0.988 - ETA: 1s - loss: 0.0430 - acc: 0.988 - ETA: 1s - loss: 0.0436 - acc: 0.988 - ETA: 1s - loss: 0.0439 - acc: 0.988 - ETA: 1s - loss: 0.0463 - acc: 0.987 - ETA: 1s - loss: 0.0460 - acc: 0.987 - ETA: 1s - loss: 0.0457 - acc: 0.988 - ETA: 1s - loss: 0.0453 - acc: 0.988 - ETA: 1s - loss: 0.0447 - acc: 0.988 - ETA: 1s - loss: 0.0453 - acc: 0.987 - ETA: 1s - loss: 0.0468 - acc: 0.987 - ETA: 1s - loss: 0.0472 - acc: 0.986 - ETA: 0s - loss: 0.0472 - acc: 0.986 - ETA: 0s - loss: 0.0468 - acc: 0.987 - ETA: 0s - loss: 0.0469 - acc: 0.986 - ETA: 0s - loss: 0.0465 - acc: 0.987 - ETA: 0s - loss: 0.0468 - acc: 0.987 - ETA: 0s - loss: 0.0466 - acc: 0.987 - ETA: 0s - loss: 0.0465 - acc: 0.987 - ETA: 0s - loss: 0.0463 - acc: 0.987 - ETA: 0s - loss: 0.0471 - acc: 0.986 - ETA: 0s - loss: 0.0469 - acc: 0.986 - ETA: 0s - loss: 0.0467 - acc: 0.987 - ETA: 0s - loss: 0.0465 - acc: 0.987 - ETA: 0s - loss: 0.0466 - acc: 0.987 - ETA: 0s - loss: 0.0468 - acc: 0.987 - ETA: 0s - loss: 0.0466 - acc: 0.987 - ETA: 0s - loss: 0.0471 - acc: 0.986 - ETA: 0s - loss: 0.0468 - acc: 0.986 - ETA: 0s - loss: 0.0471 - acc: 0.986 - 5s 696us/step - loss: 0.0473 - acc: 0.9867 - val_loss: 0.7395 - val_acc: 0.8024\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 0.0395 - acc: 1.000 - ETA: 4s - loss: 0.0149 - acc: 1.000 - ETA: 4s - loss: 0.0199 - acc: 1.000 - ETA: 4s - loss: 0.0164 - acc: 1.000 - ETA: 4s - loss: 0.0144 - acc: 1.000 - ETA: 4s - loss: 0.0132 - acc: 1.000 - ETA: 4s - loss: 0.0123 - acc: 1.000 - ETA: 4s - loss: 0.0216 - acc: 0.998 - ETA: 4s - loss: 0.0226 - acc: 0.998 - ETA: 4s - loss: 0.0219 - acc: 0.998 - ETA: 3s - loss: 0.0204 - acc: 0.998 - ETA: 3s - loss: 0.0219 - acc: 0.997 - ETA: 3s - loss: 0.0263 - acc: 0.995 - ETA: 3s - loss: 0.0261 - acc: 0.996 - ETA: 3s - loss: 0.0275 - acc: 0.994 - ETA: 3s - loss: 0.0283 - acc: 0.994 - ETA: 3s - loss: 0.0298 - acc: 0.993 - ETA: 3s - loss: 0.0308 - acc: 0.993 - ETA: 3s - loss: 0.0312 - acc: 0.993 - ETA: 3s - loss: 0.0306 - acc: 0.992 - ETA: 3s - loss: 0.0305 - acc: 0.992 - ETA: 3s - loss: 0.0318 - acc: 0.992 - ETA: 3s - loss: 0.0321 - acc: 0.992 - ETA: 3s - loss: 0.0319 - acc: 0.992 - ETA: 3s - loss: 0.0317 - acc: 0.991 - ETA: 3s - loss: 0.0314 - acc: 0.992 - ETA: 3s - loss: 0.0307 - acc: 0.992 - ETA: 3s - loss: 0.0302 - acc: 0.992 - ETA: 2s - loss: 0.0298 - acc: 0.992 - ETA: 2s - loss: 0.0292 - acc: 0.992 - ETA: 2s - loss: 0.0296 - acc: 0.992 - ETA: 2s - loss: 0.0328 - acc: 0.992 - ETA: 2s - loss: 0.0325 - acc: 0.992 - ETA: 2s - loss: 0.0322 - acc: 0.992 - ETA: 2s - loss: 0.0329 - acc: 0.992 - ETA: 2s - loss: 0.0333 - acc: 0.992 - ETA: 2s - loss: 0.0326 - acc: 0.992 - ETA: 2s - loss: 0.0328 - acc: 0.992 - ETA: 2s - loss: 0.0325 - acc: 0.992 - ETA: 2s - loss: 0.0322 - acc: 0.992 - ETA: 2s - loss: 0.0331 - acc: 0.992 - ETA: 2s - loss: 0.0328 - acc: 0.992 - ETA: 2s - loss: 0.0330 - acc: 0.992 - ETA: 2s - loss: 0.0328 - acc: 0.992 - ETA: 2s - loss: 0.0328 - acc: 0.992 - ETA: 2s - loss: 0.0323 - acc: 0.992 - ETA: 1s - loss: 0.0319 - acc: 0.992 - ETA: 1s - loss: 0.0317 - acc: 0.992 - ETA: 1s - loss: 0.0313 - acc: 0.992 - ETA: 1s - loss: 0.0308 - acc: 0.992 - ETA: 1s - loss: 0.0306 - acc: 0.993 - ETA: 1s - loss: 0.0307 - acc: 0.993 - ETA: 1s - loss: 0.0308 - acc: 0.992 - ETA: 1s - loss: 0.0312 - acc: 0.992 - ETA: 1s - loss: 0.0308 - acc: 0.992 - ETA: 1s - loss: 0.0306 - acc: 0.993 - ETA: 1s - loss: 0.0303 - acc: 0.993 - ETA: 1s - loss: 0.0300 - acc: 0.993 - ETA: 1s - loss: 0.0308 - acc: 0.992 - ETA: 1s - loss: 0.0307 - acc: 0.993 - ETA: 1s - loss: 0.0316 - acc: 0.992 - ETA: 1s - loss: 0.0314 - acc: 0.992 - ETA: 1s - loss: 0.0311 - acc: 0.992 - ETA: 1s - loss: 0.0310 - acc: 0.992 - ETA: 1s - loss: 0.0314 - acc: 0.992 - ETA: 0s - loss: 0.0324 - acc: 0.992 - ETA: 0s - loss: 0.0323 - acc: 0.992 - ETA: 0s - loss: 0.0323 - acc: 0.992 - ETA: 0s - loss: 0.0320 - acc: 0.992 - ETA: 0s - loss: 0.0316 - acc: 0.992 - ETA: 0s - loss: 0.0319 - acc: 0.992 - ETA: 0s - loss: 0.0317 - acc: 0.992 - ETA: 0s - loss: 0.0322 - acc: 0.991 - ETA: 0s - loss: 0.0326 - acc: 0.991 - ETA: 0s - loss: 0.0324 - acc: 0.991 - ETA: 0s - loss: 0.0333 - acc: 0.991 - ETA: 0s - loss: 0.0329 - acc: 0.991 - ETA: 0s - loss: 0.0331 - acc: 0.991 - ETA: 0s - loss: 0.0331 - acc: 0.991 - ETA: 0s - loss: 0.0332 - acc: 0.991 - ETA: 0s - loss: 0.0331 - acc: 0.991 - ETA: 0s - loss: 0.0335 - acc: 0.991 - ETA: 0s - loss: 0.0334 - acc: 0.990 - 5s 695us/step - loss: 0.0332 - acc: 0.9910 - val_loss: 0.7225 - val_acc: 0.8228\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0083 - acc: 1.000 - ETA: 4s - loss: 0.0188 - acc: 1.000 - ETA: 4s - loss: 0.0179 - acc: 1.000 - ETA: 4s - loss: 0.0176 - acc: 1.000 - ETA: 4s - loss: 0.0170 - acc: 1.000 - ETA: 4s - loss: 0.0165 - acc: 1.000 - ETA: 4s - loss: 0.0166 - acc: 1.000 - ETA: 4s - loss: 0.0201 - acc: 0.996 - ETA: 3s - loss: 0.0200 - acc: 0.995 - ETA: 3s - loss: 0.0207 - acc: 0.994 - ETA: 3s - loss: 0.0208 - acc: 0.995 - ETA: 3s - loss: 0.0200 - acc: 0.995 - ETA: 3s - loss: 0.0192 - acc: 0.996 - ETA: 3s - loss: 0.0201 - acc: 0.995 - ETA: 3s - loss: 0.0194 - acc: 0.995 - ETA: 3s - loss: 0.0205 - acc: 0.995 - ETA: 3s - loss: 0.0209 - acc: 0.994 - ETA: 3s - loss: 0.0212 - acc: 0.994 - ETA: 3s - loss: 0.0208 - acc: 0.994 - ETA: 3s - loss: 0.0219 - acc: 0.994 - ETA: 3s - loss: 0.0214 - acc: 0.994 - ETA: 3s - loss: 0.0208 - acc: 0.994 - ETA: 3s - loss: 0.0206 - acc: 0.995 - ETA: 3s - loss: 0.0219 - acc: 0.994 - ETA: 3s - loss: 0.0218 - acc: 0.993 - ETA: 3s - loss: 0.0223 - acc: 0.993 - ETA: 3s - loss: 0.0224 - acc: 0.993 - ETA: 2s - loss: 0.0220 - acc: 0.993 - ETA: 2s - loss: 0.0227 - acc: 0.993 - ETA: 2s - loss: 0.0221 - acc: 0.993 - ETA: 2s - loss: 0.0226 - acc: 0.993 - ETA: 2s - loss: 0.0224 - acc: 0.993 - ETA: 2s - loss: 0.0225 - acc: 0.993 - ETA: 2s - loss: 0.0222 - acc: 0.994 - ETA: 2s - loss: 0.0219 - acc: 0.994 - ETA: 2s - loss: 0.0239 - acc: 0.994 - ETA: 2s - loss: 0.0245 - acc: 0.993 - ETA: 2s - loss: 0.0276 - acc: 0.991 - ETA: 2s - loss: 0.0275 - acc: 0.991 - ETA: 2s - loss: 0.0271 - acc: 0.991 - ETA: 2s - loss: 0.0267 - acc: 0.992 - ETA: 2s - loss: 0.0267 - acc: 0.991 - ETA: 2s - loss: 0.0263 - acc: 0.992 - ETA: 2s - loss: 0.0271 - acc: 0.992 - ETA: 2s - loss: 0.0274 - acc: 0.991 - ETA: 2s - loss: 0.0272 - acc: 0.992 - ETA: 1s - loss: 0.0270 - acc: 0.992 - ETA: 1s - loss: 0.0268 - acc: 0.992 - ETA: 1s - loss: 0.0267 - acc: 0.992 - ETA: 1s - loss: 0.0268 - acc: 0.992 - ETA: 1s - loss: 0.0269 - acc: 0.992 - ETA: 1s - loss: 0.0268 - acc: 0.992 - ETA: 1s - loss: 0.0266 - acc: 0.992 - ETA: 1s - loss: 0.0265 - acc: 0.992 - ETA: 1s - loss: 0.0263 - acc: 0.992 - ETA: 1s - loss: 0.0264 - acc: 0.992 - ETA: 1s - loss: 0.0266 - acc: 0.992 - ETA: 1s - loss: 0.0263 - acc: 0.992 - ETA: 1s - loss: 0.0262 - acc: 0.992 - ETA: 1s - loss: 0.0259 - acc: 0.992 - ETA: 1s - loss: 0.0261 - acc: 0.992 - ETA: 1s - loss: 0.0283 - acc: 0.992 - ETA: 1s - loss: 0.0281 - acc: 0.992 - ETA: 1s - loss: 0.0277 - acc: 0.992 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.992 - ETA: 0s - loss: 0.0280 - acc: 0.992 - ETA: 0s - loss: 0.0278 - acc: 0.992 - ETA: 0s - loss: 0.0277 - acc: 0.992 - ETA: 0s - loss: 0.0283 - acc: 0.991 - ETA: 0s - loss: 0.0282 - acc: 0.992 - ETA: 0s - loss: 0.0279 - acc: 0.992 - ETA: 0s - loss: 0.0277 - acc: 0.992 - ETA: 0s - loss: 0.0275 - acc: 0.992 - ETA: 0s - loss: 0.0276 - acc: 0.992 - ETA: 0s - loss: 0.0274 - acc: 0.992 - ETA: 0s - loss: 0.0272 - acc: 0.992 - ETA: 0s - loss: 0.0270 - acc: 0.992 - ETA: 0s - loss: 0.0268 - acc: 0.992 - 5s 688us/step - loss: 0.0267 - acc: 0.9927 - val_loss: 0.7761 - val_acc: 0.8216\n",
      "\n",
      "Epoch 00010: val_loss did not improve\n",
      "Epoch 11/20\n",
      "6680/6680 [==============================] - ETA: 18s - loss: 0.0028 - acc: 1.00 - ETA: 7s - loss: 0.0028 - acc: 1.0000 - ETA: 5s - loss: 0.0032 - acc: 1.000 - ETA: 5s - loss: 0.0114 - acc: 0.996 - ETA: 5s - loss: 0.0095 - acc: 0.997 - ETA: 4s - loss: 0.0138 - acc: 0.992 - ETA: 4s - loss: 0.0124 - acc: 0.994 - ETA: 4s - loss: 0.0149 - acc: 0.993 - ETA: 4s - loss: 0.0151 - acc: 0.993 - ETA: 4s - loss: 0.0139 - acc: 0.994 - ETA: 4s - loss: 0.0129 - acc: 0.995 - ETA: 4s - loss: 0.0122 - acc: 0.995 - ETA: 3s - loss: 0.0137 - acc: 0.996 - ETA: 3s - loss: 0.0136 - acc: 0.995 - ETA: 3s - loss: 0.0133 - acc: 0.995 - ETA: 3s - loss: 0.0147 - acc: 0.995 - ETA: 3s - loss: 0.0158 - acc: 0.994 - ETA: 3s - loss: 0.0153 - acc: 0.995 - ETA: 3s - loss: 0.0149 - acc: 0.995 - ETA: 3s - loss: 0.0142 - acc: 0.995 - ETA: 3s - loss: 0.0137 - acc: 0.995 - ETA: 3s - loss: 0.0135 - acc: 0.996 - ETA: 3s - loss: 0.0144 - acc: 0.995 - ETA: 3s - loss: 0.0147 - acc: 0.995 - ETA: 3s - loss: 0.0146 - acc: 0.996 - ETA: 3s - loss: 0.0155 - acc: 0.995 - ETA: 3s - loss: 0.0153 - acc: 0.995 - ETA: 3s - loss: 0.0156 - acc: 0.995 - ETA: 2s - loss: 0.0152 - acc: 0.995 - ETA: 2s - loss: 0.0161 - acc: 0.995 - ETA: 2s - loss: 0.0159 - acc: 0.995 - ETA: 2s - loss: 0.0155 - acc: 0.995 - ETA: 2s - loss: 0.0164 - acc: 0.994 - ETA: 2s - loss: 0.0160 - acc: 0.994 - ETA: 2s - loss: 0.0160 - acc: 0.995 - ETA: 2s - loss: 0.0158 - acc: 0.995 - ETA: 2s - loss: 0.0167 - acc: 0.994 - ETA: 2s - loss: 0.0168 - acc: 0.995 - ETA: 2s - loss: 0.0165 - acc: 0.995 - ETA: 2s - loss: 0.0164 - acc: 0.995 - ETA: 2s - loss: 0.0184 - acc: 0.994 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0181 - acc: 0.994 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0187 - acc: 0.994 - ETA: 1s - loss: 0.0188 - acc: 0.994 - ETA: 1s - loss: 0.0186 - acc: 0.994 - ETA: 1s - loss: 0.0186 - acc: 0.994 - ETA: 1s - loss: 0.0198 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0197 - acc: 0.994 - ETA: 1s - loss: 0.0195 - acc: 0.994 - ETA: 1s - loss: 0.0193 - acc: 0.994 - ETA: 1s - loss: 0.0193 - acc: 0.995 - ETA: 1s - loss: 0.0200 - acc: 0.994 - ETA: 1s - loss: 0.0198 - acc: 0.994 - ETA: 1s - loss: 0.0196 - acc: 0.995 - ETA: 1s - loss: 0.0194 - acc: 0.995 - ETA: 1s - loss: 0.0194 - acc: 0.995 - ETA: 1s - loss: 0.0192 - acc: 0.995 - ETA: 1s - loss: 0.0191 - acc: 0.995 - ETA: 1s - loss: 0.0190 - acc: 0.995 - ETA: 1s - loss: 0.0190 - acc: 0.995 - ETA: 1s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0190 - acc: 0.995 - ETA: 0s - loss: 0.0189 - acc: 0.995 - ETA: 0s - loss: 0.0190 - acc: 0.995 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0187 - acc: 0.995 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.995 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0188 - acc: 0.994 - ETA: 0s - loss: 0.0189 - acc: 0.994 - ETA: 0s - loss: 0.0190 - acc: 0.994 - ETA: 0s - loss: 0.0202 - acc: 0.994 - ETA: 0s - loss: 0.0200 - acc: 0.994 - ETA: 0s - loss: 0.0198 - acc: 0.994 - ETA: 0s - loss: 0.0203 - acc: 0.994 - ETA: 0s - loss: 0.0217 - acc: 0.994 - ETA: 0s - loss: 0.0216 - acc: 0.994 - ETA: 0s - loss: 0.0215 - acc: 0.994 - 5s 702us/step - loss: 0.0215 - acc: 0.9943 - val_loss: 0.7657 - val_acc: 0.8168\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0011 - acc: 1.000 - ETA: 4s - loss: 0.0028 - acc: 1.000 - ETA: 4s - loss: 0.0030 - acc: 1.000 - ETA: 4s - loss: 0.0031 - acc: 1.000 - ETA: 4s - loss: 0.0037 - acc: 1.000 - ETA: 4s - loss: 0.0046 - acc: 1.000 - ETA: 4s - loss: 0.0045 - acc: 1.000 - ETA: 4s - loss: 0.0094 - acc: 0.998 - ETA: 4s - loss: 0.0090 - acc: 0.998 - ETA: 4s - loss: 0.0084 - acc: 0.998 - ETA: 4s - loss: 0.0078 - acc: 0.998 - ETA: 3s - loss: 0.0079 - acc: 0.998 - ETA: 3s - loss: 0.0075 - acc: 0.999 - ETA: 3s - loss: 0.0097 - acc: 0.997 - ETA: 3s - loss: 0.0096 - acc: 0.997 - ETA: 3s - loss: 0.0141 - acc: 0.996 - ETA: 3s - loss: 0.0139 - acc: 0.996 - ETA: 3s - loss: 0.0134 - acc: 0.997 - ETA: 3s - loss: 0.0143 - acc: 0.995 - ETA: 3s - loss: 0.0150 - acc: 0.995 - ETA: 3s - loss: 0.0161 - acc: 0.995 - ETA: 3s - loss: 0.0161 - acc: 0.995 - ETA: 3s - loss: 0.0158 - acc: 0.995 - ETA: 3s - loss: 0.0154 - acc: 0.995 - ETA: 3s - loss: 0.0149 - acc: 0.995 - ETA: 3s - loss: 0.0206 - acc: 0.995 - ETA: 3s - loss: 0.0201 - acc: 0.995 - ETA: 3s - loss: 0.0195 - acc: 0.995 - ETA: 3s - loss: 0.0194 - acc: 0.995 - ETA: 2s - loss: 0.0190 - acc: 0.995 - ETA: 2s - loss: 0.0185 - acc: 0.995 - ETA: 2s - loss: 0.0180 - acc: 0.995 - ETA: 2s - loss: 0.0183 - acc: 0.995 - ETA: 2s - loss: 0.0181 - acc: 0.995 - ETA: 2s - loss: 0.0176 - acc: 0.995 - ETA: 2s - loss: 0.0175 - acc: 0.995 - ETA: 2s - loss: 0.0178 - acc: 0.995 - ETA: 2s - loss: 0.0174 - acc: 0.995 - ETA: 2s - loss: 0.0170 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.996 - ETA: 2s - loss: 0.0166 - acc: 0.996 - ETA: 2s - loss: 0.0172 - acc: 0.995 - ETA: 2s - loss: 0.0168 - acc: 0.995 - ETA: 2s - loss: 0.0166 - acc: 0.995 - ETA: 2s - loss: 0.0165 - acc: 0.995 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0161 - acc: 0.996 - ETA: 1s - loss: 0.0158 - acc: 0.996 - ETA: 1s - loss: 0.0163 - acc: 0.996 - ETA: 1s - loss: 0.0160 - acc: 0.996 - ETA: 1s - loss: 0.0158 - acc: 0.996 - ETA: 1s - loss: 0.0156 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.996 - ETA: 1s - loss: 0.0157 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0158 - acc: 0.995 - ETA: 1s - loss: 0.0156 - acc: 0.995 - ETA: 1s - loss: 0.0154 - acc: 0.996 - ETA: 1s - loss: 0.0162 - acc: 0.995 - ETA: 1s - loss: 0.0161 - acc: 0.995 - ETA: 1s - loss: 0.0159 - acc: 0.996 - ETA: 1s - loss: 0.0166 - acc: 0.995 - ETA: 1s - loss: 0.0167 - acc: 0.995 - ETA: 0s - loss: 0.0165 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0169 - acc: 0.995 - ETA: 0s - loss: 0.0175 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0171 - acc: 0.995 - ETA: 0s - loss: 0.0170 - acc: 0.995 - ETA: 0s - loss: 0.0173 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0174 - acc: 0.995 - ETA: 0s - loss: 0.0182 - acc: 0.995 - ETA: 0s - loss: 0.0179 - acc: 0.995 - ETA: 0s - loss: 0.0178 - acc: 0.995 - ETA: 0s - loss: 0.0181 - acc: 0.995 - ETA: 0s - loss: 0.0182 - acc: 0.995 - 5s 675us/step - loss: 0.0181 - acc: 0.9952 - val_loss: 0.8165 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0011 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.000 - ETA: 4s - loss: 0.0020 - acc: 1.000 - ETA: 4s - loss: 0.0018 - acc: 1.000 - ETA: 4s - loss: 0.0034 - acc: 1.000 - ETA: 3s - loss: 0.0122 - acc: 0.997 - ETA: 3s - loss: 0.0114 - acc: 0.998 - ETA: 3s - loss: 0.0104 - acc: 0.998 - ETA: 3s - loss: 0.0099 - acc: 0.998 - ETA: 3s - loss: 0.0177 - acc: 0.996 - ETA: 3s - loss: 0.0160 - acc: 0.996 - ETA: 3s - loss: 0.0147 - acc: 0.996 - ETA: 3s - loss: 0.0143 - acc: 0.997 - ETA: 3s - loss: 0.0134 - acc: 0.997 - ETA: 3s - loss: 0.0126 - acc: 0.997 - ETA: 3s - loss: 0.0118 - acc: 0.997 - ETA: 3s - loss: 0.0131 - acc: 0.997 - ETA: 3s - loss: 0.0130 - acc: 0.997 - ETA: 3s - loss: 0.0130 - acc: 0.997 - ETA: 3s - loss: 0.0126 - acc: 0.997 - ETA: 3s - loss: 0.0123 - acc: 0.997 - ETA: 3s - loss: 0.0125 - acc: 0.997 - ETA: 3s - loss: 0.0120 - acc: 0.997 - ETA: 3s - loss: 0.0117 - acc: 0.998 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0114 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0108 - acc: 0.998 - ETA: 2s - loss: 0.0109 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0103 - acc: 0.998 - ETA: 2s - loss: 0.0104 - acc: 0.998 - ETA: 2s - loss: 0.0123 - acc: 0.998 - ETA: 2s - loss: 0.0121 - acc: 0.998 - ETA: 2s - loss: 0.0119 - acc: 0.998 - ETA: 2s - loss: 0.0118 - acc: 0.998 - ETA: 2s - loss: 0.0122 - acc: 0.998 - ETA: 2s - loss: 0.0124 - acc: 0.997 - ETA: 2s - loss: 0.0128 - acc: 0.997 - ETA: 2s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0123 - acc: 0.997 - ETA: 1s - loss: 0.0121 - acc: 0.997 - ETA: 1s - loss: 0.0121 - acc: 0.997 - ETA: 1s - loss: 0.0127 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0125 - acc: 0.997 - ETA: 1s - loss: 0.0132 - acc: 0.997 - ETA: 1s - loss: 0.0131 - acc: 0.997 - ETA: 0s - loss: 0.0130 - acc: 0.997 - ETA: 0s - loss: 0.0129 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.997 - ETA: 0s - loss: 0.0126 - acc: 0.997 - ETA: 0s - loss: 0.0129 - acc: 0.997 - ETA: 0s - loss: 0.0132 - acc: 0.997 - ETA: 0s - loss: 0.0131 - acc: 0.997 - ETA: 0s - loss: 0.0130 - acc: 0.997 - ETA: 0s - loss: 0.0129 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0146 - acc: 0.997 - ETA: 0s - loss: 0.0145 - acc: 0.997 - ETA: 0s - loss: 0.0145 - acc: 0.997 - ETA: 0s - loss: 0.0145 - acc: 0.997 - ETA: 0s - loss: 0.0143 - acc: 0.997 - ETA: 0s - loss: 0.0143 - acc: 0.997 - ETA: 0s - loss: 0.0147 - acc: 0.997 - ETA: 0s - loss: 0.0146 - acc: 0.997 - 4s 669us/step - loss: 0.0145 - acc: 0.9975 - val_loss: 0.7858 - val_acc: 0.8323\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0094 - acc: 1.000 - ETA: 4s - loss: 0.0044 - acc: 1.000 - ETA: 4s - loss: 0.0045 - acc: 1.000 - ETA: 4s - loss: 0.0037 - acc: 1.000 - ETA: 4s - loss: 0.0047 - acc: 1.000 - ETA: 4s - loss: 0.0040 - acc: 1.000 - ETA: 4s - loss: 0.0045 - acc: 1.000 - ETA: 4s - loss: 0.0045 - acc: 1.000 - ETA: 3s - loss: 0.0045 - acc: 1.000 - ETA: 3s - loss: 0.0048 - acc: 1.000 - ETA: 3s - loss: 0.0048 - acc: 1.000 - ETA: 3s - loss: 0.0045 - acc: 1.000 - ETA: 3s - loss: 0.0045 - acc: 1.000 - ETA: 3s - loss: 0.0042 - acc: 1.000 - ETA: 3s - loss: 0.0041 - acc: 1.000 - ETA: 3s - loss: 0.0039 - acc: 1.000 - ETA: 3s - loss: 0.0039 - acc: 1.000 - ETA: 3s - loss: 0.0042 - acc: 1.000 - ETA: 3s - loss: 0.0041 - acc: 1.000 - ETA: 3s - loss: 0.0041 - acc: 1.000 - ETA: 3s - loss: 0.0042 - acc: 1.000 - ETA: 3s - loss: 0.0041 - acc: 1.000 - ETA: 3s - loss: 0.0040 - acc: 1.000 - ETA: 3s - loss: 0.0039 - acc: 1.000 - ETA: 3s - loss: 0.0043 - acc: 1.000 - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0042 - acc: 1.000 - ETA: 2s - loss: 0.0077 - acc: 0.999 - ETA: 2s - loss: 0.0087 - acc: 0.998 - ETA: 2s - loss: 0.0089 - acc: 0.998 - ETA: 2s - loss: 0.0087 - acc: 0.998 - ETA: 2s - loss: 0.0085 - acc: 0.998 - ETA: 2s - loss: 0.0084 - acc: 0.998 - ETA: 2s - loss: 0.0106 - acc: 0.998 - ETA: 2s - loss: 0.0116 - acc: 0.998 - ETA: 2s - loss: 0.0114 - acc: 0.998 - ETA: 2s - loss: 0.0114 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 2s - loss: 0.0109 - acc: 0.998 - ETA: 2s - loss: 0.0109 - acc: 0.998 - ETA: 2s - loss: 0.0113 - acc: 0.998 - ETA: 2s - loss: 0.0111 - acc: 0.998 - ETA: 1s - loss: 0.0109 - acc: 0.998 - ETA: 1s - loss: 0.0107 - acc: 0.998 - ETA: 1s - loss: 0.0108 - acc: 0.998 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0124 - acc: 0.997 - ETA: 1s - loss: 0.0131 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0137 - acc: 0.997 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0132 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 1s - loss: 0.0129 - acc: 0.997 - ETA: 1s - loss: 0.0128 - acc: 0.997 - ETA: 1s - loss: 0.0126 - acc: 0.997 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0136 - acc: 0.997 - ETA: 1s - loss: 0.0134 - acc: 0.997 - ETA: 1s - loss: 0.0133 - acc: 0.997 - ETA: 1s - loss: 0.0132 - acc: 0.997 - ETA: 1s - loss: 0.0130 - acc: 0.997 - ETA: 0s - loss: 0.0129 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.997 - ETA: 0s - loss: 0.0131 - acc: 0.997 - ETA: 0s - loss: 0.0130 - acc: 0.997 - ETA: 0s - loss: 0.0129 - acc: 0.997 - ETA: 0s - loss: 0.0127 - acc: 0.997 - ETA: 0s - loss: 0.0126 - acc: 0.997 - ETA: 0s - loss: 0.0127 - acc: 0.997 - ETA: 0s - loss: 0.0126 - acc: 0.997 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - ETA: 0s - loss: 0.0122 - acc: 0.997 - ETA: 0s - loss: 0.0121 - acc: 0.997 - ETA: 0s - loss: 0.0125 - acc: 0.997 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0124 - acc: 0.997 - ETA: 0s - loss: 0.0123 - acc: 0.997 - ETA: 0s - loss: 0.0128 - acc: 0.997 - 4s 668us/step - loss: 0.0127 - acc: 0.9973 - val_loss: 0.8465 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00014: val_loss did not improve\n",
      "Epoch 15/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 0.0614 - acc: 0.950 - ETA: 4s - loss: 0.0197 - acc: 0.990 - ETA: 4s - loss: 0.0120 - acc: 0.994 - ETA: 4s - loss: 0.0097 - acc: 0.996 - ETA: 4s - loss: 0.0083 - acc: 0.997 - ETA: 4s - loss: 0.0073 - acc: 0.997 - ETA: 4s - loss: 0.0076 - acc: 0.998 - ETA: 3s - loss: 0.0093 - acc: 0.996 - ETA: 3s - loss: 0.0083 - acc: 0.997 - ETA: 3s - loss: 0.0079 - acc: 0.997 - ETA: 3s - loss: 0.0086 - acc: 0.996 - ETA: 3s - loss: 0.0081 - acc: 0.996 - ETA: 3s - loss: 0.0076 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 3s - loss: 0.0069 - acc: 0.997 - ETA: 3s - loss: 0.0066 - acc: 0.997 - ETA: 3s - loss: 0.0083 - acc: 0.995 - ETA: 3s - loss: 0.0082 - acc: 0.995 - ETA: 3s - loss: 0.0080 - acc: 0.995 - ETA: 3s - loss: 0.0076 - acc: 0.996 - ETA: 3s - loss: 0.0073 - acc: 0.996 - ETA: 3s - loss: 0.0070 - acc: 0.996 - ETA: 3s - loss: 0.0069 - acc: 0.996 - ETA: 3s - loss: 0.0078 - acc: 0.996 - ETA: 3s - loss: 0.0076 - acc: 0.996 - ETA: 3s - loss: 0.0075 - acc: 0.996 - ETA: 2s - loss: 0.0073 - acc: 0.996 - ETA: 2s - loss: 0.0071 - acc: 0.996 - ETA: 2s - loss: 0.0092 - acc: 0.996 - ETA: 2s - loss: 0.0093 - acc: 0.996 - ETA: 2s - loss: 0.0092 - acc: 0.996 - ETA: 2s - loss: 0.0089 - acc: 0.996 - ETA: 2s - loss: 0.0091 - acc: 0.996 - ETA: 2s - loss: 0.0089 - acc: 0.996 - ETA: 2s - loss: 0.0090 - acc: 0.996 - ETA: 2s - loss: 0.0088 - acc: 0.996 - ETA: 2s - loss: 0.0086 - acc: 0.996 - ETA: 2s - loss: 0.0084 - acc: 0.996 - ETA: 2s - loss: 0.0086 - acc: 0.996 - ETA: 2s - loss: 0.0085 - acc: 0.996 - ETA: 2s - loss: 0.0083 - acc: 0.996 - ETA: 2s - loss: 0.0083 - acc: 0.996 - ETA: 2s - loss: 0.0081 - acc: 0.996 - ETA: 1s - loss: 0.0080 - acc: 0.996 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0077 - acc: 0.997 - ETA: 1s - loss: 0.0077 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0073 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0074 - acc: 0.997 - ETA: 1s - loss: 0.0089 - acc: 0.997 - ETA: 1s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0087 - acc: 0.997 - ETA: 1s - loss: 0.0103 - acc: 0.997 - ETA: 1s - loss: 0.0101 - acc: 0.997 - ETA: 1s - loss: 0.0106 - acc: 0.996 - ETA: 0s - loss: 0.0105 - acc: 0.996 - ETA: 0s - loss: 0.0103 - acc: 0.996 - ETA: 0s - loss: 0.0102 - acc: 0.996 - ETA: 0s - loss: 0.0101 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.996 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0098 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0092 - acc: 0.997 - 5s 676us/step - loss: 0.0092 - acc: 0.9975 - val_loss: 0.8295 - val_acc: 0.8240\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 0.0207 - acc: 1.000 - ETA: 4s - loss: 0.0072 - acc: 1.000 - ETA: 4s - loss: 0.0053 - acc: 1.000 - ETA: 4s - loss: 0.0069 - acc: 0.996 - ETA: 4s - loss: 0.0194 - acc: 0.991 - ETA: 4s - loss: 0.0160 - acc: 0.992 - ETA: 3s - loss: 0.0135 - acc: 0.994 - ETA: 3s - loss: 0.0155 - acc: 0.993 - ETA: 3s - loss: 0.0137 - acc: 0.993 - ETA: 3s - loss: 0.0133 - acc: 0.994 - ETA: 3s - loss: 0.0125 - acc: 0.995 - ETA: 3s - loss: 0.0115 - acc: 0.995 - ETA: 3s - loss: 0.0106 - acc: 0.995 - ETA: 3s - loss: 0.0099 - acc: 0.996 - ETA: 3s - loss: 0.0092 - acc: 0.996 - ETA: 3s - loss: 0.0087 - acc: 0.996 - ETA: 3s - loss: 0.0082 - acc: 0.996 - ETA: 3s - loss: 0.0081 - acc: 0.997 - ETA: 3s - loss: 0.0077 - acc: 0.997 - ETA: 3s - loss: 0.0074 - acc: 0.997 - ETA: 3s - loss: 0.0071 - acc: 0.997 - ETA: 3s - loss: 0.0069 - acc: 0.997 - ETA: 3s - loss: 0.0069 - acc: 0.997 - ETA: 3s - loss: 0.0067 - acc: 0.997 - ETA: 3s - loss: 0.0065 - acc: 0.997 - ETA: 3s - loss: 0.0063 - acc: 0.998 - ETA: 3s - loss: 0.0069 - acc: 0.997 - ETA: 2s - loss: 0.0104 - acc: 0.997 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0095 - acc: 0.997 - ETA: 2s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0099 - acc: 0.997 - ETA: 2s - loss: 0.0097 - acc: 0.997 - ETA: 2s - loss: 0.0095 - acc: 0.997 - ETA: 2s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0090 - acc: 0.997 - ETA: 2s - loss: 0.0089 - acc: 0.997 - ETA: 2s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0091 - acc: 0.997 - ETA: 2s - loss: 0.0088 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0094 - acc: 0.997 - ETA: 2s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0094 - acc: 0.997 - ETA: 1s - loss: 0.0093 - acc: 0.997 - ETA: 1s - loss: 0.0095 - acc: 0.997 - ETA: 1s - loss: 0.0094 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0108 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0114 - acc: 0.996 - ETA: 1s - loss: 0.0113 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0113 - acc: 0.997 - ETA: 1s - loss: 0.0111 - acc: 0.997 - ETA: 1s - loss: 0.0110 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 1s - loss: 0.0109 - acc: 0.997 - ETA: 1s - loss: 0.0107 - acc: 0.997 - ETA: 0s - loss: 0.0106 - acc: 0.997 - ETA: 0s - loss: 0.0105 - acc: 0.997 - ETA: 0s - loss: 0.0104 - acc: 0.997 - ETA: 0s - loss: 0.0103 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0102 - acc: 0.997 - ETA: 0s - loss: 0.0101 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0100 - acc: 0.997 - ETA: 0s - loss: 0.0099 - acc: 0.997 - ETA: 0s - loss: 0.0097 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0096 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0095 - acc: 0.997 - ETA: 0s - loss: 0.0094 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - ETA: 0s - loss: 0.0093 - acc: 0.997 - 4s 667us/step - loss: 0.0092 - acc: 0.9976 - val_loss: 0.8481 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 6.4812e-04 - acc: 1.000 - ETA: 4s - loss: 0.0427 - acc: 0.9800    - ETA: 4s - loss: 0.0244 - acc: 0.988 - ETA: 4s - loss: 0.0170 - acc: 0.992 - ETA: 4s - loss: 0.0133 - acc: 0.994 - ETA: 4s - loss: 0.0110 - acc: 0.995 - ETA: 4s - loss: 0.0096 - acc: 0.996 - ETA: 4s - loss: 0.0089 - acc: 0.996 - ETA: 3s - loss: 0.0079 - acc: 0.997 - ETA: 3s - loss: 0.0134 - acc: 0.995 - ETA: 3s - loss: 0.0122 - acc: 0.996 - ETA: 3s - loss: 0.0110 - acc: 0.996 - ETA: 3s - loss: 0.0102 - acc: 0.997 - ETA: 3s - loss: 0.0129 - acc: 0.996 - ETA: 3s - loss: 0.0131 - acc: 0.995 - ETA: 3s - loss: 0.0176 - acc: 0.995 - ETA: 3s - loss: 0.0166 - acc: 0.995 - ETA: 3s - loss: 0.0162 - acc: 0.995 - ETA: 3s - loss: 0.0154 - acc: 0.996 - ETA: 3s - loss: 0.0147 - acc: 0.996 - ETA: 3s - loss: 0.0140 - acc: 0.996 - ETA: 3s - loss: 0.0134 - acc: 0.996 - ETA: 3s - loss: 0.0128 - acc: 0.996 - ETA: 3s - loss: 0.0126 - acc: 0.996 - ETA: 3s - loss: 0.0121 - acc: 0.997 - ETA: 3s - loss: 0.0117 - acc: 0.997 - ETA: 2s - loss: 0.0113 - acc: 0.997 - ETA: 2s - loss: 0.0109 - acc: 0.997 - ETA: 2s - loss: 0.0109 - acc: 0.997 - ETA: 2s - loss: 0.0106 - acc: 0.997 - ETA: 2s - loss: 0.0103 - acc: 0.997 - ETA: 2s - loss: 0.0104 - acc: 0.996 - ETA: 2s - loss: 0.0101 - acc: 0.997 - ETA: 2s - loss: 0.0099 - acc: 0.997 - ETA: 2s - loss: 0.0097 - acc: 0.997 - ETA: 2s - loss: 0.0100 - acc: 0.996 - ETA: 2s - loss: 0.0098 - acc: 0.997 - ETA: 2s - loss: 0.0096 - acc: 0.997 - ETA: 2s - loss: 0.0098 - acc: 0.996 - ETA: 2s - loss: 0.0096 - acc: 0.996 - ETA: 2s - loss: 0.0094 - acc: 0.997 - ETA: 2s - loss: 0.0092 - acc: 0.997 - ETA: 2s - loss: 0.0090 - acc: 0.997 - ETA: 2s - loss: 0.0088 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0084 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0086 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 1s - loss: 0.0083 - acc: 0.997 - ETA: 1s - loss: 0.0082 - acc: 0.997 - ETA: 1s - loss: 0.0081 - acc: 0.997 - ETA: 1s - loss: 0.0080 - acc: 0.997 - ETA: 1s - loss: 0.0079 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0078 - acc: 0.997 - ETA: 1s - loss: 0.0077 - acc: 0.997 - ETA: 1s - loss: 0.0076 - acc: 0.997 - ETA: 1s - loss: 0.0075 - acc: 0.997 - ETA: 1s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0087 - acc: 0.997 - ETA: 0s - loss: 0.0086 - acc: 0.997 - ETA: 0s - loss: 0.0085 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0084 - acc: 0.997 - ETA: 0s - loss: 0.0083 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0082 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - ETA: 0s - loss: 0.0079 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0078 - acc: 0.997 - ETA: 0s - loss: 0.0077 - acc: 0.997 - ETA: 0s - loss: 0.0081 - acc: 0.997 - ETA: 0s - loss: 0.0080 - acc: 0.997 - 5s 676us/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.9184 - val_acc: 0.8287\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 9.3342e-04 - acc: 1.000 - ETA: 4s - loss: 4.6043e-04 - acc: 1.000 - ETA: 4s - loss: 0.0135 - acc: 0.9944    - ETA: 4s - loss: 0.0095 - acc: 0.996 - ETA: 4s - loss: 0.0073 - acc: 0.997 - ETA: 4s - loss: 0.0062 - acc: 0.997 - ETA: 4s - loss: 0.0058 - acc: 0.998 - ETA: 4s - loss: 0.0050 - acc: 0.998 - ETA: 4s - loss: 0.0045 - acc: 0.998 - ETA: 4s - loss: 0.0042 - acc: 0.998 - ETA: 3s - loss: 0.0040 - acc: 0.998 - ETA: 3s - loss: 0.0053 - acc: 0.997 - ETA: 3s - loss: 0.0051 - acc: 0.998 - ETA: 3s - loss: 0.0047 - acc: 0.998 - ETA: 3s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0043 - acc: 0.998 - ETA: 3s - loss: 0.0057 - acc: 0.997 - ETA: 3s - loss: 0.0055 - acc: 0.997 - ETA: 3s - loss: 0.0052 - acc: 0.997 - ETA: 3s - loss: 0.0050 - acc: 0.998 - ETA: 3s - loss: 0.0048 - acc: 0.998 - ETA: 3s - loss: 0.0046 - acc: 0.998 - ETA: 3s - loss: 0.0044 - acc: 0.998 - ETA: 3s - loss: 0.0042 - acc: 0.998 - ETA: 3s - loss: 0.0041 - acc: 0.998 - ETA: 3s - loss: 0.0039 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0039 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0037 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0040 - acc: 0.998 - ETA: 2s - loss: 0.0040 - acc: 0.998 - ETA: 2s - loss: 0.0039 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0037 - acc: 0.998 - ETA: 2s - loss: 0.0037 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0035 - acc: 0.998 - ETA: 2s - loss: 0.0034 - acc: 0.998 - ETA: 2s - loss: 0.0035 - acc: 0.998 - ETA: 2s - loss: 0.0035 - acc: 0.998 - ETA: 2s - loss: 0.0034 - acc: 0.998 - ETA: 2s - loss: 0.0034 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0046 - acc: 0.998 - ETA: 1s - loss: 0.0045 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0043 - acc: 0.998 - ETA: 1s - loss: 0.0044 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0061 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0060 - acc: 0.998 - ETA: 0s - loss: 0.0059 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0066 - acc: 0.998 - 4s 669us/step - loss: 0.0065 - acc: 0.9982 - val_loss: 0.9029 - val_acc: 0.8204\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "Epoch 19/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 1.1631e-04 - acc: 1.000 - ETA: 4s - loss: 8.9360e-04 - acc: 1.000 - ETA: 4s - loss: 0.0011 - acc: 1.0000    - ETA: 4s - loss: 9.7559e-04 - acc: 1.000 - ETA: 4s - loss: 0.0012 - acc: 1.0000    - ETA: 4s - loss: 9.9980e-04 - acc: 1.000 - ETA: 3s - loss: 0.0024 - acc: 0.9981    - ETA: 3s - loss: 0.0021 - acc: 0.998 - ETA: 3s - loss: 0.0021 - acc: 0.998 - ETA: 3s - loss: 0.0019 - acc: 0.998 - ETA: 3s - loss: 0.0017 - acc: 0.998 - ETA: 3s - loss: 0.0016 - acc: 0.998 - ETA: 3s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0043 - acc: 0.998 - ETA: 3s - loss: 0.0040 - acc: 0.998 - ETA: 3s - loss: 0.0051 - acc: 0.997 - ETA: 3s - loss: 0.0048 - acc: 0.997 - ETA: 3s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0043 - acc: 0.998 - ETA: 3s - loss: 0.0041 - acc: 0.998 - ETA: 3s - loss: 0.0061 - acc: 0.997 - ETA: 3s - loss: 0.0059 - acc: 0.997 - ETA: 3s - loss: 0.0056 - acc: 0.997 - ETA: 3s - loss: 0.0054 - acc: 0.998 - ETA: 2s - loss: 0.0052 - acc: 0.998 - ETA: 2s - loss: 0.0051 - acc: 0.998 - ETA: 2s - loss: 0.0050 - acc: 0.998 - ETA: 2s - loss: 0.0048 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.997 - ETA: 2s - loss: 0.0070 - acc: 0.997 - ETA: 2s - loss: 0.0071 - acc: 0.997 - ETA: 2s - loss: 0.0069 - acc: 0.997 - ETA: 2s - loss: 0.0067 - acc: 0.997 - ETA: 2s - loss: 0.0066 - acc: 0.997 - ETA: 2s - loss: 0.0064 - acc: 0.997 - ETA: 2s - loss: 0.0063 - acc: 0.997 - ETA: 2s - loss: 0.0061 - acc: 0.997 - ETA: 2s - loss: 0.0059 - acc: 0.997 - ETA: 2s - loss: 0.0058 - acc: 0.997 - ETA: 2s - loss: 0.0057 - acc: 0.997 - ETA: 2s - loss: 0.0056 - acc: 0.997 - ETA: 2s - loss: 0.0055 - acc: 0.998 - ETA: 2s - loss: 0.0054 - acc: 0.998 - ETA: 1s - loss: 0.0053 - acc: 0.998 - ETA: 1s - loss: 0.0052 - acc: 0.998 - ETA: 1s - loss: 0.0051 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.997 - ETA: 1s - loss: 0.0057 - acc: 0.998 - ETA: 1s - loss: 0.0056 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0064 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0063 - acc: 0.997 - ETA: 1s - loss: 0.0062 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.997 - ETA: 1s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0060 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - ETA: 0s - loss: 0.0059 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - ETA: 0s - loss: 0.0058 - acc: 0.997 - ETA: 0s - loss: 0.0057 - acc: 0.997 - ETA: 0s - loss: 0.0056 - acc: 0.997 - ETA: 0s - loss: 0.0056 - acc: 0.997 - ETA: 0s - loss: 0.0055 - acc: 0.997 - ETA: 0s - loss: 0.0055 - acc: 0.997 - ETA: 0s - loss: 0.0055 - acc: 0.997 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0067 - acc: 0.997 - ETA: 0s - loss: 0.0066 - acc: 0.997 - 4s 671us/step - loss: 0.0066 - acc: 0.9979 - val_loss: 0.8954 - val_acc: 0.8204\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 6s - loss: 0.0010 - acc: 1.000 - ETA: 4s - loss: 0.0015 - acc: 1.000 - ETA: 4s - loss: 0.0011 - acc: 1.000 - ETA: 4s - loss: 0.0111 - acc: 0.996 - ETA: 4s - loss: 0.0087 - acc: 0.997 - ETA: 3s - loss: 0.0069 - acc: 0.997 - ETA: 3s - loss: 0.0059 - acc: 0.998 - ETA: 3s - loss: 0.0056 - acc: 0.998 - ETA: 3s - loss: 0.0050 - acc: 0.998 - ETA: 3s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0041 - acc: 0.998 - ETA: 3s - loss: 0.0039 - acc: 0.998 - ETA: 3s - loss: 0.0048 - acc: 0.998 - ETA: 3s - loss: 0.0045 - acc: 0.998 - ETA: 3s - loss: 0.0051 - acc: 0.997 - ETA: 3s - loss: 0.0048 - acc: 0.997 - ETA: 3s - loss: 0.0049 - acc: 0.997 - ETA: 3s - loss: 0.0046 - acc: 0.997 - ETA: 3s - loss: 0.0044 - acc: 0.998 - ETA: 3s - loss: 0.0042 - acc: 0.998 - ETA: 3s - loss: 0.0041 - acc: 0.998 - ETA: 3s - loss: 0.0040 - acc: 0.998 - ETA: 3s - loss: 0.0038 - acc: 0.998 - ETA: 3s - loss: 0.0037 - acc: 0.998 - ETA: 3s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0035 - acc: 0.998 - ETA: 2s - loss: 0.0034 - acc: 0.998 - ETA: 2s - loss: 0.0042 - acc: 0.998 - ETA: 2s - loss: 0.0041 - acc: 0.998 - ETA: 2s - loss: 0.0040 - acc: 0.998 - ETA: 2s - loss: 0.0039 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0038 - acc: 0.998 - ETA: 2s - loss: 0.0037 - acc: 0.998 - ETA: 2s - loss: 0.0036 - acc: 0.998 - ETA: 2s - loss: 0.0066 - acc: 0.997 - ETA: 2s - loss: 0.0064 - acc: 0.998 - ETA: 2s - loss: 0.0063 - acc: 0.998 - ETA: 2s - loss: 0.0062 - acc: 0.998 - ETA: 2s - loss: 0.0061 - acc: 0.998 - ETA: 2s - loss: 0.0060 - acc: 0.998 - ETA: 2s - loss: 0.0071 - acc: 0.997 - ETA: 2s - loss: 0.0069 - acc: 0.998 - ETA: 2s - loss: 0.0068 - acc: 0.998 - ETA: 1s - loss: 0.0067 - acc: 0.998 - ETA: 1s - loss: 0.0065 - acc: 0.998 - ETA: 1s - loss: 0.0064 - acc: 0.998 - ETA: 1s - loss: 0.0063 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0062 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.998 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0061 - acc: 0.997 - ETA: 1s - loss: 0.0060 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0059 - acc: 0.998 - ETA: 1s - loss: 0.0058 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0057 - acc: 0.998 - ETA: 0s - loss: 0.0056 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0055 - acc: 0.998 - ETA: 0s - loss: 0.0054 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0053 - acc: 0.998 - ETA: 0s - loss: 0.0052 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0051 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0050 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - ETA: 0s - loss: 0.0049 - acc: 0.998 - 4s 665us/step - loss: 0.0049 - acc: 0.9985 - val_loss: 0.9514 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25923e37f60>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TODO: Train the model.\n",
    "checkpointer1 = ModelCheckpoint(filepath='saved_models/weights.best.Resnet50.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "Resnet50_model.fit(train_Resnet50, train_targets, \n",
    "          validation_data=(valid_Resnet50, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer1], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "Resnet50_model.load_weights('saved_models/weights.best.Resnet50.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 81.1005%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "Resnet50_predictions = [np.argmax(Resnet50_model.predict(np.expand_dims(feature, axis=0))) for feature in test_Resnet50]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy1 = 100*np.sum(np.array(Resnet50_predictions)==np.argmax(test_targets, axis=1))/len(Resnet50_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import *\n",
    "def dog_breed(imge):\n",
    "    bottleneck_feature =extract_Resnet50(path_to_tensor(imge))\n",
    "    Resnet50_predictions = Resnet50_model.predict(bottleneck_feature)\n",
    "    return dog_names[np.argmax(Resnet50_predictions)]\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "def detector(imge):\n",
    "    if(dog_detector(imge)):\n",
    "        print(dog_breed(imge))\n",
    "    elif(face_detector(imge)):\n",
    "        print(\"You look like a ...\")\n",
    "        print(dog_breed(imge))\n",
    "    elif(1):\n",
    "        print(\"Oops!! Something is not right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ The out was better than expected :).\n",
    "The possible points of improvement are:\n",
    "1. Image Augmentation.\n",
    "2. Trying any other pre-trained model.\n",
    "3. Supply the model with more pictures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Golden_retriever\n"
     ]
    }
   ],
   "source": [
    "## TODO: Execute your algorithm from Step 6 on\n",
    "## at least 6 images on your computer.\n",
    "## Feel free to use as many code cells as needed.\n",
    "detector('My_Images/Dog_1.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German_shepherd_dog\n",
      "Anatolian_shepherd_dog\n",
      "Boxer\n",
      "Alaskan_malamute\n"
     ]
    }
   ],
   "source": [
    "detector('My_Images/Dog_2.jpg')\n",
    "detector('My_Images/Dog_3.jpg')\n",
    "detector('My_Images/Dog_4.jpg')\n",
    "detector('My_Images/Dog_5.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops!! Something is not right\n"
     ]
    }
   ],
   "source": [
    "detector('My_Images/Pen.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You look like a ...\n",
      "English_toy_spaniel\n",
      "You look like a ...\n",
      "Basenji\n"
     ]
    }
   ],
   "source": [
    "detector('My_Images/Human_3.jpg')\n",
    "detector('My_Images/Human_4.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oops!! Something is not right\n",
      "Oops!! Something is not right\n"
     ]
    }
   ],
   "source": [
    "detector('My_Images/Tree.jpg')\n",
    "detector('My_Images/Chimpanzee.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
